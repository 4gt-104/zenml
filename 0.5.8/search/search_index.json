{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"API Overview Modules artifact_stores : An artifact store is a place where artifacts are stored. These artifacts may artifact_stores.base_artifact_store : Definition of an Artifact Store artifact_stores.local_artifact_store artifacts : Artifacts are the data that power your experimentation and model training. It is artifacts.base_artifact : The below code is copied from the TFX source repo with minor changes. artifacts.constants artifacts.data_analysis_artifact artifacts.data_artifact artifacts.model_artifact artifacts.schema_artifact artifacts.statistics_artifact artifacts.type_registry cli : ZenML CLI cli.artifact_store cli.base cli.cli : .. currentmodule:: ce_cli.cli cli.config : CLI for manipulating ZenML local and global config file. cli.container_registry cli.example cli.integration cli.metadata_store cli.orchestrator cli.pipeline : CLI to interact with pipelines. cli.stack : CLI for manipulating ZenML local and global config file. cli.utils cli.version config : The config module contains classes and functions that manage user-specific config.config_keys config.constants config.global_config : Global config for the ZenML installation. constants container_registries container_registries.base_container_registry : Base class for all container registries. core : The core module is where all the base ZenML functionality is defined, core.base_component core.component_factory : Factory to register all components. core.constants core.git_wrapper : Wrapper class to handle Git integration core.local_service core.mapping_utils core.repo : Base ZenML repository core.utils enums exceptions : ZenML specific exception definitions integrations : The ZenML integrations module contains sub-modules for each integration that we integrations.airflow : The Airflow integration sub-module powers an alternative to the local integrations.airflow.orchestrators integrations.airflow.orchestrators.airflow_component : Definition for Airflow component for TFX. integrations.airflow.orchestrators.airflow_dag_runner : Definition of Airflow TFX runner. This is an unmodified copy from the TFX integrations.airflow.orchestrators.airflow_orchestrator integrations.constants integrations.dash integrations.dash.visualizers integrations.dash.visualizers.pipeline_run_lineage_visualizer integrations.evidently : The Evidently integration provides a way to monitor your models in production. integrations.evidently.steps integrations.evidently.steps.evidently_profile integrations.evidently.visualizers integrations.evidently.visualizers.evidently_visualizer integrations.facets : The Facets integration provides a simple way to visualize post-execution objects integrations.facets.visualizers integrations.facets.visualizers.facet_statistics_visualizer integrations.gcp : The GCP integration submodule provides a way to run ZenML pipelines in a cloud integrations.gcp.artifact_stores integrations.gcp.artifact_stores.gcp_artifact_store integrations.gcp.io integrations.gcp.io.gcs_plugin : Plugin which is created to add Google Cloud Store support to ZenML. integrations.graphviz integrations.graphviz.visualizers integrations.graphviz.visualizers.pipeline_run_dag_visualizer integrations.integration integrations.kubeflow : The Kubeflow integration sub-module powers an alternative to the local integrations.kubeflow.container_entrypoint : Main entrypoint for containers with Kubeflow TFX component executors. integrations.kubeflow.docker_utils integrations.kubeflow.metadata integrations.kubeflow.metadata.kubeflow_metadata_store integrations.kubeflow.orchestrators integrations.kubeflow.orchestrators.kubeflow_component : Kubeflow Pipelines based implementation of TFX components. integrations.kubeflow.orchestrators.kubeflow_dag_runner : The below code is copied from the TFX source repo with minor changes. integrations.kubeflow.orchestrators.kubeflow_orchestrator integrations.kubeflow.orchestrators.kubeflow_utils : Common utility for Kubeflow-based orchestrator. integrations.kubeflow.orchestrators.local_deployment_utils integrations.mlflow : The mlflow integrations currently enables you to use mlflow tracking as a integrations.mlflow.mlflow_utils integrations.plotly integrations.plotly.visualizers integrations.plotly.visualizers.pipeline_lineage_visualizer integrations.pytorch integrations.pytorch.materializers integrations.pytorch.materializers.pytorch_materializer integrations.pytorch.materializers.pytorch_types integrations.pytorch_lightning integrations.pytorch_lightning.materializers integrations.pytorch_lightning.materializers.pytorch_lightning_materializer integrations.registry integrations.sklearn integrations.sklearn.helpers integrations.sklearn.helpers.digits integrations.sklearn.materializers integrations.sklearn.materializers.sklearn_materializer integrations.sklearn.steps integrations.sklearn.steps.sklearn_evaluator integrations.sklearn.steps.sklearn_splitter integrations.sklearn.steps.sklearn_standard_scaler integrations.tensorflow integrations.tensorflow.materializers integrations.tensorflow.materializers.keras_materializer integrations.tensorflow.materializers.tf_dataset_materializer integrations.tensorflow.steps integrations.tensorflow.steps.tensorflow_trainer integrations.utils io : The io module handles file operations for the ZenML package. It offers a io.fileio io.fileio_registry : Filesystem registry managing filesystem plugins. io.filesystem io.utils logger materializers : Materializers are used to convert a ZenML artifact into a specific format. They materializers.base_materializer materializers.beam_materializer materializers.built_in_materializer materializers.default_materializer_registry materializers.numpy_materializer materializers.pandas_materializer metadata_stores : The configuration of each pipeline, step, backend, and produced artifacts are metadata_stores.base_metadata_store metadata_stores.mysql_metadata_store metadata_stores.sqlite_metadata_store orchestrators : An orchestrator is a special kind of backend that manages the running of each orchestrators.base_orchestrator orchestrators.local orchestrators.local.local_dag_runner : Inspired by local dag runner implementation orchestrators.local.local_orchestrator orchestrators.utils pipelines : A ZenML pipeline is a sequence of tasks that execute in a specific order and pipelines.base_pipeline pipelines.builtin_pipelines pipelines.builtin_pipelines.training_pipeline pipelines.pipeline_decorator post_execution : After executing a pipeline, the user needs to be able to fetch it from history post_execution.artifact post_execution.pipeline post_execution.pipeline_run post_execution.step stacks : A stack is made up of the following three core components: an Artifact Store, a stacks.base_stack stacks.constants steps : A step is a single piece or stage of a ZenML pipeline. Think of each step as steps.base_step steps.base_step_config steps.builtin_steps steps.builtin_steps.pandas_analyzer steps.builtin_steps.pandas_datasource steps.step_context steps.step_decorator steps.step_interfaces steps.step_interfaces.base_analyzer_step steps.step_interfaces.base_datasource_step steps.step_interfaces.base_drift_detection_step steps.step_interfaces.base_evaluator_step steps.step_interfaces.base_preprocessor_step steps.step_interfaces.base_split_step steps.step_interfaces.base_trainer_step steps.step_output steps.utils : The collection of utility functions/classes are inspired by their original utils : The utils module contains utility functions handling analytics, reading and utils.analytics_utils : Analytics code for ZenML utils.daemon : Utility functions to start/stop daemon processes. utils.networking_utils utils.source_utils : These utils are predicated on the following definitions: utils.string_utils utils.yaml_utils visualizers : The visualizers module offers a way of constructing and displaying visualizers.base_pipeline_run_visualizer visualizers.base_pipeline_visualizer visualizers.base_step_visualizer visualizers.base_visualizer Classes base_artifact_store.BaseArtifactStore : Base class for all ZenML Artifact Store. local_artifact_store.LocalArtifactStore : Artifact Store for local artifacts. base_artifact.BaseArtifact : Base class for all ZenML artifacts. data_analysis_artifact.DataAnalysisArtifact : Class for all ZenML data analysis artifacts. data_artifact.DataArtifact : Class for all ZenML data artifacts. model_artifact.ModelArtifact : Class for all ZenML model artifacts. schema_artifact.SchemaArtifact : Class for all ZenML schema artifacts. statistics_artifact.StatisticsArtifact : Class for all ZenML statistics artifacts. type_registry.ArtifactTypeRegistry : A registry to keep track of which datatypes map to which artifact example.Example : Class for all example objects. example.ExamplesRepo : Class for the examples repository object. example.GitExamplesHandler : Class for the GitExamplesHandler that interfaces with the CLI tool. example.LocalExample : Class to encapsulate all properties and methods of the local example config_keys.ConfigKeys : Class to validate dictionary configurations. config_keys.PipelineConfigurationKeys : Keys for a pipeline configuration dict. config_keys.StepConfigurationKeys : Keys for a step configuration dict. global_config.GlobalConfig : Class definition for the global config. base_container_registry.BaseContainerRegistry : Base class for all ZenML container registries. base_component.BaseComponent : Class definition for the base config. component_factory.ComponentFactory : Definition of ComponentFactory to track all BaseComponent subclasses. git_wrapper.GitWrapper : Wrapper class for Git. local_service.LocalService : Definition of a local service that keeps track of all ZenML mapping_utils.UUIDSourceTuple : Container used to store UUID and source information repo.Repository : ZenML repository definition. enums.ArtifactStoreTypes : All supported Artifact Store types. enums.ExecutionStatus : Enum that represents the current status of a step or pipeline run. enums.LoggingLevels : Enum for logging levels. enums.MLMetadataTypes : All supported ML Metadata types. enums.OrchestratorTypes : All supported Orchestrator types enums.StackTypes : All supported Stack types. exceptions.AlreadyExistsException : Raises exception when the name already exist in the system but an exceptions.ArtifactInterfaceError : Raises exception when interacting with the Artifact interface exceptions.DoesNotExistException : Raises exception when the entity does not exist in the system but an exceptions.DuplicateRunNameError : Raises exception when a run with the same name already exists. exceptions.EmptyDatasourceException : Raises exception when a datasource data is accessed without running exceptions.GitException : Raises exception when a problem occurs in git resolution. exceptions.InitializationException : Raises exception when a function is run before zenml initialization. exceptions.IntegrationError : Raises exceptions when a requested integration can not be activated. exceptions.MissingStepParameterError : Raises exceptions when a step parameter is missing when running a exceptions.PipelineConfigurationError : Raises exceptions when a pipeline configuration contains exceptions.PipelineInterfaceError : Raises exception when interacting with the Pipeline interface exceptions.PipelineNotSucceededException : Raises exception when trying to fetch artifacts from a not succeeded exceptions.StepContextError : Raises exception when interacting with a StepContext exceptions.StepInterfaceError : Raises exception when interacting with the Step interface airflow.AirflowIntegration : Definition of Airflow Integration for ZenML. airflow_component.AirflowComponent : Airflow-specific TFX Component. airflow_dag_runner.AirflowDagRunner : Tfx runner on Airflow. airflow_dag_runner.AirflowPipelineConfig : Pipeline config for AirflowDagRunner. airflow_orchestrator.AirflowOrchestrator : Orchestrator responsible for running pipelines using Airflow. dash.DashIntegration : Definition of Dash integration for ZenML. pipeline_run_lineage_visualizer.PipelineRunLineageVisualizer : Implementation of a lineage diagram via the [dash]( evidently.EvidentlyIntegration : Definition of Evidently integration evidently_profile.EvidentlyProfileConfig : Config class for Evidently profile steps. evidently_profile.EvidentlyProfileStep : Simple step implementation which implements Evidently's functionality for evidently_visualizer.EvidentlyVisualizer : The implementation of an Evidently Visualizer. facets.FacetsIntegration : Definition of Facet integration facet_statistics_visualizer.FacetStatisticsVisualizer : The base implementation of a ZenML Visualizer. gcp.GcpIntegration : Definition of Google Cloud Platform integration for ZenML. gcp_artifact_store.GCPArtifactStore : Artifact Store for Google Cloud Storage based artifacts. gcs_plugin.ZenGCS : Filesystem that delegates to Google Cloud Store using gcsfs. graphviz.GraphvizIntegration : Definition of Graphviz integration for ZenML. pipeline_run_dag_visualizer.PipelineRunDagVisualizer : Visualize the lineage of runs in a pipeline. integration.Integration : Base class for integration in ZenML integration.IntegrationMeta : Metaclass responsible for registering different Integration kubeflow.KubeflowIntegration : Definition of Kubeflow Integration for ZenML. kubeflow_metadata_store.KubeflowMetadataStore : Kubeflow MySQL backend for ZenML metadata store. kubeflow_component.KubeflowComponent : Base component for all Kubeflow pipelines TFX components. kubeflow_dag_runner.KubeflowDagRunner : Kubeflow Pipelines runner. kubeflow_dag_runner.KubeflowDagRunnerConfig : Runtime configuration parameters specific to execution on Kubeflow. kubeflow_orchestrator.KubeflowOrchestrator : Orchestrator responsible for running pipelines using Kubeflow. mlflow.MlflowIntegration : Definition of Plotly integration for ZenML. plotly.PlotlyIntegration : Definition of Plotly integration for ZenML. pipeline_lineage_visualizer.PipelineLineageVisualizer : Visualize the lineage of runs in a pipeline using plotly. pytorch.PytorchIntegration : Definition of PyTorch integration for ZenML. pytorch_materializer.PyTorchMaterializer : Materializer to read/write Pytorch models. pytorch_types.TorchDict : A type of dict that represents saving a model. pytorch_lightning.PytorchLightningIntegration : Definition of PyTorch Lightning integration for ZenML. pytorch_lightning_materializer.PyTorchLightningMaterializer : Materializer to read/write Pytorch models. registry.IntegrationRegistry : Registry to keep track of ZenML Integrations sklearn.SklearnIntegration : Definition of sklearn integration for ZenML. sklearn_materializer.SklearnMaterializer : Materializer to read data to and from sklearn. sklearn_evaluator.SklearnEvaluator : A simple step implementation which utilizes sklearn to evaluate the sklearn_evaluator.SklearnEvaluatorConfig : Config class for the sklearn evaluator sklearn_splitter.SklearnSplitter : A simple step implementation which utilizes sklearn to split a given sklearn_splitter.SklearnSplitterConfig : Config class for the sklearn splitter sklearn_standard_scaler.SklearnStandardScaler : Simple step implementation which utilizes the StandardScaler from sklearn sklearn_standard_scaler.SklearnStandardScalerConfig : Config class for the sklearn standard scaler tensorflow.TensorflowIntegration : Definition of Tensorflow integration for ZenML. keras_materializer.KerasMaterializer : Materializer to read/write Keras models. tf_dataset_materializer.TensorflowDatasetMaterializer : Materializer to read data to and from tf.data.Dataset. tensorflow_trainer.TensorflowBinaryClassifier : Simple step implementation which creates a simple tensorflow feedforward tensorflow_trainer.TensorflowBinaryClassifierConfig : Config class for the tensorflow trainer io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation fileio_registry.FileIORegistry : Registry of pluggable filesystem implementations used in TFX components. filesystem.FileSystemMeta : Metaclass which is responsible for registering the defined filesystem filesystem.Filesystem : Abstract Filesystem class. filesystem.NotFoundError : Auxiliary not found error logger.CustomFormatter : Formats logs according to custom specifications. base_materializer.BaseMaterializer : Base Materializer to realize artifact data. base_materializer.BaseMaterializerMeta : Metaclass responsible for registering different BaseMaterializer beam_materializer.BeamMaterializer : Materializer to read data to and from beam. built_in_materializer.BuiltInMaterializer : Read/Write JSON files. default_materializer_registry.MaterializerRegistry : Matches a python type to a default materializer. numpy_materializer.NumpyMaterializer : Materializer to read data to and from pandas. pandas_materializer.PandasMaterializer : Materializer to read data to and from pandas. base_metadata_store.BaseMetadataStore : Metadata store base class to track metadata of zenml first class mysql_metadata_store.MySQLMetadataStore : MySQL backend for ZenML metadata store. sqlite_metadata_store.SQLiteMetadataStore : SQLite backend for ZenML metadata store. base_orchestrator.BaseOrchestrator : Base Orchestrator class to orchestrate ZenML pipelines. local_dag_runner.LocalDagRunner : Local TFX DAG runner. local_orchestrator.LocalOrchestrator : Orchestrator responsible for running pipelines locally. base_pipeline.BasePipeline : Abstract base class for all ZenML pipelines. base_pipeline.BasePipelineMeta : Pipeline Metaclass responsible for validating the pipeline definition. training_pipeline.TrainingPipeline : Class for the classic training pipeline implementation artifact.ArtifactView : Post-execution artifact class which can be used to read pipeline.PipelineView : Post-execution pipeline class which can be used to query pipeline_run.PipelineRunView : Post-execution pipeline run class which can be used to query step.StepView : Post-execution step class which can be used to query base_stack.BaseStack : Base stack for ZenML. base_step.BaseStep : Abstract base class for all ZenML steps. base_step.BaseStepMeta : Metaclass for BaseStep . base_step_config.BaseStepConfig : Base configuration class to pass execution params into a step. pandas_analyzer.PandasAnalyzer : Simple step implementation which analyzes a given pd.DataFrame pandas_analyzer.PandasAnalyzerConfig : Config class for the PandasAnalyzer Config pandas_datasource.PandasDatasource : Simple step implementation to ingest from a csv file using pandas pandas_datasource.PandasDatasourceConfig : Config class for the pandas csv datasource step_context.StepContext : Provides additional context inside a step function. step_context.StepContextOutput : Tuple containing materializer class and artifact for a step output. base_analyzer_step.BaseAnalyzerConfig : Base class for analyzer step configurations base_analyzer_step.BaseAnalyzerStep : Base step implementation for any analyzer step implementation on ZenML base_datasource_step.BaseDatasourceConfig : Base class for datasource configs to inherit from base_datasource_step.BaseDatasourceStep : Base step implementation for any datasource step implementation on ZenML base_drift_detection_step.BaseDriftDetectionConfig : Base class for drift detection step configurations base_drift_detection_step.BaseDriftDetectionStep : Base step implementation for any drift detection step implementation base_evaluator_step.BaseEvaluatorConfig : Base class for evaluator step configurations base_evaluator_step.BaseEvaluatorStep : Base step implementation for any evaluator step implementation on ZenML base_preprocessor_step.BasePreprocessorConfig : Base class for Preprocessor step configurations base_preprocessor_step.BasePreprocessorStep : Base step implementation for any Preprocessor step implementation on base_split_step.BaseSplitStep : Base step implementation for any split step implementation on ZenML base_split_step.BaseSplitStepConfig : Base class for split configs to inherit from base_trainer_step.BaseTrainerConfig : Base class for Trainer step configurations base_trainer_step.BaseTrainerStep : Base step implementation for any Trainer step implementation on step_output.Output : A named tuple with a default name that cannot be overridden. base_pipeline_run_visualizer.BasePipelineRunVisualizer : The base implementation of a ZenML Pipeline Run Visualizer. base_pipeline_visualizer.BasePipelineVisualizer : The base implementation of a ZenML Pipeline Visualizer. base_step_visualizer.BaseStepVisualizer : The base implementation of a ZenML Step Visualizer. base_visualizer.BaseVisualizer : Base class for all ZenML Visualizers. Functions example.check_for_version_mismatch utils.activate_integrations : Decorator that activates all ZenML integrations. utils.confirmation : Echo a confirmation string on the CLI. utils.declare : Echo a declaration on the CLI. utils.error : Echo an error string on the CLI. utils.format_component_list : Formats a list of components into a List of Dicts. This list of dicts utils.format_date : Format a date into a string. utils.install_package : Installs pypi package into the current environment with pip utils.parse_unknown_options : Parse unknown options from the CLI. utils.pretty_print : Pretty print an object on the CLI. utils.print_component_properties : Prints the properties of a component. utils.print_table : Echoes the list of dicts in a table format. The input object should be a utils.title : Echo a title formatted string on the CLI. utils.uninstall_package : Uninstalls pypi package from the current environment with pip utils.warning : Echo a warning string on the CLI. constants.handle_bool_env_var : Converts normal env var to boolean constants.handle_int_env_var : Converts normal env var to int mapping_utils.get_component_from_key : Given a key and a mapping, return an initialized component. mapping_utils.get_components_from_store : Returns a list of components from a store. mapping_utils.get_key_from_uuid : Return the key that points to a certain uuid in a mapping. utils.define_json_config_settings_source : Define a function to essentially deserialize a model from a serialized utils.generate_customise_sources : Generate a customise_sources function as defined here: container_entrypoint.main : Runs a single step defined by the command line arguments. docker_utils.build_docker_image : Builds a docker image. docker_utils.create_custom_build_context : Creates a docker build context. docker_utils.generate_dockerfile_contents : Generates a Dockerfile. docker_utils.get_current_environment_requirements : Returns a dict of package requirements for the environment that docker_utils.get_image_digest : Gets the digest of a docker image. docker_utils.push_docker_image : Pushes a docker image to a container registry. kubeflow_dag_runner.get_default_pipeline_operator_funcs : Returns a default list of pipeline operator functions. kubeflow_dag_runner.get_default_pod_labels : Returns the default pod label dict for Kubeflow. kubeflow_utils.replace_placeholder : Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. local_deployment_utils.check_prerequisites : Checks whether all prerequisites for a local kubeflow pipelines local_deployment_utils.create_k3d_cluster : Creates a K3D cluster. local_deployment_utils.delete_k3d_cluster : Deletes a K3D cluster with the given name. local_deployment_utils.deploy_kubeflow_pipelines : Deploys Kubeflow Pipelines. local_deployment_utils.k3d_cluster_exists : Checks whether there exists a K3D cluster with the given name. local_deployment_utils.kubeflow_pipelines_ready : Returns whether all Kubeflow Pipelines pods are ready. local_deployment_utils.start_kfp_ui_daemon : Starts a daemon process that forwards ports so the Kubeflow Pipelines local_deployment_utils.write_local_registry_yaml : Writes a K3D registry config file. mlflow_utils.enable_mlflow : Outer decorator function for the creation of a ZenML pipeline with mlflow mlflow_utils.enable_mlflow_init : Outer decorator function for extending the init method for pipelines mlflow_utils.enable_mlflow_run : Outer decorator function for extending the run method for pipelines mlflow_utils.local_mlflow_backend : Returns the local mlflow backend inside the global zenml directory mlflow_utils.setup_mlflow : Setup all mlflow related configurations. This includes specifying which digits.get_digits : Returns the digits dataset in the form of a tuple of numpy digits.get_digits_model : Creates a support vector classifier for digits dataset. utils.get_integration_for_module : Gets the integration class for a module inside an integration. utils.get_requirements_for_module : Gets requirements for a module inside an integration. fileio.append_file : Appends file_contents to file. fileio.convert_to_str : Converts a PathType to a str using UTF-8. fileio.copy : Copy a file from the source to the destination. fileio.copy_dir : Copies dir from source to destination. fileio.create_dir_if_not_exists : Creates directory if it does not exist. fileio.create_dir_recursive_if_not_exists : Creates directory recursively if it does not exist. fileio.create_file_if_not_exists : Creates file if it does not exist. fileio.file_exists : Returns True if the given path exists. fileio.find_files : Find files in a directory that match pattern. fileio.get_grandparent : Get grandparent of dir. fileio.get_parent : Get parent of dir. fileio.glob : Return the paths that match a glob pattern. fileio.is_dir : Returns whether the given path points to a directory. fileio.is_remote : Returns True if path exists remotely. fileio.is_root : Returns true if path has no parent in local filesystem. fileio.list_dir : Returns a list of files under dir. fileio.make_dirs : Make a directory at the given path, recursively creating parents. fileio.mkdir : Make a directory at the given path; parent directory must exist. fileio.move : Moves dir or file from source to destination. Can be used to rename. fileio.open : Open a file at the given path. fileio.remove : Remove the file at the given path. Dangerous operation. fileio.rename : Rename source file to destination file. fileio.resolve_relative_path : Takes relative path and resolves it absolutely. fileio.rm_dir : Deletes dir recursively. Dangerous operation. fileio.stat : Return the stat descriptor for a given file path. fileio.walk : Return an iterator that walks the contents of the given directory. utils.create_tarfile : Create a compressed representation of source_dir. utils.extract_tarfile : Extracts all files in a compressed tar file to output_dir. utils.get_global_config_directory : Returns the global config directory for ZenML. utils.get_zenml_config_dir : Recursive function to find the zenml config starting from path. utils.get_zenml_dir : Returns path to a ZenML repository directory. utils.is_gcs_path : Returns True if path is on Google Cloud Storage. utils.is_zenml_dir : Check if dir is a zenml dir or not. utils.read_file_contents_as_string : Reads contents of file. utils.write_file_contents_as_string : Writes contents of file. logger.get_console_handler : Get console handler for logging. logger.get_file_handler : Return a file handler for logging. logger.get_logger : Main function to get logger name,. logger.get_logging_level : Get logging level from the env variable. logger.init_logging : Initialize logging with default levels. logger.set_root_verbosity : Set the root verbosity. utils.create_tfx_pipeline : Creates a tfx pipeline from a ZenML pipeline. utils.execute_step : Executes a tfx component. pipeline_decorator.pipeline : Outer decorator function for the creation of a ZenML pipeline step_decorator.step : Outer decorator function for the creation of a ZenML step utils.do_types_match : Check whether type_a and type_b match. utils.generate_component_class : Generates a TFX component class for a ZenML step. utils.generate_component_spec_class : Generates a TFX component spec class for a ZenML step. analytics_utils.get_environment : Returns a string representing the execution environment of the pipeline. analytics_utils.get_segment_key : Get key for authorizing to Segment backend. analytics_utils.get_system_info : Returns system info as a dict. analytics_utils.in_docker : Returns: True if running in a Docker container, else False analytics_utils.in_google_colab : Returns: True if running in a Google Colab env, else False analytics_utils.in_paperspace_gradient : Returns: True if running in a Paperspace Gradient env, else False analytics_utils.parametrized : This is a meta-decorator, that is, a decorator for decorators. analytics_utils.layer : Internal layer analytics_utils.track_event : Track segment event if user opted-in. daemon.check_if_daemon_is_running : Checks whether a daemon process indicated by the PID file is running. daemon.run_as_daemon : Runs a function as a daemon process. daemon.stop_daemon : Stops a daemon process. networking_utils.find_available_port : Finds a local unoccupied port. networking_utils.port_available : Checks if a local port is available. source_utils.create_zenml_pin : Creates a ZenML pin for source pinning from release version. source_utils.get_absolute_path_from_module_source : Get a directory path from module source. source_utils.get_class_source_from_source : Gets class source from source, i.e. module.path@version, returns version. source_utils.get_module_source_from_class : Takes class input and returns module_source. If class is already string source_utils.get_module_source_from_file_path : Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py source_utils.get_module_source_from_source : Gets module source from source. E.g. some.module.file.class@version , source_utils.get_relative_path_from_module_source : Get a directory path from module, relative to root of repository. source_utils.import_class_by_path : Imports a class based on a given path source_utils.import_python_file : Imports a python file. source_utils.is_inside_repository : Returns whether a file is inside a zenml repository. source_utils.is_standard_pin : Returns True if pin is valid ZenML pin, else False. source_utils.is_standard_source : Returns True if source is a standard ZenML source. source_utils.is_third_party_module : Returns whether a file belongs to a third party package. source_utils.load_source_path_class : Loads a Python class from the source. source_utils.resolve_class : Resolves a class into a serializable source string. source_utils.resolve_standard_source : Creates a ZenML pin for source pinning from release version. string_utils.get_human_readable_filesize : Convert a file size in bytes into a human-readable string. string_utils.get_human_readable_time : Convert seconds into a human-readable string. yaml_utils.is_yaml : Returns True if file_path is YAML, else False yaml_utils.read_json : Read JSON on file path and returns contents as dict. yaml_utils.read_yaml : Read YAML on file path and returns contents as dict. yaml_utils.write_json : Write contents as JSON format to file_path. yaml_utils.write_yaml : Write contents as YAML format to file_path. This file was automatically generated via lazydocs .","title":"Home"},{"location":"#api-overview","text":"","title":"API Overview"},{"location":"#modules","text":"artifact_stores : An artifact store is a place where artifacts are stored. These artifacts may artifact_stores.base_artifact_store : Definition of an Artifact Store artifact_stores.local_artifact_store artifacts : Artifacts are the data that power your experimentation and model training. It is artifacts.base_artifact : The below code is copied from the TFX source repo with minor changes. artifacts.constants artifacts.data_analysis_artifact artifacts.data_artifact artifacts.model_artifact artifacts.schema_artifact artifacts.statistics_artifact artifacts.type_registry cli : ZenML CLI cli.artifact_store cli.base cli.cli : .. currentmodule:: ce_cli.cli cli.config : CLI for manipulating ZenML local and global config file. cli.container_registry cli.example cli.integration cli.metadata_store cli.orchestrator cli.pipeline : CLI to interact with pipelines. cli.stack : CLI for manipulating ZenML local and global config file. cli.utils cli.version config : The config module contains classes and functions that manage user-specific config.config_keys config.constants config.global_config : Global config for the ZenML installation. constants container_registries container_registries.base_container_registry : Base class for all container registries. core : The core module is where all the base ZenML functionality is defined, core.base_component core.component_factory : Factory to register all components. core.constants core.git_wrapper : Wrapper class to handle Git integration core.local_service core.mapping_utils core.repo : Base ZenML repository core.utils enums exceptions : ZenML specific exception definitions integrations : The ZenML integrations module contains sub-modules for each integration that we integrations.airflow : The Airflow integration sub-module powers an alternative to the local integrations.airflow.orchestrators integrations.airflow.orchestrators.airflow_component : Definition for Airflow component for TFX. integrations.airflow.orchestrators.airflow_dag_runner : Definition of Airflow TFX runner. This is an unmodified copy from the TFX integrations.airflow.orchestrators.airflow_orchestrator integrations.constants integrations.dash integrations.dash.visualizers integrations.dash.visualizers.pipeline_run_lineage_visualizer integrations.evidently : The Evidently integration provides a way to monitor your models in production. integrations.evidently.steps integrations.evidently.steps.evidently_profile integrations.evidently.visualizers integrations.evidently.visualizers.evidently_visualizer integrations.facets : The Facets integration provides a simple way to visualize post-execution objects integrations.facets.visualizers integrations.facets.visualizers.facet_statistics_visualizer integrations.gcp : The GCP integration submodule provides a way to run ZenML pipelines in a cloud integrations.gcp.artifact_stores integrations.gcp.artifact_stores.gcp_artifact_store integrations.gcp.io integrations.gcp.io.gcs_plugin : Plugin which is created to add Google Cloud Store support to ZenML. integrations.graphviz integrations.graphviz.visualizers integrations.graphviz.visualizers.pipeline_run_dag_visualizer integrations.integration integrations.kubeflow : The Kubeflow integration sub-module powers an alternative to the local integrations.kubeflow.container_entrypoint : Main entrypoint for containers with Kubeflow TFX component executors. integrations.kubeflow.docker_utils integrations.kubeflow.metadata integrations.kubeflow.metadata.kubeflow_metadata_store integrations.kubeflow.orchestrators integrations.kubeflow.orchestrators.kubeflow_component : Kubeflow Pipelines based implementation of TFX components. integrations.kubeflow.orchestrators.kubeflow_dag_runner : The below code is copied from the TFX source repo with minor changes. integrations.kubeflow.orchestrators.kubeflow_orchestrator integrations.kubeflow.orchestrators.kubeflow_utils : Common utility for Kubeflow-based orchestrator. integrations.kubeflow.orchestrators.local_deployment_utils integrations.mlflow : The mlflow integrations currently enables you to use mlflow tracking as a integrations.mlflow.mlflow_utils integrations.plotly integrations.plotly.visualizers integrations.plotly.visualizers.pipeline_lineage_visualizer integrations.pytorch integrations.pytorch.materializers integrations.pytorch.materializers.pytorch_materializer integrations.pytorch.materializers.pytorch_types integrations.pytorch_lightning integrations.pytorch_lightning.materializers integrations.pytorch_lightning.materializers.pytorch_lightning_materializer integrations.registry integrations.sklearn integrations.sklearn.helpers integrations.sklearn.helpers.digits integrations.sklearn.materializers integrations.sklearn.materializers.sklearn_materializer integrations.sklearn.steps integrations.sklearn.steps.sklearn_evaluator integrations.sklearn.steps.sklearn_splitter integrations.sklearn.steps.sklearn_standard_scaler integrations.tensorflow integrations.tensorflow.materializers integrations.tensorflow.materializers.keras_materializer integrations.tensorflow.materializers.tf_dataset_materializer integrations.tensorflow.steps integrations.tensorflow.steps.tensorflow_trainer integrations.utils io : The io module handles file operations for the ZenML package. It offers a io.fileio io.fileio_registry : Filesystem registry managing filesystem plugins. io.filesystem io.utils logger materializers : Materializers are used to convert a ZenML artifact into a specific format. They materializers.base_materializer materializers.beam_materializer materializers.built_in_materializer materializers.default_materializer_registry materializers.numpy_materializer materializers.pandas_materializer metadata_stores : The configuration of each pipeline, step, backend, and produced artifacts are metadata_stores.base_metadata_store metadata_stores.mysql_metadata_store metadata_stores.sqlite_metadata_store orchestrators : An orchestrator is a special kind of backend that manages the running of each orchestrators.base_orchestrator orchestrators.local orchestrators.local.local_dag_runner : Inspired by local dag runner implementation orchestrators.local.local_orchestrator orchestrators.utils pipelines : A ZenML pipeline is a sequence of tasks that execute in a specific order and pipelines.base_pipeline pipelines.builtin_pipelines pipelines.builtin_pipelines.training_pipeline pipelines.pipeline_decorator post_execution : After executing a pipeline, the user needs to be able to fetch it from history post_execution.artifact post_execution.pipeline post_execution.pipeline_run post_execution.step stacks : A stack is made up of the following three core components: an Artifact Store, a stacks.base_stack stacks.constants steps : A step is a single piece or stage of a ZenML pipeline. Think of each step as steps.base_step steps.base_step_config steps.builtin_steps steps.builtin_steps.pandas_analyzer steps.builtin_steps.pandas_datasource steps.step_context steps.step_decorator steps.step_interfaces steps.step_interfaces.base_analyzer_step steps.step_interfaces.base_datasource_step steps.step_interfaces.base_drift_detection_step steps.step_interfaces.base_evaluator_step steps.step_interfaces.base_preprocessor_step steps.step_interfaces.base_split_step steps.step_interfaces.base_trainer_step steps.step_output steps.utils : The collection of utility functions/classes are inspired by their original utils : The utils module contains utility functions handling analytics, reading and utils.analytics_utils : Analytics code for ZenML utils.daemon : Utility functions to start/stop daemon processes. utils.networking_utils utils.source_utils : These utils are predicated on the following definitions: utils.string_utils utils.yaml_utils visualizers : The visualizers module offers a way of constructing and displaying visualizers.base_pipeline_run_visualizer visualizers.base_pipeline_visualizer visualizers.base_step_visualizer visualizers.base_visualizer","title":"Modules"},{"location":"#classes","text":"base_artifact_store.BaseArtifactStore : Base class for all ZenML Artifact Store. local_artifact_store.LocalArtifactStore : Artifact Store for local artifacts. base_artifact.BaseArtifact : Base class for all ZenML artifacts. data_analysis_artifact.DataAnalysisArtifact : Class for all ZenML data analysis artifacts. data_artifact.DataArtifact : Class for all ZenML data artifacts. model_artifact.ModelArtifact : Class for all ZenML model artifacts. schema_artifact.SchemaArtifact : Class for all ZenML schema artifacts. statistics_artifact.StatisticsArtifact : Class for all ZenML statistics artifacts. type_registry.ArtifactTypeRegistry : A registry to keep track of which datatypes map to which artifact example.Example : Class for all example objects. example.ExamplesRepo : Class for the examples repository object. example.GitExamplesHandler : Class for the GitExamplesHandler that interfaces with the CLI tool. example.LocalExample : Class to encapsulate all properties and methods of the local example config_keys.ConfigKeys : Class to validate dictionary configurations. config_keys.PipelineConfigurationKeys : Keys for a pipeline configuration dict. config_keys.StepConfigurationKeys : Keys for a step configuration dict. global_config.GlobalConfig : Class definition for the global config. base_container_registry.BaseContainerRegistry : Base class for all ZenML container registries. base_component.BaseComponent : Class definition for the base config. component_factory.ComponentFactory : Definition of ComponentFactory to track all BaseComponent subclasses. git_wrapper.GitWrapper : Wrapper class for Git. local_service.LocalService : Definition of a local service that keeps track of all ZenML mapping_utils.UUIDSourceTuple : Container used to store UUID and source information repo.Repository : ZenML repository definition. enums.ArtifactStoreTypes : All supported Artifact Store types. enums.ExecutionStatus : Enum that represents the current status of a step or pipeline run. enums.LoggingLevels : Enum for logging levels. enums.MLMetadataTypes : All supported ML Metadata types. enums.OrchestratorTypes : All supported Orchestrator types enums.StackTypes : All supported Stack types. exceptions.AlreadyExistsException : Raises exception when the name already exist in the system but an exceptions.ArtifactInterfaceError : Raises exception when interacting with the Artifact interface exceptions.DoesNotExistException : Raises exception when the entity does not exist in the system but an exceptions.DuplicateRunNameError : Raises exception when a run with the same name already exists. exceptions.EmptyDatasourceException : Raises exception when a datasource data is accessed without running exceptions.GitException : Raises exception when a problem occurs in git resolution. exceptions.InitializationException : Raises exception when a function is run before zenml initialization. exceptions.IntegrationError : Raises exceptions when a requested integration can not be activated. exceptions.MissingStepParameterError : Raises exceptions when a step parameter is missing when running a exceptions.PipelineConfigurationError : Raises exceptions when a pipeline configuration contains exceptions.PipelineInterfaceError : Raises exception when interacting with the Pipeline interface exceptions.PipelineNotSucceededException : Raises exception when trying to fetch artifacts from a not succeeded exceptions.StepContextError : Raises exception when interacting with a StepContext exceptions.StepInterfaceError : Raises exception when interacting with the Step interface airflow.AirflowIntegration : Definition of Airflow Integration for ZenML. airflow_component.AirflowComponent : Airflow-specific TFX Component. airflow_dag_runner.AirflowDagRunner : Tfx runner on Airflow. airflow_dag_runner.AirflowPipelineConfig : Pipeline config for AirflowDagRunner. airflow_orchestrator.AirflowOrchestrator : Orchestrator responsible for running pipelines using Airflow. dash.DashIntegration : Definition of Dash integration for ZenML. pipeline_run_lineage_visualizer.PipelineRunLineageVisualizer : Implementation of a lineage diagram via the [dash]( evidently.EvidentlyIntegration : Definition of Evidently integration evidently_profile.EvidentlyProfileConfig : Config class for Evidently profile steps. evidently_profile.EvidentlyProfileStep : Simple step implementation which implements Evidently's functionality for evidently_visualizer.EvidentlyVisualizer : The implementation of an Evidently Visualizer. facets.FacetsIntegration : Definition of Facet integration facet_statistics_visualizer.FacetStatisticsVisualizer : The base implementation of a ZenML Visualizer. gcp.GcpIntegration : Definition of Google Cloud Platform integration for ZenML. gcp_artifact_store.GCPArtifactStore : Artifact Store for Google Cloud Storage based artifacts. gcs_plugin.ZenGCS : Filesystem that delegates to Google Cloud Store using gcsfs. graphviz.GraphvizIntegration : Definition of Graphviz integration for ZenML. pipeline_run_dag_visualizer.PipelineRunDagVisualizer : Visualize the lineage of runs in a pipeline. integration.Integration : Base class for integration in ZenML integration.IntegrationMeta : Metaclass responsible for registering different Integration kubeflow.KubeflowIntegration : Definition of Kubeflow Integration for ZenML. kubeflow_metadata_store.KubeflowMetadataStore : Kubeflow MySQL backend for ZenML metadata store. kubeflow_component.KubeflowComponent : Base component for all Kubeflow pipelines TFX components. kubeflow_dag_runner.KubeflowDagRunner : Kubeflow Pipelines runner. kubeflow_dag_runner.KubeflowDagRunnerConfig : Runtime configuration parameters specific to execution on Kubeflow. kubeflow_orchestrator.KubeflowOrchestrator : Orchestrator responsible for running pipelines using Kubeflow. mlflow.MlflowIntegration : Definition of Plotly integration for ZenML. plotly.PlotlyIntegration : Definition of Plotly integration for ZenML. pipeline_lineage_visualizer.PipelineLineageVisualizer : Visualize the lineage of runs in a pipeline using plotly. pytorch.PytorchIntegration : Definition of PyTorch integration for ZenML. pytorch_materializer.PyTorchMaterializer : Materializer to read/write Pytorch models. pytorch_types.TorchDict : A type of dict that represents saving a model. pytorch_lightning.PytorchLightningIntegration : Definition of PyTorch Lightning integration for ZenML. pytorch_lightning_materializer.PyTorchLightningMaterializer : Materializer to read/write Pytorch models. registry.IntegrationRegistry : Registry to keep track of ZenML Integrations sklearn.SklearnIntegration : Definition of sklearn integration for ZenML. sklearn_materializer.SklearnMaterializer : Materializer to read data to and from sklearn. sklearn_evaluator.SklearnEvaluator : A simple step implementation which utilizes sklearn to evaluate the sklearn_evaluator.SklearnEvaluatorConfig : Config class for the sklearn evaluator sklearn_splitter.SklearnSplitter : A simple step implementation which utilizes sklearn to split a given sklearn_splitter.SklearnSplitterConfig : Config class for the sklearn splitter sklearn_standard_scaler.SklearnStandardScaler : Simple step implementation which utilizes the StandardScaler from sklearn sklearn_standard_scaler.SklearnStandardScalerConfig : Config class for the sklearn standard scaler tensorflow.TensorflowIntegration : Definition of Tensorflow integration for ZenML. keras_materializer.KerasMaterializer : Materializer to read/write Keras models. tf_dataset_materializer.TensorflowDatasetMaterializer : Materializer to read data to and from tf.data.Dataset. tensorflow_trainer.TensorflowBinaryClassifier : Simple step implementation which creates a simple tensorflow feedforward tensorflow_trainer.TensorflowBinaryClassifierConfig : Config class for the tensorflow trainer io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation fileio_registry.FileIORegistry : Registry of pluggable filesystem implementations used in TFX components. filesystem.FileSystemMeta : Metaclass which is responsible for registering the defined filesystem filesystem.Filesystem : Abstract Filesystem class. filesystem.NotFoundError : Auxiliary not found error logger.CustomFormatter : Formats logs according to custom specifications. base_materializer.BaseMaterializer : Base Materializer to realize artifact data. base_materializer.BaseMaterializerMeta : Metaclass responsible for registering different BaseMaterializer beam_materializer.BeamMaterializer : Materializer to read data to and from beam. built_in_materializer.BuiltInMaterializer : Read/Write JSON files. default_materializer_registry.MaterializerRegistry : Matches a python type to a default materializer. numpy_materializer.NumpyMaterializer : Materializer to read data to and from pandas. pandas_materializer.PandasMaterializer : Materializer to read data to and from pandas. base_metadata_store.BaseMetadataStore : Metadata store base class to track metadata of zenml first class mysql_metadata_store.MySQLMetadataStore : MySQL backend for ZenML metadata store. sqlite_metadata_store.SQLiteMetadataStore : SQLite backend for ZenML metadata store. base_orchestrator.BaseOrchestrator : Base Orchestrator class to orchestrate ZenML pipelines. local_dag_runner.LocalDagRunner : Local TFX DAG runner. local_orchestrator.LocalOrchestrator : Orchestrator responsible for running pipelines locally. base_pipeline.BasePipeline : Abstract base class for all ZenML pipelines. base_pipeline.BasePipelineMeta : Pipeline Metaclass responsible for validating the pipeline definition. training_pipeline.TrainingPipeline : Class for the classic training pipeline implementation artifact.ArtifactView : Post-execution artifact class which can be used to read pipeline.PipelineView : Post-execution pipeline class which can be used to query pipeline_run.PipelineRunView : Post-execution pipeline run class which can be used to query step.StepView : Post-execution step class which can be used to query base_stack.BaseStack : Base stack for ZenML. base_step.BaseStep : Abstract base class for all ZenML steps. base_step.BaseStepMeta : Metaclass for BaseStep . base_step_config.BaseStepConfig : Base configuration class to pass execution params into a step. pandas_analyzer.PandasAnalyzer : Simple step implementation which analyzes a given pd.DataFrame pandas_analyzer.PandasAnalyzerConfig : Config class for the PandasAnalyzer Config pandas_datasource.PandasDatasource : Simple step implementation to ingest from a csv file using pandas pandas_datasource.PandasDatasourceConfig : Config class for the pandas csv datasource step_context.StepContext : Provides additional context inside a step function. step_context.StepContextOutput : Tuple containing materializer class and artifact for a step output. base_analyzer_step.BaseAnalyzerConfig : Base class for analyzer step configurations base_analyzer_step.BaseAnalyzerStep : Base step implementation for any analyzer step implementation on ZenML base_datasource_step.BaseDatasourceConfig : Base class for datasource configs to inherit from base_datasource_step.BaseDatasourceStep : Base step implementation for any datasource step implementation on ZenML base_drift_detection_step.BaseDriftDetectionConfig : Base class for drift detection step configurations base_drift_detection_step.BaseDriftDetectionStep : Base step implementation for any drift detection step implementation base_evaluator_step.BaseEvaluatorConfig : Base class for evaluator step configurations base_evaluator_step.BaseEvaluatorStep : Base step implementation for any evaluator step implementation on ZenML base_preprocessor_step.BasePreprocessorConfig : Base class for Preprocessor step configurations base_preprocessor_step.BasePreprocessorStep : Base step implementation for any Preprocessor step implementation on base_split_step.BaseSplitStep : Base step implementation for any split step implementation on ZenML base_split_step.BaseSplitStepConfig : Base class for split configs to inherit from base_trainer_step.BaseTrainerConfig : Base class for Trainer step configurations base_trainer_step.BaseTrainerStep : Base step implementation for any Trainer step implementation on step_output.Output : A named tuple with a default name that cannot be overridden. base_pipeline_run_visualizer.BasePipelineRunVisualizer : The base implementation of a ZenML Pipeline Run Visualizer. base_pipeline_visualizer.BasePipelineVisualizer : The base implementation of a ZenML Pipeline Visualizer. base_step_visualizer.BaseStepVisualizer : The base implementation of a ZenML Step Visualizer. base_visualizer.BaseVisualizer : Base class for all ZenML Visualizers.","title":"Classes"},{"location":"#functions","text":"example.check_for_version_mismatch utils.activate_integrations : Decorator that activates all ZenML integrations. utils.confirmation : Echo a confirmation string on the CLI. utils.declare : Echo a declaration on the CLI. utils.error : Echo an error string on the CLI. utils.format_component_list : Formats a list of components into a List of Dicts. This list of dicts utils.format_date : Format a date into a string. utils.install_package : Installs pypi package into the current environment with pip utils.parse_unknown_options : Parse unknown options from the CLI. utils.pretty_print : Pretty print an object on the CLI. utils.print_component_properties : Prints the properties of a component. utils.print_table : Echoes the list of dicts in a table format. The input object should be a utils.title : Echo a title formatted string on the CLI. utils.uninstall_package : Uninstalls pypi package from the current environment with pip utils.warning : Echo a warning string on the CLI. constants.handle_bool_env_var : Converts normal env var to boolean constants.handle_int_env_var : Converts normal env var to int mapping_utils.get_component_from_key : Given a key and a mapping, return an initialized component. mapping_utils.get_components_from_store : Returns a list of components from a store. mapping_utils.get_key_from_uuid : Return the key that points to a certain uuid in a mapping. utils.define_json_config_settings_source : Define a function to essentially deserialize a model from a serialized utils.generate_customise_sources : Generate a customise_sources function as defined here: container_entrypoint.main : Runs a single step defined by the command line arguments. docker_utils.build_docker_image : Builds a docker image. docker_utils.create_custom_build_context : Creates a docker build context. docker_utils.generate_dockerfile_contents : Generates a Dockerfile. docker_utils.get_current_environment_requirements : Returns a dict of package requirements for the environment that docker_utils.get_image_digest : Gets the digest of a docker image. docker_utils.push_docker_image : Pushes a docker image to a container registry. kubeflow_dag_runner.get_default_pipeline_operator_funcs : Returns a default list of pipeline operator functions. kubeflow_dag_runner.get_default_pod_labels : Returns the default pod label dict for Kubeflow. kubeflow_utils.replace_placeholder : Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. local_deployment_utils.check_prerequisites : Checks whether all prerequisites for a local kubeflow pipelines local_deployment_utils.create_k3d_cluster : Creates a K3D cluster. local_deployment_utils.delete_k3d_cluster : Deletes a K3D cluster with the given name. local_deployment_utils.deploy_kubeflow_pipelines : Deploys Kubeflow Pipelines. local_deployment_utils.k3d_cluster_exists : Checks whether there exists a K3D cluster with the given name. local_deployment_utils.kubeflow_pipelines_ready : Returns whether all Kubeflow Pipelines pods are ready. local_deployment_utils.start_kfp_ui_daemon : Starts a daemon process that forwards ports so the Kubeflow Pipelines local_deployment_utils.write_local_registry_yaml : Writes a K3D registry config file. mlflow_utils.enable_mlflow : Outer decorator function for the creation of a ZenML pipeline with mlflow mlflow_utils.enable_mlflow_init : Outer decorator function for extending the init method for pipelines mlflow_utils.enable_mlflow_run : Outer decorator function for extending the run method for pipelines mlflow_utils.local_mlflow_backend : Returns the local mlflow backend inside the global zenml directory mlflow_utils.setup_mlflow : Setup all mlflow related configurations. This includes specifying which digits.get_digits : Returns the digits dataset in the form of a tuple of numpy digits.get_digits_model : Creates a support vector classifier for digits dataset. utils.get_integration_for_module : Gets the integration class for a module inside an integration. utils.get_requirements_for_module : Gets requirements for a module inside an integration. fileio.append_file : Appends file_contents to file. fileio.convert_to_str : Converts a PathType to a str using UTF-8. fileio.copy : Copy a file from the source to the destination. fileio.copy_dir : Copies dir from source to destination. fileio.create_dir_if_not_exists : Creates directory if it does not exist. fileio.create_dir_recursive_if_not_exists : Creates directory recursively if it does not exist. fileio.create_file_if_not_exists : Creates file if it does not exist. fileio.file_exists : Returns True if the given path exists. fileio.find_files : Find files in a directory that match pattern. fileio.get_grandparent : Get grandparent of dir. fileio.get_parent : Get parent of dir. fileio.glob : Return the paths that match a glob pattern. fileio.is_dir : Returns whether the given path points to a directory. fileio.is_remote : Returns True if path exists remotely. fileio.is_root : Returns true if path has no parent in local filesystem. fileio.list_dir : Returns a list of files under dir. fileio.make_dirs : Make a directory at the given path, recursively creating parents. fileio.mkdir : Make a directory at the given path; parent directory must exist. fileio.move : Moves dir or file from source to destination. Can be used to rename. fileio.open : Open a file at the given path. fileio.remove : Remove the file at the given path. Dangerous operation. fileio.rename : Rename source file to destination file. fileio.resolve_relative_path : Takes relative path and resolves it absolutely. fileio.rm_dir : Deletes dir recursively. Dangerous operation. fileio.stat : Return the stat descriptor for a given file path. fileio.walk : Return an iterator that walks the contents of the given directory. utils.create_tarfile : Create a compressed representation of source_dir. utils.extract_tarfile : Extracts all files in a compressed tar file to output_dir. utils.get_global_config_directory : Returns the global config directory for ZenML. utils.get_zenml_config_dir : Recursive function to find the zenml config starting from path. utils.get_zenml_dir : Returns path to a ZenML repository directory. utils.is_gcs_path : Returns True if path is on Google Cloud Storage. utils.is_zenml_dir : Check if dir is a zenml dir or not. utils.read_file_contents_as_string : Reads contents of file. utils.write_file_contents_as_string : Writes contents of file. logger.get_console_handler : Get console handler for logging. logger.get_file_handler : Return a file handler for logging. logger.get_logger : Main function to get logger name,. logger.get_logging_level : Get logging level from the env variable. logger.init_logging : Initialize logging with default levels. logger.set_root_verbosity : Set the root verbosity. utils.create_tfx_pipeline : Creates a tfx pipeline from a ZenML pipeline. utils.execute_step : Executes a tfx component. pipeline_decorator.pipeline : Outer decorator function for the creation of a ZenML pipeline step_decorator.step : Outer decorator function for the creation of a ZenML step utils.do_types_match : Check whether type_a and type_b match. utils.generate_component_class : Generates a TFX component class for a ZenML step. utils.generate_component_spec_class : Generates a TFX component spec class for a ZenML step. analytics_utils.get_environment : Returns a string representing the execution environment of the pipeline. analytics_utils.get_segment_key : Get key for authorizing to Segment backend. analytics_utils.get_system_info : Returns system info as a dict. analytics_utils.in_docker : Returns: True if running in a Docker container, else False analytics_utils.in_google_colab : Returns: True if running in a Google Colab env, else False analytics_utils.in_paperspace_gradient : Returns: True if running in a Paperspace Gradient env, else False analytics_utils.parametrized : This is a meta-decorator, that is, a decorator for decorators. analytics_utils.layer : Internal layer analytics_utils.track_event : Track segment event if user opted-in. daemon.check_if_daemon_is_running : Checks whether a daemon process indicated by the PID file is running. daemon.run_as_daemon : Runs a function as a daemon process. daemon.stop_daemon : Stops a daemon process. networking_utils.find_available_port : Finds a local unoccupied port. networking_utils.port_available : Checks if a local port is available. source_utils.create_zenml_pin : Creates a ZenML pin for source pinning from release version. source_utils.get_absolute_path_from_module_source : Get a directory path from module source. source_utils.get_class_source_from_source : Gets class source from source, i.e. module.path@version, returns version. source_utils.get_module_source_from_class : Takes class input and returns module_source. If class is already string source_utils.get_module_source_from_file_path : Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py source_utils.get_module_source_from_source : Gets module source from source. E.g. some.module.file.class@version , source_utils.get_relative_path_from_module_source : Get a directory path from module, relative to root of repository. source_utils.import_class_by_path : Imports a class based on a given path source_utils.import_python_file : Imports a python file. source_utils.is_inside_repository : Returns whether a file is inside a zenml repository. source_utils.is_standard_pin : Returns True if pin is valid ZenML pin, else False. source_utils.is_standard_source : Returns True if source is a standard ZenML source. source_utils.is_third_party_module : Returns whether a file belongs to a third party package. source_utils.load_source_path_class : Loads a Python class from the source. source_utils.resolve_class : Resolves a class into a serializable source string. source_utils.resolve_standard_source : Creates a ZenML pin for source pinning from release version. string_utils.get_human_readable_filesize : Convert a file size in bytes into a human-readable string. string_utils.get_human_readable_time : Convert seconds into a human-readable string. yaml_utils.is_yaml : Returns True if file_path is YAML, else False yaml_utils.read_json : Read JSON on file path and returns contents as dict. yaml_utils.read_yaml : Read YAML on file path and returns contents as dict. yaml_utils.write_json : Write contents as JSON format to file_path. yaml_utils.write_yaml : Write contents as YAML format to file_path. This file was automatically generated via lazydocs .","title":"Functions"},{"location":"OVERVIEW/","text":"API Overview Modules artifact_stores : An artifact store is a place where artifacts are stored. These artifacts may artifact_stores.base_artifact_store : Definition of an Artifact Store artifact_stores.local_artifact_store artifacts : Artifacts are the data that power your experimentation and model training. It is artifacts.base_artifact : The below code is copied from the TFX source repo with minor changes. artifacts.constants artifacts.data_analysis_artifact artifacts.data_artifact artifacts.model_artifact artifacts.schema_artifact artifacts.statistics_artifact artifacts.type_registry cli : ZenML CLI cli.artifact_store cli.base cli.cli : .. currentmodule:: ce_cli.cli cli.config : CLI for manipulating ZenML local and global config file. cli.container_registry cli.example cli.integration cli.metadata_store cli.orchestrator cli.pipeline : CLI to interact with pipelines. cli.stack : CLI for manipulating ZenML local and global config file. cli.utils cli.version config : The config module contains classes and functions that manage user-specific config.config_keys config.constants config.global_config : Global config for the ZenML installation. constants container_registries container_registries.base_container_registry : Base class for all container registries. core : The core module is where all the base ZenML functionality is defined, core.base_component core.component_factory : Factory to register all components. core.constants core.git_wrapper : Wrapper class to handle Git integration core.local_service core.mapping_utils core.repo : Base ZenML repository core.utils enums exceptions : ZenML specific exception definitions integrations : The ZenML integrations module contains sub-modules for each integration that we integrations.airflow : The Airflow integration sub-module powers an alternative to the local integrations.airflow.orchestrators integrations.airflow.orchestrators.airflow_component : Definition for Airflow component for TFX. integrations.airflow.orchestrators.airflow_dag_runner : Definition of Airflow TFX runner. This is an unmodified copy from the TFX integrations.airflow.orchestrators.airflow_orchestrator integrations.constants integrations.dash integrations.dash.visualizers integrations.dash.visualizers.pipeline_run_lineage_visualizer integrations.evidently : The Evidently integration provides a way to monitor your models in production. integrations.evidently.steps integrations.evidently.steps.evidently_profile integrations.evidently.visualizers integrations.evidently.visualizers.evidently_visualizer integrations.facets : The Facets integration provides a simple way to visualize post-execution objects integrations.facets.visualizers integrations.facets.visualizers.facet_statistics_visualizer integrations.gcp : The GCP integration submodule provides a way to run ZenML pipelines in a cloud integrations.gcp.artifact_stores integrations.gcp.artifact_stores.gcp_artifact_store integrations.gcp.io integrations.gcp.io.gcs_plugin : Plugin which is created to add Google Cloud Store support to ZenML. integrations.graphviz integrations.graphviz.visualizers integrations.graphviz.visualizers.pipeline_run_dag_visualizer integrations.integration integrations.kubeflow : The Kubeflow integration sub-module powers an alternative to the local integrations.kubeflow.container_entrypoint : Main entrypoint for containers with Kubeflow TFX component executors. integrations.kubeflow.docker_utils integrations.kubeflow.metadata integrations.kubeflow.metadata.kubeflow_metadata_store integrations.kubeflow.orchestrators integrations.kubeflow.orchestrators.kubeflow_component : Kubeflow Pipelines based implementation of TFX components. integrations.kubeflow.orchestrators.kubeflow_dag_runner : The below code is copied from the TFX source repo with minor changes. integrations.kubeflow.orchestrators.kubeflow_orchestrator integrations.kubeflow.orchestrators.kubeflow_utils : Common utility for Kubeflow-based orchestrator. integrations.kubeflow.orchestrators.local_deployment_utils integrations.mlflow : The mlflow integrations currently enables you to use mlflow tracking as a integrations.mlflow.mlflow_utils integrations.plotly integrations.plotly.visualizers integrations.plotly.visualizers.pipeline_lineage_visualizer integrations.pytorch integrations.pytorch.materializers integrations.pytorch.materializers.pytorch_materializer integrations.pytorch.materializers.pytorch_types integrations.pytorch_lightning integrations.pytorch_lightning.materializers integrations.pytorch_lightning.materializers.pytorch_lightning_materializer integrations.registry integrations.sklearn integrations.sklearn.helpers integrations.sklearn.helpers.digits integrations.sklearn.materializers integrations.sklearn.materializers.sklearn_materializer integrations.sklearn.steps integrations.sklearn.steps.sklearn_evaluator integrations.sklearn.steps.sklearn_splitter integrations.sklearn.steps.sklearn_standard_scaler integrations.tensorflow integrations.tensorflow.materializers integrations.tensorflow.materializers.keras_materializer integrations.tensorflow.materializers.tf_dataset_materializer integrations.tensorflow.steps integrations.tensorflow.steps.tensorflow_trainer integrations.utils io : The io module handles file operations for the ZenML package. It offers a io.fileio io.fileio_registry : Filesystem registry managing filesystem plugins. io.filesystem io.utils logger materializers : Materializers are used to convert a ZenML artifact into a specific format. They materializers.base_materializer materializers.beam_materializer materializers.built_in_materializer materializers.default_materializer_registry materializers.numpy_materializer materializers.pandas_materializer metadata_stores : The configuration of each pipeline, step, backend, and produced artifacts are metadata_stores.base_metadata_store metadata_stores.mysql_metadata_store metadata_stores.sqlite_metadata_store orchestrators : An orchestrator is a special kind of backend that manages the running of each orchestrators.base_orchestrator orchestrators.local orchestrators.local.local_dag_runner : Inspired by local dag runner implementation orchestrators.local.local_orchestrator orchestrators.utils pipelines : A ZenML pipeline is a sequence of tasks that execute in a specific order and pipelines.base_pipeline pipelines.builtin_pipelines pipelines.builtin_pipelines.training_pipeline pipelines.pipeline_decorator post_execution : After executing a pipeline, the user needs to be able to fetch it from history post_execution.artifact post_execution.pipeline post_execution.pipeline_run post_execution.step stacks : A stack is made up of the following three core components: an Artifact Store, a stacks.base_stack stacks.constants steps : A step is a single piece or stage of a ZenML pipeline. Think of each step as steps.base_step steps.base_step_config steps.builtin_steps steps.builtin_steps.pandas_analyzer steps.builtin_steps.pandas_datasource steps.step_context steps.step_decorator steps.step_interfaces steps.step_interfaces.base_analyzer_step steps.step_interfaces.base_datasource_step steps.step_interfaces.base_drift_detection_step steps.step_interfaces.base_evaluator_step steps.step_interfaces.base_preprocessor_step steps.step_interfaces.base_split_step steps.step_interfaces.base_trainer_step steps.step_output steps.utils : The collection of utility functions/classes are inspired by their original utils : The utils module contains utility functions handling analytics, reading and utils.analytics_utils : Analytics code for ZenML utils.daemon : Utility functions to start/stop daemon processes. utils.networking_utils utils.source_utils : These utils are predicated on the following definitions: utils.string_utils utils.yaml_utils visualizers : The visualizers module offers a way of constructing and displaying visualizers.base_pipeline_run_visualizer visualizers.base_pipeline_visualizer visualizers.base_step_visualizer visualizers.base_visualizer Classes base_artifact_store.BaseArtifactStore : Base class for all ZenML Artifact Store. local_artifact_store.LocalArtifactStore : Artifact Store for local artifacts. base_artifact.BaseArtifact : Base class for all ZenML artifacts. data_analysis_artifact.DataAnalysisArtifact : Class for all ZenML data analysis artifacts. data_artifact.DataArtifact : Class for all ZenML data artifacts. model_artifact.ModelArtifact : Class for all ZenML model artifacts. schema_artifact.SchemaArtifact : Class for all ZenML schema artifacts. statistics_artifact.StatisticsArtifact : Class for all ZenML statistics artifacts. type_registry.ArtifactTypeRegistry : A registry to keep track of which datatypes map to which artifact example.Example : Class for all example objects. example.ExamplesRepo : Class for the examples repository object. example.GitExamplesHandler : Class for the GitExamplesHandler that interfaces with the CLI tool. example.LocalExample : Class to encapsulate all properties and methods of the local example config_keys.ConfigKeys : Class to validate dictionary configurations. config_keys.PipelineConfigurationKeys : Keys for a pipeline configuration dict. config_keys.StepConfigurationKeys : Keys for a step configuration dict. global_config.GlobalConfig : Class definition for the global config. base_container_registry.BaseContainerRegistry : Base class for all ZenML container registries. base_component.BaseComponent : Class definition for the base config. component_factory.ComponentFactory : Definition of ComponentFactory to track all BaseComponent subclasses. git_wrapper.GitWrapper : Wrapper class for Git. local_service.LocalService : Definition of a local service that keeps track of all ZenML mapping_utils.UUIDSourceTuple : Container used to store UUID and source information repo.Repository : ZenML repository definition. enums.ArtifactStoreTypes : All supported Artifact Store types. enums.ExecutionStatus : Enum that represents the current status of a step or pipeline run. enums.LoggingLevels : Enum for logging levels. enums.MLMetadataTypes : All supported ML Metadata types. enums.OrchestratorTypes : All supported Orchestrator types enums.StackTypes : All supported Stack types. exceptions.AlreadyExistsException : Raises exception when the name already exist in the system but an exceptions.ArtifactInterfaceError : Raises exception when interacting with the Artifact interface exceptions.DoesNotExistException : Raises exception when the entity does not exist in the system but an exceptions.DuplicateRunNameError : Raises exception when a run with the same name already exists. exceptions.EmptyDatasourceException : Raises exception when a datasource data is accessed without running exceptions.GitException : Raises exception when a problem occurs in git resolution. exceptions.InitializationException : Raises exception when a function is run before zenml initialization. exceptions.IntegrationError : Raises exceptions when a requested integration can not be activated. exceptions.MissingStepParameterError : Raises exceptions when a step parameter is missing when running a exceptions.PipelineConfigurationError : Raises exceptions when a pipeline configuration contains exceptions.PipelineInterfaceError : Raises exception when interacting with the Pipeline interface exceptions.PipelineNotSucceededException : Raises exception when trying to fetch artifacts from a not succeeded exceptions.StepContextError : Raises exception when interacting with a StepContext exceptions.StepInterfaceError : Raises exception when interacting with the Step interface airflow.AirflowIntegration : Definition of Airflow Integration for ZenML. airflow_component.AirflowComponent : Airflow-specific TFX Component. airflow_dag_runner.AirflowDagRunner : Tfx runner on Airflow. airflow_dag_runner.AirflowPipelineConfig : Pipeline config for AirflowDagRunner. airflow_orchestrator.AirflowOrchestrator : Orchestrator responsible for running pipelines using Airflow. dash.DashIntegration : Definition of Dash integration for ZenML. pipeline_run_lineage_visualizer.PipelineRunLineageVisualizer : Implementation of a lineage diagram via the [dash]( evidently.EvidentlyIntegration : Definition of Evidently integration evidently_profile.EvidentlyProfileConfig : Config class for Evidently profile steps. evidently_profile.EvidentlyProfileStep : Simple step implementation which implements Evidently's functionality for evidently_visualizer.EvidentlyVisualizer : The implementation of an Evidently Visualizer. facets.FacetsIntegration : Definition of Facet integration facet_statistics_visualizer.FacetStatisticsVisualizer : The base implementation of a ZenML Visualizer. gcp.GcpIntegration : Definition of Google Cloud Platform integration for ZenML. gcp_artifact_store.GCPArtifactStore : Artifact Store for Google Cloud Storage based artifacts. gcs_plugin.ZenGCS : Filesystem that delegates to Google Cloud Store using gcsfs. graphviz.GraphvizIntegration : Definition of Graphviz integration for ZenML. pipeline_run_dag_visualizer.PipelineRunDagVisualizer : Visualize the lineage of runs in a pipeline. integration.Integration : Base class for integration in ZenML integration.IntegrationMeta : Metaclass responsible for registering different Integration kubeflow.KubeflowIntegration : Definition of Kubeflow Integration for ZenML. kubeflow_metadata_store.KubeflowMetadataStore : Kubeflow MySQL backend for ZenML metadata store. kubeflow_component.KubeflowComponent : Base component for all Kubeflow pipelines TFX components. kubeflow_dag_runner.KubeflowDagRunner : Kubeflow Pipelines runner. kubeflow_dag_runner.KubeflowDagRunnerConfig : Runtime configuration parameters specific to execution on Kubeflow. kubeflow_orchestrator.KubeflowOrchestrator : Orchestrator responsible for running pipelines using Kubeflow. mlflow.MlflowIntegration : Definition of Plotly integration for ZenML. plotly.PlotlyIntegration : Definition of Plotly integration for ZenML. pipeline_lineage_visualizer.PipelineLineageVisualizer : Visualize the lineage of runs in a pipeline using plotly. pytorch.PytorchIntegration : Definition of PyTorch integration for ZenML. pytorch_materializer.PyTorchMaterializer : Materializer to read/write Pytorch models. pytorch_types.TorchDict : A type of dict that represents saving a model. pytorch_lightning.PytorchLightningIntegration : Definition of PyTorch Lightning integration for ZenML. pytorch_lightning_materializer.PyTorchLightningMaterializer : Materializer to read/write Pytorch models. registry.IntegrationRegistry : Registry to keep track of ZenML Integrations sklearn.SklearnIntegration : Definition of sklearn integration for ZenML. sklearn_materializer.SklearnMaterializer : Materializer to read data to and from sklearn. sklearn_evaluator.SklearnEvaluator : A simple step implementation which utilizes sklearn to evaluate the sklearn_evaluator.SklearnEvaluatorConfig : Config class for the sklearn evaluator sklearn_splitter.SklearnSplitter : A simple step implementation which utilizes sklearn to split a given sklearn_splitter.SklearnSplitterConfig : Config class for the sklearn splitter sklearn_standard_scaler.SklearnStandardScaler : Simple step implementation which utilizes the StandardScaler from sklearn sklearn_standard_scaler.SklearnStandardScalerConfig : Config class for the sklearn standard scaler tensorflow.TensorflowIntegration : Definition of Tensorflow integration for ZenML. keras_materializer.KerasMaterializer : Materializer to read/write Keras models. tf_dataset_materializer.TensorflowDatasetMaterializer : Materializer to read data to and from tf.data.Dataset. tensorflow_trainer.TensorflowBinaryClassifier : Simple step implementation which creates a simple tensorflow feedforward tensorflow_trainer.TensorflowBinaryClassifierConfig : Config class for the tensorflow trainer io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation fileio_registry.FileIORegistry : Registry of pluggable filesystem implementations used in TFX components. filesystem.FileSystemMeta : Metaclass which is responsible for registering the defined filesystem filesystem.Filesystem : Abstract Filesystem class. filesystem.NotFoundError : Auxiliary not found error logger.CustomFormatter : Formats logs according to custom specifications. base_materializer.BaseMaterializer : Base Materializer to realize artifact data. base_materializer.BaseMaterializerMeta : Metaclass responsible for registering different BaseMaterializer beam_materializer.BeamMaterializer : Materializer to read data to and from beam. built_in_materializer.BuiltInMaterializer : Read/Write JSON files. default_materializer_registry.MaterializerRegistry : Matches a python type to a default materializer. numpy_materializer.NumpyMaterializer : Materializer to read data to and from pandas. pandas_materializer.PandasMaterializer : Materializer to read data to and from pandas. base_metadata_store.BaseMetadataStore : Metadata store base class to track metadata of zenml first class mysql_metadata_store.MySQLMetadataStore : MySQL backend for ZenML metadata store. sqlite_metadata_store.SQLiteMetadataStore : SQLite backend for ZenML metadata store. base_orchestrator.BaseOrchestrator : Base Orchestrator class to orchestrate ZenML pipelines. local_dag_runner.LocalDagRunner : Local TFX DAG runner. local_orchestrator.LocalOrchestrator : Orchestrator responsible for running pipelines locally. base_pipeline.BasePipeline : Abstract base class for all ZenML pipelines. base_pipeline.BasePipelineMeta : Pipeline Metaclass responsible for validating the pipeline definition. training_pipeline.TrainingPipeline : Class for the classic training pipeline implementation artifact.ArtifactView : Post-execution artifact class which can be used to read pipeline.PipelineView : Post-execution pipeline class which can be used to query pipeline_run.PipelineRunView : Post-execution pipeline run class which can be used to query step.StepView : Post-execution step class which can be used to query base_stack.BaseStack : Base stack for ZenML. base_step.BaseStep : Abstract base class for all ZenML steps. base_step.BaseStepMeta : Metaclass for BaseStep . base_step_config.BaseStepConfig : Base configuration class to pass execution params into a step. pandas_analyzer.PandasAnalyzer : Simple step implementation which analyzes a given pd.DataFrame pandas_analyzer.PandasAnalyzerConfig : Config class for the PandasAnalyzer Config pandas_datasource.PandasDatasource : Simple step implementation to ingest from a csv file using pandas pandas_datasource.PandasDatasourceConfig : Config class for the pandas csv datasource step_context.StepContext : Provides additional context inside a step function. step_context.StepContextOutput : Tuple containing materializer class and artifact for a step output. base_analyzer_step.BaseAnalyzerConfig : Base class for analyzer step configurations base_analyzer_step.BaseAnalyzerStep : Base step implementation for any analyzer step implementation on ZenML base_datasource_step.BaseDatasourceConfig : Base class for datasource configs to inherit from base_datasource_step.BaseDatasourceStep : Base step implementation for any datasource step implementation on ZenML base_drift_detection_step.BaseDriftDetectionConfig : Base class for drift detection step configurations base_drift_detection_step.BaseDriftDetectionStep : Base step implementation for any drift detection step implementation base_evaluator_step.BaseEvaluatorConfig : Base class for evaluator step configurations base_evaluator_step.BaseEvaluatorStep : Base step implementation for any evaluator step implementation on ZenML base_preprocessor_step.BasePreprocessorConfig : Base class for Preprocessor step configurations base_preprocessor_step.BasePreprocessorStep : Base step implementation for any Preprocessor step implementation on base_split_step.BaseSplitStep : Base step implementation for any split step implementation on ZenML base_split_step.BaseSplitStepConfig : Base class for split configs to inherit from base_trainer_step.BaseTrainerConfig : Base class for Trainer step configurations base_trainer_step.BaseTrainerStep : Base step implementation for any Trainer step implementation on step_output.Output : A named tuple with a default name that cannot be overridden. base_pipeline_run_visualizer.BasePipelineRunVisualizer : The base implementation of a ZenML Pipeline Run Visualizer. base_pipeline_visualizer.BasePipelineVisualizer : The base implementation of a ZenML Pipeline Visualizer. base_step_visualizer.BaseStepVisualizer : The base implementation of a ZenML Step Visualizer. base_visualizer.BaseVisualizer : Base class for all ZenML Visualizers. Functions example.check_for_version_mismatch utils.activate_integrations : Decorator that activates all ZenML integrations. utils.confirmation : Echo a confirmation string on the CLI. utils.declare : Echo a declaration on the CLI. utils.error : Echo an error string on the CLI. utils.format_component_list : Formats a list of components into a List of Dicts. This list of dicts utils.format_date : Format a date into a string. utils.install_package : Installs pypi package into the current environment with pip utils.parse_unknown_options : Parse unknown options from the CLI. utils.pretty_print : Pretty print an object on the CLI. utils.print_component_properties : Prints the properties of a component. utils.print_table : Echoes the list of dicts in a table format. The input object should be a utils.title : Echo a title formatted string on the CLI. utils.uninstall_package : Uninstalls pypi package from the current environment with pip utils.warning : Echo a warning string on the CLI. constants.handle_bool_env_var : Converts normal env var to boolean constants.handle_int_env_var : Converts normal env var to int mapping_utils.get_component_from_key : Given a key and a mapping, return an initialized component. mapping_utils.get_components_from_store : Returns a list of components from a store. mapping_utils.get_key_from_uuid : Return the key that points to a certain uuid in a mapping. utils.define_json_config_settings_source : Define a function to essentially deserialize a model from a serialized utils.generate_customise_sources : Generate a customise_sources function as defined here: container_entrypoint.main : Runs a single step defined by the command line arguments. docker_utils.build_docker_image : Builds a docker image. docker_utils.create_custom_build_context : Creates a docker build context. docker_utils.generate_dockerfile_contents : Generates a Dockerfile. docker_utils.get_current_environment_requirements : Returns a dict of package requirements for the environment that docker_utils.get_image_digest : Gets the digest of a docker image. docker_utils.push_docker_image : Pushes a docker image to a container registry. kubeflow_dag_runner.get_default_pipeline_operator_funcs : Returns a default list of pipeline operator functions. kubeflow_dag_runner.get_default_pod_labels : Returns the default pod label dict for Kubeflow. kubeflow_utils.replace_placeholder : Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. local_deployment_utils.check_prerequisites : Checks whether all prerequisites for a local kubeflow pipelines local_deployment_utils.create_k3d_cluster : Creates a K3D cluster. local_deployment_utils.delete_k3d_cluster : Deletes a K3D cluster with the given name. local_deployment_utils.deploy_kubeflow_pipelines : Deploys Kubeflow Pipelines. local_deployment_utils.k3d_cluster_exists : Checks whether there exists a K3D cluster with the given name. local_deployment_utils.kubeflow_pipelines_ready : Returns whether all Kubeflow Pipelines pods are ready. local_deployment_utils.start_kfp_ui_daemon : Starts a daemon process that forwards ports so the Kubeflow Pipelines local_deployment_utils.write_local_registry_yaml : Writes a K3D registry config file. mlflow_utils.enable_mlflow : Outer decorator function for the creation of a ZenML pipeline with mlflow mlflow_utils.enable_mlflow_init : Outer decorator function for extending the init method for pipelines mlflow_utils.enable_mlflow_run : Outer decorator function for extending the run method for pipelines mlflow_utils.local_mlflow_backend : Returns the local mlflow backend inside the global zenml directory mlflow_utils.setup_mlflow : Setup all mlflow related configurations. This includes specifying which digits.get_digits : Returns the digits dataset in the form of a tuple of numpy digits.get_digits_model : Creates a support vector classifier for digits dataset. utils.get_integration_for_module : Gets the integration class for a module inside an integration. utils.get_requirements_for_module : Gets requirements for a module inside an integration. fileio.append_file : Appends file_contents to file. fileio.convert_to_str : Converts a PathType to a str using UTF-8. fileio.copy : Copy a file from the source to the destination. fileio.copy_dir : Copies dir from source to destination. fileio.create_dir_if_not_exists : Creates directory if it does not exist. fileio.create_dir_recursive_if_not_exists : Creates directory recursively if it does not exist. fileio.create_file_if_not_exists : Creates file if it does not exist. fileio.file_exists : Returns True if the given path exists. fileio.find_files : Find files in a directory that match pattern. fileio.get_grandparent : Get grandparent of dir. fileio.get_parent : Get parent of dir. fileio.glob : Return the paths that match a glob pattern. fileio.is_dir : Returns whether the given path points to a directory. fileio.is_remote : Returns True if path exists remotely. fileio.is_root : Returns true if path has no parent in local filesystem. fileio.list_dir : Returns a list of files under dir. fileio.make_dirs : Make a directory at the given path, recursively creating parents. fileio.mkdir : Make a directory at the given path; parent directory must exist. fileio.move : Moves dir or file from source to destination. Can be used to rename. fileio.open : Open a file at the given path. fileio.remove : Remove the file at the given path. Dangerous operation. fileio.rename : Rename source file to destination file. fileio.resolve_relative_path : Takes relative path and resolves it absolutely. fileio.rm_dir : Deletes dir recursively. Dangerous operation. fileio.stat : Return the stat descriptor for a given file path. fileio.walk : Return an iterator that walks the contents of the given directory. utils.create_tarfile : Create a compressed representation of source_dir. utils.extract_tarfile : Extracts all files in a compressed tar file to output_dir. utils.get_global_config_directory : Returns the global config directory for ZenML. utils.get_zenml_config_dir : Recursive function to find the zenml config starting from path. utils.get_zenml_dir : Returns path to a ZenML repository directory. utils.is_gcs_path : Returns True if path is on Google Cloud Storage. utils.is_zenml_dir : Check if dir is a zenml dir or not. utils.read_file_contents_as_string : Reads contents of file. utils.write_file_contents_as_string : Writes contents of file. logger.get_console_handler : Get console handler for logging. logger.get_file_handler : Return a file handler for logging. logger.get_logger : Main function to get logger name,. logger.get_logging_level : Get logging level from the env variable. logger.init_logging : Initialize logging with default levels. logger.set_root_verbosity : Set the root verbosity. utils.create_tfx_pipeline : Creates a tfx pipeline from a ZenML pipeline. utils.execute_step : Executes a tfx component. pipeline_decorator.pipeline : Outer decorator function for the creation of a ZenML pipeline step_decorator.step : Outer decorator function for the creation of a ZenML step utils.do_types_match : Check whether type_a and type_b match. utils.generate_component_class : Generates a TFX component class for a ZenML step. utils.generate_component_spec_class : Generates a TFX component spec class for a ZenML step. analytics_utils.get_environment : Returns a string representing the execution environment of the pipeline. analytics_utils.get_segment_key : Get key for authorizing to Segment backend. analytics_utils.get_system_info : Returns system info as a dict. analytics_utils.in_docker : Returns: True if running in a Docker container, else False analytics_utils.in_google_colab : Returns: True if running in a Google Colab env, else False analytics_utils.in_paperspace_gradient : Returns: True if running in a Paperspace Gradient env, else False analytics_utils.parametrized : This is a meta-decorator, that is, a decorator for decorators. analytics_utils.layer : Internal layer analytics_utils.track_event : Track segment event if user opted-in. daemon.check_if_daemon_is_running : Checks whether a daemon process indicated by the PID file is running. daemon.run_as_daemon : Runs a function as a daemon process. daemon.stop_daemon : Stops a daemon process. networking_utils.find_available_port : Finds a local unoccupied port. networking_utils.port_available : Checks if a local port is available. source_utils.create_zenml_pin : Creates a ZenML pin for source pinning from release version. source_utils.get_absolute_path_from_module_source : Get a directory path from module source. source_utils.get_class_source_from_source : Gets class source from source, i.e. module.path@version, returns version. source_utils.get_module_source_from_class : Takes class input and returns module_source. If class is already string source_utils.get_module_source_from_file_path : Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py source_utils.get_module_source_from_source : Gets module source from source. E.g. some.module.file.class@version , source_utils.get_relative_path_from_module_source : Get a directory path from module, relative to root of repository. source_utils.import_class_by_path : Imports a class based on a given path source_utils.import_python_file : Imports a python file. source_utils.is_inside_repository : Returns whether a file is inside a zenml repository. source_utils.is_standard_pin : Returns True if pin is valid ZenML pin, else False. source_utils.is_standard_source : Returns True if source is a standard ZenML source. source_utils.is_third_party_module : Returns whether a file belongs to a third party package. source_utils.load_source_path_class : Loads a Python class from the source. source_utils.resolve_class : Resolves a class into a serializable source string. source_utils.resolve_standard_source : Creates a ZenML pin for source pinning from release version. string_utils.get_human_readable_filesize : Convert a file size in bytes into a human-readable string. string_utils.get_human_readable_time : Convert seconds into a human-readable string. yaml_utils.is_yaml : Returns True if file_path is YAML, else False yaml_utils.read_json : Read JSON on file path and returns contents as dict. yaml_utils.read_yaml : Read YAML on file path and returns contents as dict. yaml_utils.write_json : Write contents as JSON format to file_path. yaml_utils.write_yaml : Write contents as YAML format to file_path. This file was automatically generated via lazydocs .","title":"Overview"},{"location":"OVERVIEW/#api-overview","text":"","title":"API Overview"},{"location":"OVERVIEW/#modules","text":"artifact_stores : An artifact store is a place where artifacts are stored. These artifacts may artifact_stores.base_artifact_store : Definition of an Artifact Store artifact_stores.local_artifact_store artifacts : Artifacts are the data that power your experimentation and model training. It is artifacts.base_artifact : The below code is copied from the TFX source repo with minor changes. artifacts.constants artifacts.data_analysis_artifact artifacts.data_artifact artifacts.model_artifact artifacts.schema_artifact artifacts.statistics_artifact artifacts.type_registry cli : ZenML CLI cli.artifact_store cli.base cli.cli : .. currentmodule:: ce_cli.cli cli.config : CLI for manipulating ZenML local and global config file. cli.container_registry cli.example cli.integration cli.metadata_store cli.orchestrator cli.pipeline : CLI to interact with pipelines. cli.stack : CLI for manipulating ZenML local and global config file. cli.utils cli.version config : The config module contains classes and functions that manage user-specific config.config_keys config.constants config.global_config : Global config for the ZenML installation. constants container_registries container_registries.base_container_registry : Base class for all container registries. core : The core module is where all the base ZenML functionality is defined, core.base_component core.component_factory : Factory to register all components. core.constants core.git_wrapper : Wrapper class to handle Git integration core.local_service core.mapping_utils core.repo : Base ZenML repository core.utils enums exceptions : ZenML specific exception definitions integrations : The ZenML integrations module contains sub-modules for each integration that we integrations.airflow : The Airflow integration sub-module powers an alternative to the local integrations.airflow.orchestrators integrations.airflow.orchestrators.airflow_component : Definition for Airflow component for TFX. integrations.airflow.orchestrators.airflow_dag_runner : Definition of Airflow TFX runner. This is an unmodified copy from the TFX integrations.airflow.orchestrators.airflow_orchestrator integrations.constants integrations.dash integrations.dash.visualizers integrations.dash.visualizers.pipeline_run_lineage_visualizer integrations.evidently : The Evidently integration provides a way to monitor your models in production. integrations.evidently.steps integrations.evidently.steps.evidently_profile integrations.evidently.visualizers integrations.evidently.visualizers.evidently_visualizer integrations.facets : The Facets integration provides a simple way to visualize post-execution objects integrations.facets.visualizers integrations.facets.visualizers.facet_statistics_visualizer integrations.gcp : The GCP integration submodule provides a way to run ZenML pipelines in a cloud integrations.gcp.artifact_stores integrations.gcp.artifact_stores.gcp_artifact_store integrations.gcp.io integrations.gcp.io.gcs_plugin : Plugin which is created to add Google Cloud Store support to ZenML. integrations.graphviz integrations.graphviz.visualizers integrations.graphviz.visualizers.pipeline_run_dag_visualizer integrations.integration integrations.kubeflow : The Kubeflow integration sub-module powers an alternative to the local integrations.kubeflow.container_entrypoint : Main entrypoint for containers with Kubeflow TFX component executors. integrations.kubeflow.docker_utils integrations.kubeflow.metadata integrations.kubeflow.metadata.kubeflow_metadata_store integrations.kubeflow.orchestrators integrations.kubeflow.orchestrators.kubeflow_component : Kubeflow Pipelines based implementation of TFX components. integrations.kubeflow.orchestrators.kubeflow_dag_runner : The below code is copied from the TFX source repo with minor changes. integrations.kubeflow.orchestrators.kubeflow_orchestrator integrations.kubeflow.orchestrators.kubeflow_utils : Common utility for Kubeflow-based orchestrator. integrations.kubeflow.orchestrators.local_deployment_utils integrations.mlflow : The mlflow integrations currently enables you to use mlflow tracking as a integrations.mlflow.mlflow_utils integrations.plotly integrations.plotly.visualizers integrations.plotly.visualizers.pipeline_lineage_visualizer integrations.pytorch integrations.pytorch.materializers integrations.pytorch.materializers.pytorch_materializer integrations.pytorch.materializers.pytorch_types integrations.pytorch_lightning integrations.pytorch_lightning.materializers integrations.pytorch_lightning.materializers.pytorch_lightning_materializer integrations.registry integrations.sklearn integrations.sklearn.helpers integrations.sklearn.helpers.digits integrations.sklearn.materializers integrations.sklearn.materializers.sklearn_materializer integrations.sklearn.steps integrations.sklearn.steps.sklearn_evaluator integrations.sklearn.steps.sklearn_splitter integrations.sklearn.steps.sklearn_standard_scaler integrations.tensorflow integrations.tensorflow.materializers integrations.tensorflow.materializers.keras_materializer integrations.tensorflow.materializers.tf_dataset_materializer integrations.tensorflow.steps integrations.tensorflow.steps.tensorflow_trainer integrations.utils io : The io module handles file operations for the ZenML package. It offers a io.fileio io.fileio_registry : Filesystem registry managing filesystem plugins. io.filesystem io.utils logger materializers : Materializers are used to convert a ZenML artifact into a specific format. They materializers.base_materializer materializers.beam_materializer materializers.built_in_materializer materializers.default_materializer_registry materializers.numpy_materializer materializers.pandas_materializer metadata_stores : The configuration of each pipeline, step, backend, and produced artifacts are metadata_stores.base_metadata_store metadata_stores.mysql_metadata_store metadata_stores.sqlite_metadata_store orchestrators : An orchestrator is a special kind of backend that manages the running of each orchestrators.base_orchestrator orchestrators.local orchestrators.local.local_dag_runner : Inspired by local dag runner implementation orchestrators.local.local_orchestrator orchestrators.utils pipelines : A ZenML pipeline is a sequence of tasks that execute in a specific order and pipelines.base_pipeline pipelines.builtin_pipelines pipelines.builtin_pipelines.training_pipeline pipelines.pipeline_decorator post_execution : After executing a pipeline, the user needs to be able to fetch it from history post_execution.artifact post_execution.pipeline post_execution.pipeline_run post_execution.step stacks : A stack is made up of the following three core components: an Artifact Store, a stacks.base_stack stacks.constants steps : A step is a single piece or stage of a ZenML pipeline. Think of each step as steps.base_step steps.base_step_config steps.builtin_steps steps.builtin_steps.pandas_analyzer steps.builtin_steps.pandas_datasource steps.step_context steps.step_decorator steps.step_interfaces steps.step_interfaces.base_analyzer_step steps.step_interfaces.base_datasource_step steps.step_interfaces.base_drift_detection_step steps.step_interfaces.base_evaluator_step steps.step_interfaces.base_preprocessor_step steps.step_interfaces.base_split_step steps.step_interfaces.base_trainer_step steps.step_output steps.utils : The collection of utility functions/classes are inspired by their original utils : The utils module contains utility functions handling analytics, reading and utils.analytics_utils : Analytics code for ZenML utils.daemon : Utility functions to start/stop daemon processes. utils.networking_utils utils.source_utils : These utils are predicated on the following definitions: utils.string_utils utils.yaml_utils visualizers : The visualizers module offers a way of constructing and displaying visualizers.base_pipeline_run_visualizer visualizers.base_pipeline_visualizer visualizers.base_step_visualizer visualizers.base_visualizer","title":"Modules"},{"location":"OVERVIEW/#classes","text":"base_artifact_store.BaseArtifactStore : Base class for all ZenML Artifact Store. local_artifact_store.LocalArtifactStore : Artifact Store for local artifacts. base_artifact.BaseArtifact : Base class for all ZenML artifacts. data_analysis_artifact.DataAnalysisArtifact : Class for all ZenML data analysis artifacts. data_artifact.DataArtifact : Class for all ZenML data artifacts. model_artifact.ModelArtifact : Class for all ZenML model artifacts. schema_artifact.SchemaArtifact : Class for all ZenML schema artifacts. statistics_artifact.StatisticsArtifact : Class for all ZenML statistics artifacts. type_registry.ArtifactTypeRegistry : A registry to keep track of which datatypes map to which artifact example.Example : Class for all example objects. example.ExamplesRepo : Class for the examples repository object. example.GitExamplesHandler : Class for the GitExamplesHandler that interfaces with the CLI tool. example.LocalExample : Class to encapsulate all properties and methods of the local example config_keys.ConfigKeys : Class to validate dictionary configurations. config_keys.PipelineConfigurationKeys : Keys for a pipeline configuration dict. config_keys.StepConfigurationKeys : Keys for a step configuration dict. global_config.GlobalConfig : Class definition for the global config. base_container_registry.BaseContainerRegistry : Base class for all ZenML container registries. base_component.BaseComponent : Class definition for the base config. component_factory.ComponentFactory : Definition of ComponentFactory to track all BaseComponent subclasses. git_wrapper.GitWrapper : Wrapper class for Git. local_service.LocalService : Definition of a local service that keeps track of all ZenML mapping_utils.UUIDSourceTuple : Container used to store UUID and source information repo.Repository : ZenML repository definition. enums.ArtifactStoreTypes : All supported Artifact Store types. enums.ExecutionStatus : Enum that represents the current status of a step or pipeline run. enums.LoggingLevels : Enum for logging levels. enums.MLMetadataTypes : All supported ML Metadata types. enums.OrchestratorTypes : All supported Orchestrator types enums.StackTypes : All supported Stack types. exceptions.AlreadyExistsException : Raises exception when the name already exist in the system but an exceptions.ArtifactInterfaceError : Raises exception when interacting with the Artifact interface exceptions.DoesNotExistException : Raises exception when the entity does not exist in the system but an exceptions.DuplicateRunNameError : Raises exception when a run with the same name already exists. exceptions.EmptyDatasourceException : Raises exception when a datasource data is accessed without running exceptions.GitException : Raises exception when a problem occurs in git resolution. exceptions.InitializationException : Raises exception when a function is run before zenml initialization. exceptions.IntegrationError : Raises exceptions when a requested integration can not be activated. exceptions.MissingStepParameterError : Raises exceptions when a step parameter is missing when running a exceptions.PipelineConfigurationError : Raises exceptions when a pipeline configuration contains exceptions.PipelineInterfaceError : Raises exception when interacting with the Pipeline interface exceptions.PipelineNotSucceededException : Raises exception when trying to fetch artifacts from a not succeeded exceptions.StepContextError : Raises exception when interacting with a StepContext exceptions.StepInterfaceError : Raises exception when interacting with the Step interface airflow.AirflowIntegration : Definition of Airflow Integration for ZenML. airflow_component.AirflowComponent : Airflow-specific TFX Component. airflow_dag_runner.AirflowDagRunner : Tfx runner on Airflow. airflow_dag_runner.AirflowPipelineConfig : Pipeline config for AirflowDagRunner. airflow_orchestrator.AirflowOrchestrator : Orchestrator responsible for running pipelines using Airflow. dash.DashIntegration : Definition of Dash integration for ZenML. pipeline_run_lineage_visualizer.PipelineRunLineageVisualizer : Implementation of a lineage diagram via the [dash]( evidently.EvidentlyIntegration : Definition of Evidently integration evidently_profile.EvidentlyProfileConfig : Config class for Evidently profile steps. evidently_profile.EvidentlyProfileStep : Simple step implementation which implements Evidently's functionality for evidently_visualizer.EvidentlyVisualizer : The implementation of an Evidently Visualizer. facets.FacetsIntegration : Definition of Facet integration facet_statistics_visualizer.FacetStatisticsVisualizer : The base implementation of a ZenML Visualizer. gcp.GcpIntegration : Definition of Google Cloud Platform integration for ZenML. gcp_artifact_store.GCPArtifactStore : Artifact Store for Google Cloud Storage based artifacts. gcs_plugin.ZenGCS : Filesystem that delegates to Google Cloud Store using gcsfs. graphviz.GraphvizIntegration : Definition of Graphviz integration for ZenML. pipeline_run_dag_visualizer.PipelineRunDagVisualizer : Visualize the lineage of runs in a pipeline. integration.Integration : Base class for integration in ZenML integration.IntegrationMeta : Metaclass responsible for registering different Integration kubeflow.KubeflowIntegration : Definition of Kubeflow Integration for ZenML. kubeflow_metadata_store.KubeflowMetadataStore : Kubeflow MySQL backend for ZenML metadata store. kubeflow_component.KubeflowComponent : Base component for all Kubeflow pipelines TFX components. kubeflow_dag_runner.KubeflowDagRunner : Kubeflow Pipelines runner. kubeflow_dag_runner.KubeflowDagRunnerConfig : Runtime configuration parameters specific to execution on Kubeflow. kubeflow_orchestrator.KubeflowOrchestrator : Orchestrator responsible for running pipelines using Kubeflow. mlflow.MlflowIntegration : Definition of Plotly integration for ZenML. plotly.PlotlyIntegration : Definition of Plotly integration for ZenML. pipeline_lineage_visualizer.PipelineLineageVisualizer : Visualize the lineage of runs in a pipeline using plotly. pytorch.PytorchIntegration : Definition of PyTorch integration for ZenML. pytorch_materializer.PyTorchMaterializer : Materializer to read/write Pytorch models. pytorch_types.TorchDict : A type of dict that represents saving a model. pytorch_lightning.PytorchLightningIntegration : Definition of PyTorch Lightning integration for ZenML. pytorch_lightning_materializer.PyTorchLightningMaterializer : Materializer to read/write Pytorch models. registry.IntegrationRegistry : Registry to keep track of ZenML Integrations sklearn.SklearnIntegration : Definition of sklearn integration for ZenML. sklearn_materializer.SklearnMaterializer : Materializer to read data to and from sklearn. sklearn_evaluator.SklearnEvaluator : A simple step implementation which utilizes sklearn to evaluate the sklearn_evaluator.SklearnEvaluatorConfig : Config class for the sklearn evaluator sklearn_splitter.SklearnSplitter : A simple step implementation which utilizes sklearn to split a given sklearn_splitter.SklearnSplitterConfig : Config class for the sklearn splitter sklearn_standard_scaler.SklearnStandardScaler : Simple step implementation which utilizes the StandardScaler from sklearn sklearn_standard_scaler.SklearnStandardScalerConfig : Config class for the sklearn standard scaler tensorflow.TensorflowIntegration : Definition of Tensorflow integration for ZenML. keras_materializer.KerasMaterializer : Materializer to read/write Keras models. tf_dataset_materializer.TensorflowDatasetMaterializer : Materializer to read data to and from tf.data.Dataset. tensorflow_trainer.TensorflowBinaryClassifier : Simple step implementation which creates a simple tensorflow feedforward tensorflow_trainer.TensorflowBinaryClassifierConfig : Config class for the tensorflow trainer io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation fileio_registry.FileIORegistry : Registry of pluggable filesystem implementations used in TFX components. filesystem.FileSystemMeta : Metaclass which is responsible for registering the defined filesystem filesystem.Filesystem : Abstract Filesystem class. filesystem.NotFoundError : Auxiliary not found error logger.CustomFormatter : Formats logs according to custom specifications. base_materializer.BaseMaterializer : Base Materializer to realize artifact data. base_materializer.BaseMaterializerMeta : Metaclass responsible for registering different BaseMaterializer beam_materializer.BeamMaterializer : Materializer to read data to and from beam. built_in_materializer.BuiltInMaterializer : Read/Write JSON files. default_materializer_registry.MaterializerRegistry : Matches a python type to a default materializer. numpy_materializer.NumpyMaterializer : Materializer to read data to and from pandas. pandas_materializer.PandasMaterializer : Materializer to read data to and from pandas. base_metadata_store.BaseMetadataStore : Metadata store base class to track metadata of zenml first class mysql_metadata_store.MySQLMetadataStore : MySQL backend for ZenML metadata store. sqlite_metadata_store.SQLiteMetadataStore : SQLite backend for ZenML metadata store. base_orchestrator.BaseOrchestrator : Base Orchestrator class to orchestrate ZenML pipelines. local_dag_runner.LocalDagRunner : Local TFX DAG runner. local_orchestrator.LocalOrchestrator : Orchestrator responsible for running pipelines locally. base_pipeline.BasePipeline : Abstract base class for all ZenML pipelines. base_pipeline.BasePipelineMeta : Pipeline Metaclass responsible for validating the pipeline definition. training_pipeline.TrainingPipeline : Class for the classic training pipeline implementation artifact.ArtifactView : Post-execution artifact class which can be used to read pipeline.PipelineView : Post-execution pipeline class which can be used to query pipeline_run.PipelineRunView : Post-execution pipeline run class which can be used to query step.StepView : Post-execution step class which can be used to query base_stack.BaseStack : Base stack for ZenML. base_step.BaseStep : Abstract base class for all ZenML steps. base_step.BaseStepMeta : Metaclass for BaseStep . base_step_config.BaseStepConfig : Base configuration class to pass execution params into a step. pandas_analyzer.PandasAnalyzer : Simple step implementation which analyzes a given pd.DataFrame pandas_analyzer.PandasAnalyzerConfig : Config class for the PandasAnalyzer Config pandas_datasource.PandasDatasource : Simple step implementation to ingest from a csv file using pandas pandas_datasource.PandasDatasourceConfig : Config class for the pandas csv datasource step_context.StepContext : Provides additional context inside a step function. step_context.StepContextOutput : Tuple containing materializer class and artifact for a step output. base_analyzer_step.BaseAnalyzerConfig : Base class for analyzer step configurations base_analyzer_step.BaseAnalyzerStep : Base step implementation for any analyzer step implementation on ZenML base_datasource_step.BaseDatasourceConfig : Base class for datasource configs to inherit from base_datasource_step.BaseDatasourceStep : Base step implementation for any datasource step implementation on ZenML base_drift_detection_step.BaseDriftDetectionConfig : Base class for drift detection step configurations base_drift_detection_step.BaseDriftDetectionStep : Base step implementation for any drift detection step implementation base_evaluator_step.BaseEvaluatorConfig : Base class for evaluator step configurations base_evaluator_step.BaseEvaluatorStep : Base step implementation for any evaluator step implementation on ZenML base_preprocessor_step.BasePreprocessorConfig : Base class for Preprocessor step configurations base_preprocessor_step.BasePreprocessorStep : Base step implementation for any Preprocessor step implementation on base_split_step.BaseSplitStep : Base step implementation for any split step implementation on ZenML base_split_step.BaseSplitStepConfig : Base class for split configs to inherit from base_trainer_step.BaseTrainerConfig : Base class for Trainer step configurations base_trainer_step.BaseTrainerStep : Base step implementation for any Trainer step implementation on step_output.Output : A named tuple with a default name that cannot be overridden. base_pipeline_run_visualizer.BasePipelineRunVisualizer : The base implementation of a ZenML Pipeline Run Visualizer. base_pipeline_visualizer.BasePipelineVisualizer : The base implementation of a ZenML Pipeline Visualizer. base_step_visualizer.BaseStepVisualizer : The base implementation of a ZenML Step Visualizer. base_visualizer.BaseVisualizer : Base class for all ZenML Visualizers.","title":"Classes"},{"location":"OVERVIEW/#functions","text":"example.check_for_version_mismatch utils.activate_integrations : Decorator that activates all ZenML integrations. utils.confirmation : Echo a confirmation string on the CLI. utils.declare : Echo a declaration on the CLI. utils.error : Echo an error string on the CLI. utils.format_component_list : Formats a list of components into a List of Dicts. This list of dicts utils.format_date : Format a date into a string. utils.install_package : Installs pypi package into the current environment with pip utils.parse_unknown_options : Parse unknown options from the CLI. utils.pretty_print : Pretty print an object on the CLI. utils.print_component_properties : Prints the properties of a component. utils.print_table : Echoes the list of dicts in a table format. The input object should be a utils.title : Echo a title formatted string on the CLI. utils.uninstall_package : Uninstalls pypi package from the current environment with pip utils.warning : Echo a warning string on the CLI. constants.handle_bool_env_var : Converts normal env var to boolean constants.handle_int_env_var : Converts normal env var to int mapping_utils.get_component_from_key : Given a key and a mapping, return an initialized component. mapping_utils.get_components_from_store : Returns a list of components from a store. mapping_utils.get_key_from_uuid : Return the key that points to a certain uuid in a mapping. utils.define_json_config_settings_source : Define a function to essentially deserialize a model from a serialized utils.generate_customise_sources : Generate a customise_sources function as defined here: container_entrypoint.main : Runs a single step defined by the command line arguments. docker_utils.build_docker_image : Builds a docker image. docker_utils.create_custom_build_context : Creates a docker build context. docker_utils.generate_dockerfile_contents : Generates a Dockerfile. docker_utils.get_current_environment_requirements : Returns a dict of package requirements for the environment that docker_utils.get_image_digest : Gets the digest of a docker image. docker_utils.push_docker_image : Pushes a docker image to a container registry. kubeflow_dag_runner.get_default_pipeline_operator_funcs : Returns a default list of pipeline operator functions. kubeflow_dag_runner.get_default_pod_labels : Returns the default pod label dict for Kubeflow. kubeflow_utils.replace_placeholder : Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. local_deployment_utils.check_prerequisites : Checks whether all prerequisites for a local kubeflow pipelines local_deployment_utils.create_k3d_cluster : Creates a K3D cluster. local_deployment_utils.delete_k3d_cluster : Deletes a K3D cluster with the given name. local_deployment_utils.deploy_kubeflow_pipelines : Deploys Kubeflow Pipelines. local_deployment_utils.k3d_cluster_exists : Checks whether there exists a K3D cluster with the given name. local_deployment_utils.kubeflow_pipelines_ready : Returns whether all Kubeflow Pipelines pods are ready. local_deployment_utils.start_kfp_ui_daemon : Starts a daemon process that forwards ports so the Kubeflow Pipelines local_deployment_utils.write_local_registry_yaml : Writes a K3D registry config file. mlflow_utils.enable_mlflow : Outer decorator function for the creation of a ZenML pipeline with mlflow mlflow_utils.enable_mlflow_init : Outer decorator function for extending the init method for pipelines mlflow_utils.enable_mlflow_run : Outer decorator function for extending the run method for pipelines mlflow_utils.local_mlflow_backend : Returns the local mlflow backend inside the global zenml directory mlflow_utils.setup_mlflow : Setup all mlflow related configurations. This includes specifying which digits.get_digits : Returns the digits dataset in the form of a tuple of numpy digits.get_digits_model : Creates a support vector classifier for digits dataset. utils.get_integration_for_module : Gets the integration class for a module inside an integration. utils.get_requirements_for_module : Gets requirements for a module inside an integration. fileio.append_file : Appends file_contents to file. fileio.convert_to_str : Converts a PathType to a str using UTF-8. fileio.copy : Copy a file from the source to the destination. fileio.copy_dir : Copies dir from source to destination. fileio.create_dir_if_not_exists : Creates directory if it does not exist. fileio.create_dir_recursive_if_not_exists : Creates directory recursively if it does not exist. fileio.create_file_if_not_exists : Creates file if it does not exist. fileio.file_exists : Returns True if the given path exists. fileio.find_files : Find files in a directory that match pattern. fileio.get_grandparent : Get grandparent of dir. fileio.get_parent : Get parent of dir. fileio.glob : Return the paths that match a glob pattern. fileio.is_dir : Returns whether the given path points to a directory. fileio.is_remote : Returns True if path exists remotely. fileio.is_root : Returns true if path has no parent in local filesystem. fileio.list_dir : Returns a list of files under dir. fileio.make_dirs : Make a directory at the given path, recursively creating parents. fileio.mkdir : Make a directory at the given path; parent directory must exist. fileio.move : Moves dir or file from source to destination. Can be used to rename. fileio.open : Open a file at the given path. fileio.remove : Remove the file at the given path. Dangerous operation. fileio.rename : Rename source file to destination file. fileio.resolve_relative_path : Takes relative path and resolves it absolutely. fileio.rm_dir : Deletes dir recursively. Dangerous operation. fileio.stat : Return the stat descriptor for a given file path. fileio.walk : Return an iterator that walks the contents of the given directory. utils.create_tarfile : Create a compressed representation of source_dir. utils.extract_tarfile : Extracts all files in a compressed tar file to output_dir. utils.get_global_config_directory : Returns the global config directory for ZenML. utils.get_zenml_config_dir : Recursive function to find the zenml config starting from path. utils.get_zenml_dir : Returns path to a ZenML repository directory. utils.is_gcs_path : Returns True if path is on Google Cloud Storage. utils.is_zenml_dir : Check if dir is a zenml dir or not. utils.read_file_contents_as_string : Reads contents of file. utils.write_file_contents_as_string : Writes contents of file. logger.get_console_handler : Get console handler for logging. logger.get_file_handler : Return a file handler for logging. logger.get_logger : Main function to get logger name,. logger.get_logging_level : Get logging level from the env variable. logger.init_logging : Initialize logging with default levels. logger.set_root_verbosity : Set the root verbosity. utils.create_tfx_pipeline : Creates a tfx pipeline from a ZenML pipeline. utils.execute_step : Executes a tfx component. pipeline_decorator.pipeline : Outer decorator function for the creation of a ZenML pipeline step_decorator.step : Outer decorator function for the creation of a ZenML step utils.do_types_match : Check whether type_a and type_b match. utils.generate_component_class : Generates a TFX component class for a ZenML step. utils.generate_component_spec_class : Generates a TFX component spec class for a ZenML step. analytics_utils.get_environment : Returns a string representing the execution environment of the pipeline. analytics_utils.get_segment_key : Get key for authorizing to Segment backend. analytics_utils.get_system_info : Returns system info as a dict. analytics_utils.in_docker : Returns: True if running in a Docker container, else False analytics_utils.in_google_colab : Returns: True if running in a Google Colab env, else False analytics_utils.in_paperspace_gradient : Returns: True if running in a Paperspace Gradient env, else False analytics_utils.parametrized : This is a meta-decorator, that is, a decorator for decorators. analytics_utils.layer : Internal layer analytics_utils.track_event : Track segment event if user opted-in. daemon.check_if_daemon_is_running : Checks whether a daemon process indicated by the PID file is running. daemon.run_as_daemon : Runs a function as a daemon process. daemon.stop_daemon : Stops a daemon process. networking_utils.find_available_port : Finds a local unoccupied port. networking_utils.port_available : Checks if a local port is available. source_utils.create_zenml_pin : Creates a ZenML pin for source pinning from release version. source_utils.get_absolute_path_from_module_source : Get a directory path from module source. source_utils.get_class_source_from_source : Gets class source from source, i.e. module.path@version, returns version. source_utils.get_module_source_from_class : Takes class input and returns module_source. If class is already string source_utils.get_module_source_from_file_path : Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py source_utils.get_module_source_from_source : Gets module source from source. E.g. some.module.file.class@version , source_utils.get_relative_path_from_module_source : Get a directory path from module, relative to root of repository. source_utils.import_class_by_path : Imports a class based on a given path source_utils.import_python_file : Imports a python file. source_utils.is_inside_repository : Returns whether a file is inside a zenml repository. source_utils.is_standard_pin : Returns True if pin is valid ZenML pin, else False. source_utils.is_standard_source : Returns True if source is a standard ZenML source. source_utils.is_third_party_module : Returns whether a file belongs to a third party package. source_utils.load_source_path_class : Loads a Python class from the source. source_utils.resolve_class : Resolves a class into a serializable source string. source_utils.resolve_standard_source : Creates a ZenML pin for source pinning from release version. string_utils.get_human_readable_filesize : Convert a file size in bytes into a human-readable string. string_utils.get_human_readable_time : Convert seconds into a human-readable string. yaml_utils.is_yaml : Returns True if file_path is YAML, else False yaml_utils.read_json : Read JSON on file path and returns contents as dict. yaml_utils.read_yaml : Read YAML on file path and returns contents as dict. yaml_utils.write_json : Write contents as JSON format to file_path. yaml_utils.write_yaml : Write contents as YAML format to file_path. This file was automatically generated via lazydocs .","title":"Functions"},{"location":"artifact_stores.base_artifact_store/","text":"module artifact_stores.base_artifact_store Definition of an Artifact Store class BaseArtifactStore Base class for all ZenML Artifact Store. Every ZenML Artifact Store should override this class. method __init__ __init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseArtifactStore instance. Args: repo_path : Path to the repository of this artifact store. method get_component_name_from_uri get_component_name_from_uri(artifact_uri: str) \u2192 str Gets component name from artifact URI. Args: artifact_uri : URI to artifact. Returns: Name of the component. method resolve_uri_locally resolve_uri_locally(artifact_uri: str, path: Optional[str] = None) \u2192 str Takes a URI that points within the artifact store, downloads the URI locally, then returns local URI. Args: artifact_uri : uri to artifact. path : optional path to download to. If None, is inferred. Returns: Locally resolved uri. This file was automatically generated via lazydocs .","title":"Artifact stores.base artifact store"},{"location":"artifact_stores.base_artifact_store/#module-artifact_storesbase_artifact_store","text":"Definition of an Artifact Store","title":"module artifact_stores.base_artifact_store"},{"location":"artifact_stores.base_artifact_store/#class-baseartifactstore","text":"Base class for all ZenML Artifact Store. Every ZenML Artifact Store should override this class.","title":"class BaseArtifactStore"},{"location":"artifact_stores.base_artifact_store/#method-__init__","text":"__init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseArtifactStore instance. Args: repo_path : Path to the repository of this artifact store.","title":"method __init__"},{"location":"artifact_stores.base_artifact_store/#method-get_component_name_from_uri","text":"get_component_name_from_uri(artifact_uri: str) \u2192 str Gets component name from artifact URI. Args: artifact_uri : URI to artifact. Returns: Name of the component.","title":"method get_component_name_from_uri"},{"location":"artifact_stores.base_artifact_store/#method-resolve_uri_locally","text":"resolve_uri_locally(artifact_uri: str, path: Optional[str] = None) \u2192 str Takes a URI that points within the artifact store, downloads the URI locally, then returns local URI. Args: artifact_uri : uri to artifact. path : optional path to download to. If None, is inferred. Returns: Locally resolved uri. This file was automatically generated via lazydocs .","title":"method resolve_uri_locally"},{"location":"artifact_stores.local_artifact_store/","text":"module artifact_stores.local_artifact_store Global Variables REMOTE_FS_PREFIX class LocalArtifactStore Artifact Store for local artifacts. classmethod must_be_local_path must_be_local_path(v: str) \u2192 str Validates that the path is a local path. This file was automatically generated via lazydocs .","title":"Artifact stores.local artifact store"},{"location":"artifact_stores.local_artifact_store/#module-artifact_storeslocal_artifact_store","text":"","title":"module artifact_stores.local_artifact_store"},{"location":"artifact_stores.local_artifact_store/#global-variables","text":"REMOTE_FS_PREFIX","title":"Global Variables"},{"location":"artifact_stores.local_artifact_store/#class-localartifactstore","text":"Artifact Store for local artifacts.","title":"class LocalArtifactStore"},{"location":"artifact_stores.local_artifact_store/#classmethod-must_be_local_path","text":"must_be_local_path(v: str) \u2192 str Validates that the path is a local path. This file was automatically generated via lazydocs .","title":"classmethod must_be_local_path"},{"location":"artifact_stores/","text":"module artifact_stores An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores . This file was automatically generated via lazydocs .","title":"Artifact stores"},{"location":"artifact_stores/#module-artifact_stores","text":"An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores . This file was automatically generated via lazydocs .","title":"module artifact_stores"},{"location":"artifacts.base_artifact/","text":"module artifacts.base_artifact The below code is copied from the TFX source repo with minor changes. All credits go to the TFX team for the core implementation Global Variables DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY class BaseArtifact Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: Upon creation, each artifact class needs to be given a unique TYPE_NAME. Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs. method __init__ __init__(*args: Any, **kwargs: Any) \u2192 None Init method for BaseArtifact property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. classmethod set_zenml_artifact_type set_zenml_artifact_type() \u2192 None Set the type of the artifact. This file was automatically generated via lazydocs .","title":"Artifacts.base artifact"},{"location":"artifacts.base_artifact/#module-artifactsbase_artifact","text":"The below code is copied from the TFX source repo with minor changes. All credits go to the TFX team for the core implementation","title":"module artifacts.base_artifact"},{"location":"artifacts.base_artifact/#global-variables","text":"DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY","title":"Global Variables"},{"location":"artifacts.base_artifact/#class-baseartifact","text":"Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: Upon creation, each artifact class needs to be given a unique TYPE_NAME. Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs.","title":"class BaseArtifact"},{"location":"artifacts.base_artifact/#method-__init__","text":"__init__(*args: Any, **kwargs: Any) \u2192 None Init method for BaseArtifact","title":"method __init__"},{"location":"artifacts.base_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"artifacts.base_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"artifacts.base_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"artifacts.base_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"artifacts.base_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"artifacts.base_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"artifacts.base_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"artifacts.base_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"artifacts.base_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"artifacts.base_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"artifacts.base_artifact/#property-uri","text":"Artifact URI.","title":"property uri"},{"location":"artifacts.base_artifact/#classmethod-set_zenml_artifact_type","text":"set_zenml_artifact_type() \u2192 None Set the type of the artifact. This file was automatically generated via lazydocs .","title":"classmethod set_zenml_artifact_type"},{"location":"artifacts.constants/","text":"module artifacts.constants Global Variables DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY This file was automatically generated via lazydocs .","title":"Artifacts.constants"},{"location":"artifacts.constants/#module-artifactsconstants","text":"","title":"module artifacts.constants"},{"location":"artifacts.constants/#global-variables","text":"DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"artifacts.data_analysis_artifact/","text":"module artifacts.data_analysis_artifact class DataAnalysisArtifact Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc. property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. This file was automatically generated via lazydocs .","title":"Artifacts.data analysis artifact"},{"location":"artifacts.data_analysis_artifact/#module-artifactsdata_analysis_artifact","text":"","title":"module artifacts.data_analysis_artifact"},{"location":"artifacts.data_analysis_artifact/#class-dataanalysisartifact","text":"Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc.","title":"class DataAnalysisArtifact"},{"location":"artifacts.data_analysis_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"artifacts.data_analysis_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"artifacts.data_analysis_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"artifacts.data_analysis_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"artifacts.data_analysis_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"artifacts.data_analysis_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"artifacts.data_analysis_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"artifacts.data_analysis_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"artifacts.data_analysis_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"artifacts.data_analysis_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"artifacts.data_analysis_artifact/#property-uri","text":"Artifact URI. This file was automatically generated via lazydocs .","title":"property uri"},{"location":"artifacts.data_artifact/","text":"module artifacts.data_artifact class DataArtifact Class for all ZenML data artifacts. property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. This file was automatically generated via lazydocs .","title":"Artifacts.data artifact"},{"location":"artifacts.data_artifact/#module-artifactsdata_artifact","text":"","title":"module artifacts.data_artifact"},{"location":"artifacts.data_artifact/#class-dataartifact","text":"Class for all ZenML data artifacts.","title":"class DataArtifact"},{"location":"artifacts.data_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"artifacts.data_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"artifacts.data_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"artifacts.data_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"artifacts.data_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"artifacts.data_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"artifacts.data_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"artifacts.data_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"artifacts.data_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"artifacts.data_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"artifacts.data_artifact/#property-uri","text":"Artifact URI. This file was automatically generated via lazydocs .","title":"property uri"},{"location":"artifacts/","text":"module artifacts Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: .. code:: python // Some code def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer. This file was automatically generated via lazydocs .","title":"Artifacts"},{"location":"artifacts/#module-artifacts","text":"Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: .. code:: python // Some code def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer. This file was automatically generated via lazydocs .","title":"module artifacts"},{"location":"artifacts.model_artifact/","text":"module artifacts.model_artifact class ModelArtifact Class for all ZenML model artifacts. property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. This file was automatically generated via lazydocs .","title":"Artifacts.model artifact"},{"location":"artifacts.model_artifact/#module-artifactsmodel_artifact","text":"","title":"module artifacts.model_artifact"},{"location":"artifacts.model_artifact/#class-modelartifact","text":"Class for all ZenML model artifacts.","title":"class ModelArtifact"},{"location":"artifacts.model_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"artifacts.model_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"artifacts.model_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"artifacts.model_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"artifacts.model_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"artifacts.model_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"artifacts.model_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"artifacts.model_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"artifacts.model_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"artifacts.model_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"artifacts.model_artifact/#property-uri","text":"Artifact URI. This file was automatically generated via lazydocs .","title":"property uri"},{"location":"artifacts.schema_artifact/","text":"module artifacts.schema_artifact class SchemaArtifact Class for all ZenML schema artifacts. property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. This file was automatically generated via lazydocs .","title":"Artifacts.schema artifact"},{"location":"artifacts.schema_artifact/#module-artifactsschema_artifact","text":"","title":"module artifacts.schema_artifact"},{"location":"artifacts.schema_artifact/#class-schemaartifact","text":"Class for all ZenML schema artifacts.","title":"class SchemaArtifact"},{"location":"artifacts.schema_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"artifacts.schema_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"artifacts.schema_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"artifacts.schema_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"artifacts.schema_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"artifacts.schema_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"artifacts.schema_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"artifacts.schema_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"artifacts.schema_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"artifacts.schema_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"artifacts.schema_artifact/#property-uri","text":"Artifact URI. This file was automatically generated via lazydocs .","title":"property uri"},{"location":"artifacts.statistics_artifact/","text":"module artifacts.statistics_artifact class StatisticsArtifact Class for all ZenML statistics artifacts. property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. This file was automatically generated via lazydocs .","title":"Artifacts.statistics artifact"},{"location":"artifacts.statistics_artifact/#module-artifactsstatistics_artifact","text":"","title":"module artifacts.statistics_artifact"},{"location":"artifacts.statistics_artifact/#class-statisticsartifact","text":"Class for all ZenML statistics artifacts.","title":"class StatisticsArtifact"},{"location":"artifacts.statistics_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"artifacts.statistics_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"artifacts.statistics_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"artifacts.statistics_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"artifacts.statistics_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"artifacts.statistics_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"artifacts.statistics_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"artifacts.statistics_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"artifacts.statistics_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"artifacts.statistics_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"artifacts.statistics_artifact/#property-uri","text":"Artifact URI. This file was automatically generated via lazydocs .","title":"property uri"},{"location":"artifacts.type_registry/","text":"module artifacts.type_registry Global Variables TYPE_CHECKING type_registry class ArtifactTypeRegistry A registry to keep track of which datatypes map to which artifact types method __init__ __init__() \u2192 None Initialization with an empty registry method get_artifact_type get_artifact_type(key: Type[Any]) \u2192 List[Type[ForwardRef('BaseArtifact')]] Method to extract the list of artifact types given the data type method register_integration register_integration( key: Type[Any], type_: List[Type[ForwardRef('BaseArtifact')]] ) \u2192 None Method to register an integration within the registry Args: key : any datatype type_ : the list of artifact type that the given datatypes is associated with This file was automatically generated via lazydocs .","title":"Artifacts.type registry"},{"location":"artifacts.type_registry/#module-artifactstype_registry","text":"","title":"module artifacts.type_registry"},{"location":"artifacts.type_registry/#global-variables","text":"TYPE_CHECKING type_registry","title":"Global Variables"},{"location":"artifacts.type_registry/#class-artifacttyperegistry","text":"A registry to keep track of which datatypes map to which artifact types","title":"class ArtifactTypeRegistry"},{"location":"artifacts.type_registry/#method-__init__","text":"__init__() \u2192 None Initialization with an empty registry","title":"method __init__"},{"location":"artifacts.type_registry/#method-get_artifact_type","text":"get_artifact_type(key: Type[Any]) \u2192 List[Type[ForwardRef('BaseArtifact')]] Method to extract the list of artifact types given the data type","title":"method get_artifact_type"},{"location":"artifacts.type_registry/#method-register_integration","text":"register_integration( key: Type[Any], type_: List[Type[ForwardRef('BaseArtifact')]] ) \u2192 None Method to register an integration within the registry Args: key : any datatype type_ : the list of artifact type that the given datatypes is associated with This file was automatically generated via lazydocs .","title":"method register_integration"},{"location":"cli.artifact_store/","text":"module cli.artifact_store Global Variables TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Cli.artifact store"},{"location":"cli.artifact_store/#module-cliartifact_store","text":"","title":"module cli.artifact_store"},{"location":"cli.artifact_store/#global-variables","text":"TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"cli.base/","text":"module cli.base Global Variables INITIALIZE_REPO This file was automatically generated via lazydocs .","title":"Cli.base"},{"location":"cli.base/#module-clibase","text":"","title":"module cli.base"},{"location":"cli.base/#global-variables","text":"INITIALIZE_REPO This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"cli.cli/","text":"module cli.cli .. currentmodule:: ce_cli.cli .. moduleauthor:: ZenML GmbH support@zenml.io This file was automatically generated via lazydocs .","title":"Cli.cli"},{"location":"cli.cli/#module-clicli","text":".. currentmodule:: ce_cli.cli .. moduleauthor:: ZenML GmbH support@zenml.io This file was automatically generated via lazydocs .","title":"module cli.cli"},{"location":"cli.config/","text":"module cli.config CLI for manipulating ZenML local and global config file. Global Variables TYPE_CHECKING OPT_IN_ANALYTICS OPT_OUT_ANALYTICS This file was automatically generated via lazydocs .","title":"Cli.config"},{"location":"cli.config/#module-cliconfig","text":"CLI for manipulating ZenML local and global config file.","title":"module cli.config"},{"location":"cli.config/#global-variables","text":"TYPE_CHECKING OPT_IN_ANALYTICS OPT_OUT_ANALYTICS This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"cli.container_registry/","text":"module cli.container_registry This file was automatically generated via lazydocs .","title":"Cli.container registry"},{"location":"cli.container_registry/#module-clicontainer_registry","text":"This file was automatically generated via lazydocs .","title":"module cli.container_registry"},{"location":"cli.example/","text":"module cli.example Global Variables zenml_version_installed GIT_REPO_URL RUN_EXAMPLE EXAMPLES_GITHUB_REPO EXAMPLES_RUN_SCRIPT SHELL_EXECUTABLE function check_for_version_mismatch check_for_version_mismatch(git_examples_handler: GitExamplesHandler) \u2192 None class LocalExample Class to encapsulate all properties and methods of the local example that can be run from the CLI method __init__ __init__(path: Path, name: str) \u2192 None Create a new LocalExample instance. Args: name : The name of the example, specifically the name of the folder on git path : Path at which the example is installed property executable_python_example Return the python file for the example property has_any_python_file Boolean that states if any python file is present property has_single_python_file Boolean that states if only one python file is present property python_files_in_dir List of all python files in the drectl in local example directory the init .py file is excluded from this list method is_present is_present() \u2192 bool Checks if the example is installed at the given path. method run_example run_example(example_runner: List[str], force: bool) \u2192 None Run the local example using the bash script at the supplied location Args: example_runner : Sequence of locations of executable file(s) to run the example force : Whether to force the install class Example Class for all example objects. method __init__ __init__(name: str, path_in_repo: Path) \u2192 None Create a new Example instance. Args: name : The name of the example, specifically the name of the folder on git path_in_repo : Path to the local example within the global zenml folder. property readme_content Returns the readme content associated with a particular example. class ExamplesRepo Class for the examples repository object. method __init__ __init__(cloning_path: Path) \u2192 None Create a new ExamplesRepo instance. property active_version In case a tagged version is checked out, this property returns that version, else None is returned property examples_dir Returns the path for the examples directory. property examples_run_bash_script property is_cloned Returns whether we have already cloned the examples repository. property latest_release Returns the latest release for the examples repository. method checkout checkout(branch: str) \u2192 None Checks out a specific branch or tag of the examples repository Raises: GitCommandError : if branch doesn't exist. method checkout_latest_release checkout_latest_release() \u2192 None Checks out the latest release of the examples repository. method clone clone() \u2192 None Clones repo to cloning_path. If you break off the operation with a KeyBoardInterrupt before the cloning is completed, this method will delete whatever was partially downloaded from your system. method delete delete() \u2192 None Delete cloning_path if it exists. class GitExamplesHandler Class for the GitExamplesHandler that interfaces with the CLI tool. method __init__ __init__() \u2192 None Create a new GitExamplesHandler instance. property examples Property that contains a list of examples property is_matching_versions Returns a boolean whether the checked out examples are on the same code version as zenml method clean_current_examples clean_current_examples() \u2192 None Deletes the ZenML examples directory from your current working directory. method copy_example copy_example(example: Example, destination_dir: str) \u2192 None Copies an example to the destination_dir. method get_examples get_examples(example_name: Optional[str] = None) \u2192 List[Example] Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name : Name of an example. method is_example is_example(example_name: Optional[str] = None) \u2192 bool Checks if the supplied example_name corresponds to an example method pull pull(version: str = '', force: bool = False, branch: str = 'main') \u2192 None Pulls the examples from the main git examples repository. method pull_latest_examples pull_latest_examples() \u2192 None Pulls the latest examples from the examples repository. This file was automatically generated via lazydocs .","title":"Cli.example"},{"location":"cli.example/#module-cliexample","text":"","title":"module cli.example"},{"location":"cli.example/#global-variables","text":"zenml_version_installed GIT_REPO_URL RUN_EXAMPLE EXAMPLES_GITHUB_REPO EXAMPLES_RUN_SCRIPT SHELL_EXECUTABLE","title":"Global Variables"},{"location":"cli.example/#function-check_for_version_mismatch","text":"check_for_version_mismatch(git_examples_handler: GitExamplesHandler) \u2192 None","title":"function check_for_version_mismatch"},{"location":"cli.example/#class-localexample","text":"Class to encapsulate all properties and methods of the local example that can be run from the CLI","title":"class LocalExample"},{"location":"cli.example/#method-__init__","text":"__init__(path: Path, name: str) \u2192 None Create a new LocalExample instance. Args: name : The name of the example, specifically the name of the folder on git path : Path at which the example is installed","title":"method __init__"},{"location":"cli.example/#property-executable_python_example","text":"Return the python file for the example","title":"property executable_python_example"},{"location":"cli.example/#property-has_any_python_file","text":"Boolean that states if any python file is present","title":"property has_any_python_file"},{"location":"cli.example/#property-has_single_python_file","text":"Boolean that states if only one python file is present","title":"property has_single_python_file"},{"location":"cli.example/#property-python_files_in_dir","text":"List of all python files in the drectl in local example directory the init .py file is excluded from this list","title":"property python_files_in_dir"},{"location":"cli.example/#method-is_present","text":"is_present() \u2192 bool Checks if the example is installed at the given path.","title":"method is_present"},{"location":"cli.example/#method-run_example","text":"run_example(example_runner: List[str], force: bool) \u2192 None Run the local example using the bash script at the supplied location Args: example_runner : Sequence of locations of executable file(s) to run the example force : Whether to force the install","title":"method run_example"},{"location":"cli.example/#class-example","text":"Class for all example objects.","title":"class Example"},{"location":"cli.example/#method-__init___1","text":"__init__(name: str, path_in_repo: Path) \u2192 None Create a new Example instance. Args: name : The name of the example, specifically the name of the folder on git path_in_repo : Path to the local example within the global zenml folder.","title":"method __init__"},{"location":"cli.example/#property-readme_content","text":"Returns the readme content associated with a particular example.","title":"property readme_content"},{"location":"cli.example/#class-examplesrepo","text":"Class for the examples repository object.","title":"class ExamplesRepo"},{"location":"cli.example/#method-__init___2","text":"__init__(cloning_path: Path) \u2192 None Create a new ExamplesRepo instance.","title":"method __init__"},{"location":"cli.example/#property-active_version","text":"In case a tagged version is checked out, this property returns that version, else None is returned","title":"property active_version"},{"location":"cli.example/#property-examples_dir","text":"Returns the path for the examples directory.","title":"property examples_dir"},{"location":"cli.example/#property-examples_run_bash_script","text":"","title":"property examples_run_bash_script"},{"location":"cli.example/#property-is_cloned","text":"Returns whether we have already cloned the examples repository.","title":"property is_cloned"},{"location":"cli.example/#property-latest_release","text":"Returns the latest release for the examples repository.","title":"property latest_release"},{"location":"cli.example/#method-checkout","text":"checkout(branch: str) \u2192 None Checks out a specific branch or tag of the examples repository Raises: GitCommandError : if branch doesn't exist.","title":"method checkout"},{"location":"cli.example/#method-checkout_latest_release","text":"checkout_latest_release() \u2192 None Checks out the latest release of the examples repository.","title":"method checkout_latest_release"},{"location":"cli.example/#method-clone","text":"clone() \u2192 None Clones repo to cloning_path. If you break off the operation with a KeyBoardInterrupt before the cloning is completed, this method will delete whatever was partially downloaded from your system.","title":"method clone"},{"location":"cli.example/#method-delete","text":"delete() \u2192 None Delete cloning_path if it exists.","title":"method delete"},{"location":"cli.example/#class-gitexampleshandler","text":"Class for the GitExamplesHandler that interfaces with the CLI tool.","title":"class GitExamplesHandler"},{"location":"cli.example/#method-__init___3","text":"__init__() \u2192 None Create a new GitExamplesHandler instance.","title":"method __init__"},{"location":"cli.example/#property-examples","text":"Property that contains a list of examples","title":"property examples"},{"location":"cli.example/#property-is_matching_versions","text":"Returns a boolean whether the checked out examples are on the same code version as zenml","title":"property is_matching_versions"},{"location":"cli.example/#method-clean_current_examples","text":"clean_current_examples() \u2192 None Deletes the ZenML examples directory from your current working directory.","title":"method clean_current_examples"},{"location":"cli.example/#method-copy_example","text":"copy_example(example: Example, destination_dir: str) \u2192 None Copies an example to the destination_dir.","title":"method copy_example"},{"location":"cli.example/#method-get_examples","text":"get_examples(example_name: Optional[str] = None) \u2192 List[Example] Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name : Name of an example.","title":"method get_examples"},{"location":"cli.example/#method-is_example","text":"is_example(example_name: Optional[str] = None) \u2192 bool Checks if the supplied example_name corresponds to an example","title":"method is_example"},{"location":"cli.example/#method-pull","text":"pull(version: str = '', force: bool = False, branch: str = 'main') \u2192 None Pulls the examples from the main git examples repository.","title":"method pull"},{"location":"cli.example/#method-pull_latest_examples","text":"pull_latest_examples() \u2192 None Pulls the latest examples from the examples repository. This file was automatically generated via lazydocs .","title":"method pull_latest_examples"},{"location":"cli.integration/","text":"module cli.integration This file was automatically generated via lazydocs .","title":"Cli.integration"},{"location":"cli.integration/#module-cliintegration","text":"This file was automatically generated via lazydocs .","title":"module cli.integration"},{"location":"cli/","text":"module cli ZenML CLI ================== The ZenML CLI tool is usually downloaded and installed via PyPI and a pip install zenml command. Please see the Installation & Setup section above for more information about that process. How to use the CLI Our CLI behaves similarly to many other CLIs for basic features. In order to find out which version of ZenML you are running, type: .. code:: bash zenml version If you ever need more information on exactly what a certain command will do, use the --help flag attached to the end of your command string. For example, to get a sense of all of the commands available to you while using the zenml command, type: .. code:: bash zenml --help If you were instead looking to know more about a specific command, you can type something like this: .. code:: bash zenml metadata-store register --help This will give you information about how to register a metadata store. (See below for more on that). Beginning a Project In order to start working on your project, initialize a ZenML repository within your current directory with ZenML\u2019s own config and resource management tools: .. code:: bash zenml init This is all you need to begin using all the MLOps goodness that ZenML provides! By default, zenml init will install its own hidden .zen folder inside the current directory from which you are running the command. You can also pass in a directory path manually using the --repo_path option: .. code:: bash zenml init --repo_path /path/to/dir If you wish to specify that you do not want analytics to be transmitted back to ZenML about your usage of the tool, pass in False to the --analytics_opt_in option: .. code:: bash zenml init --analytics_opt_in false If you wish to delete all data relating to your project from the directory, use the zenml clean command. This will: delete all pipelines delete all artifacts delete all metadata Note that the clean command is not implemented for the current version. Loading and using pre-built examples If you don\u2019t have a project of your own that you\u2019re currently working on, or if you just want to play around a bit and see some functional code, we\u2019ve got your back! You can use the ZenML CLI tool to download some pre-built examples. We know that working examples are a great way to get to know a tool, so we\u2019ve made some examples for you to use to get started. (This is something that will grow as we add more). To list all the examples available to you, type: .. code:: bash zenml example list If you want more detailed information about a specific example, use the info subcommand in combination with the name of the example, like this: .. code:: bash zenml example info quickstart If you want to pull all the examples into your current working directory (wherever you are executing the zenml command from in your terminal), the CLI will create a zenml_examples folder for you if it doesn\u2019t already exist whenever you use the pull subcommand. The default is to copy all the examples, like this: .. code:: bash zenml example pull If you\u2019d only like to pull a single example, add the name of that example (for example, quickstart ) as an argument to the same command, as follows: .. code:: bash zenml example pull quickstart If you would like to force-redownload the examples, use the --force or -f flag as in this example: .. code:: bash zenml example pull --force This will redownload all the examples afresh, using the same version of ZenML as you currently have installed. If for some reason you want to download examples corresponding to a previous release of ZenML, use the --version or -v flag to specify, as in the following example: .. code:: bash zenml example pull --force --version 0.3.8 If you wish to run the example, allowing the ZenML CLI to do the work of setting up whatever dependencies are required, use the run subcommand: .. code:: bash zenml example run quickstart Using integrations Integrations are the different pieces of a project stack that enable custom functionality. This ranges from bigger libraries like kubeflow for orchestration down to smaller visualization tools like facets . Our CLI is an easy way to get started with these integrations. To list all the integrations available to you, type: zenml integration list To see the requirements for a specific integration, use the requirements command: zenml integration requirements INTEGRATION_NAME If you wish to install the integration, using the requirements listed in the previous command, install allows you to do this for your local environment: zenml integration install INTEGRATION_NAME Note that if you don't specify a specific integration to be installed, the ZenML CLI will install all available integrations. Uninstalling a specific integration is as simple as typing: zenml integration uninstall INTEGRATION_NAME Customizing your Metadata Store The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. By default ZenML initializes your repository with a metadata store kept on your local machine. If you wish to register a new metadata store, do so with the register command: .. code:: bash zenml metadata-store register METADATA_STORE_NAME --type METADATA_STORE_TYPE [--OPTIONS] If you wish to list the metadata stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml metadata-store list If you wish to delete a particular metadata store, pass the name of the metadata store into the CLI with the following command: .. code:: bash zenml metadata-store delete METADATA_STORE_NAME Customizing your Artifact Store The artifact store is where all the inputs and outputs of your pipeline steps are stored. By default, ZenML initializes your repository with an artifact store with everything kept on your local machine. If you wish to register a new artifact store, do so with the register command: .. code:: bash zenml artifact-store register ARTIFACT_STORE_NAME --type ARTIFACT_STORE_TYPE [--OPTIONS] If you wish to list the artifact stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml artifact-store list If you wish to delete a particular artifact store, pass the name of the artifact store into the CLI with the following command: .. code:: bash zenml artifact-store delete ARTIFACT_STORE_NAME Customizing your Orchestrator An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. By default, ZenML initializes your repository with an orchestrator that runs everything on your local machine. If you wish to register a new orchestrator, do so with the register command: .. code:: bash zenml orchestrator register ORCHESTRATOR_NAME --type ORCHESTRATOR_TYPE [--ORCHESTRATOR_OPTIONS] If you wish to list the orchestrators that have already been registered within your ZenML project / repository, type: .. code:: bash zenml orchestrator list If you wish to delete a particular orchestrator, pass the name of the orchestrator into the CLI with the following command: .. code:: bash zenml orchestrator delete ORCHESTRATOR_NAME Customizing your Container Registry The container registry is where all the images that are used by a container-based orchestrator are stored. By default, a default ZenML local stack will not register a container registry. If you wish to register a new container registry, do so with the register command: zenml container-registry register REGISTRY_NAME --type REGISTRY_TYPE [--REGISTRY_OPTIONS] If you want the name of the current container registry, use the get command: zenml container-registry get To list all container registries available and registered for use, use the list command: zenml container-registry list For details about a particular container registry, use the describe command. By default (without a specific registry name passed in) it will describe the active or currently used container registry: zenml container-registry describe [REGISTRY_NAME] To delete a container registry (and all of its contents), use the delete command: zenml container-registry delete Administering the Stack The stack is a grouping of your artifact store, your metadata store and your orchestrator. With the ZenML tool, switching from a local stack to a distributed cloud environment can be accomplished with just a few CLI commands. To register a new stack, you must already have registered the individual components of the stack using the commands listed above. Use the zenml stack register command to register your stack. It takes four arguments as in the following example: .. code:: bash zenml stack register STACK_NAME -m METADATA_STORE_NAME -a ARTIFACT_STORE_NAME -o ORCHESTRATOR_NAME Each corresponding argument should be the name you passed in as an identifier for the artifact store, metadata store or orchestrator when you originally registered it. To list the stacks that you have registered within your current ZenML project, type: .. code:: bash zenml stack list To delete a stack that you have previously registered, type: .. code:: bash zenml stack delete STACK_NAME By default, ZenML uses a local stack whereby all pipelines run on your local computer. If you wish to set a different stack as the current active stack to be used when running your pipeline, type: .. code:: bash zenml stack set STACK_NAME This changes a configuration property within your local environment. To see which stack is currently set as the default active stack, type: .. code:: bash zenml stack get Global Variables TYPE_CHECKING click INITIALIZE_REPO OPT_IN_ANALYTICS OPT_OUT_ANALYTICS zenml_version_installed GIT_REPO_URL RUN_EXAMPLE EXAMPLES_GITHUB_REPO EXAMPLES_RUN_SCRIPT SHELL_EXECUTABLE ascii_arts This file was automatically generated via lazydocs .","title":"Cli"},{"location":"cli/#module-cli","text":"ZenML CLI ================== The ZenML CLI tool is usually downloaded and installed via PyPI and a pip install zenml command. Please see the Installation & Setup section above for more information about that process.","title":"module cli"},{"location":"cli/#how-to-use-the-cli","text":"Our CLI behaves similarly to many other CLIs for basic features. In order to find out which version of ZenML you are running, type: .. code:: bash zenml version If you ever need more information on exactly what a certain command will do, use the --help flag attached to the end of your command string. For example, to get a sense of all of the commands available to you while using the zenml command, type: .. code:: bash zenml --help If you were instead looking to know more about a specific command, you can type something like this: .. code:: bash zenml metadata-store register --help This will give you information about how to register a metadata store. (See below for more on that).","title":"How to use the CLI"},{"location":"cli/#beginning-a-project","text":"In order to start working on your project, initialize a ZenML repository within your current directory with ZenML\u2019s own config and resource management tools: .. code:: bash zenml init This is all you need to begin using all the MLOps goodness that ZenML provides! By default, zenml init will install its own hidden .zen folder inside the current directory from which you are running the command. You can also pass in a directory path manually using the --repo_path option: .. code:: bash zenml init --repo_path /path/to/dir If you wish to specify that you do not want analytics to be transmitted back to ZenML about your usage of the tool, pass in False to the --analytics_opt_in option: .. code:: bash zenml init --analytics_opt_in false If you wish to delete all data relating to your project from the directory, use the zenml clean command. This will: delete all pipelines delete all artifacts delete all metadata Note that the clean command is not implemented for the current version.","title":"Beginning a Project"},{"location":"cli/#loading-and-using-pre-built-examples","text":"If you don\u2019t have a project of your own that you\u2019re currently working on, or if you just want to play around a bit and see some functional code, we\u2019ve got your back! You can use the ZenML CLI tool to download some pre-built examples. We know that working examples are a great way to get to know a tool, so we\u2019ve made some examples for you to use to get started. (This is something that will grow as we add more). To list all the examples available to you, type: .. code:: bash zenml example list If you want more detailed information about a specific example, use the info subcommand in combination with the name of the example, like this: .. code:: bash zenml example info quickstart If you want to pull all the examples into your current working directory (wherever you are executing the zenml command from in your terminal), the CLI will create a zenml_examples folder for you if it doesn\u2019t already exist whenever you use the pull subcommand. The default is to copy all the examples, like this: .. code:: bash zenml example pull If you\u2019d only like to pull a single example, add the name of that example (for example, quickstart ) as an argument to the same command, as follows: .. code:: bash zenml example pull quickstart If you would like to force-redownload the examples, use the --force or -f flag as in this example: .. code:: bash zenml example pull --force This will redownload all the examples afresh, using the same version of ZenML as you currently have installed. If for some reason you want to download examples corresponding to a previous release of ZenML, use the --version or -v flag to specify, as in the following example: .. code:: bash zenml example pull --force --version 0.3.8 If you wish to run the example, allowing the ZenML CLI to do the work of setting up whatever dependencies are required, use the run subcommand: .. code:: bash zenml example run quickstart","title":"Loading and using pre-built examples"},{"location":"cli/#using-integrations","text":"Integrations are the different pieces of a project stack that enable custom functionality. This ranges from bigger libraries like kubeflow for orchestration down to smaller visualization tools like facets . Our CLI is an easy way to get started with these integrations. To list all the integrations available to you, type: zenml integration list To see the requirements for a specific integration, use the requirements command: zenml integration requirements INTEGRATION_NAME If you wish to install the integration, using the requirements listed in the previous command, install allows you to do this for your local environment: zenml integration install INTEGRATION_NAME Note that if you don't specify a specific integration to be installed, the ZenML CLI will install all available integrations. Uninstalling a specific integration is as simple as typing: zenml integration uninstall INTEGRATION_NAME","title":"Using integrations"},{"location":"cli/#customizing-your-metadata-store","text":"The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. By default ZenML initializes your repository with a metadata store kept on your local machine. If you wish to register a new metadata store, do so with the register command: .. code:: bash zenml metadata-store register METADATA_STORE_NAME --type METADATA_STORE_TYPE [--OPTIONS] If you wish to list the metadata stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml metadata-store list If you wish to delete a particular metadata store, pass the name of the metadata store into the CLI with the following command: .. code:: bash zenml metadata-store delete METADATA_STORE_NAME","title":"Customizing your Metadata Store"},{"location":"cli/#customizing-your-artifact-store","text":"The artifact store is where all the inputs and outputs of your pipeline steps are stored. By default, ZenML initializes your repository with an artifact store with everything kept on your local machine. If you wish to register a new artifact store, do so with the register command: .. code:: bash zenml artifact-store register ARTIFACT_STORE_NAME --type ARTIFACT_STORE_TYPE [--OPTIONS] If you wish to list the artifact stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml artifact-store list If you wish to delete a particular artifact store, pass the name of the artifact store into the CLI with the following command: .. code:: bash zenml artifact-store delete ARTIFACT_STORE_NAME","title":"Customizing your Artifact Store"},{"location":"cli/#customizing-your-orchestrator","text":"An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. By default, ZenML initializes your repository with an orchestrator that runs everything on your local machine. If you wish to register a new orchestrator, do so with the register command: .. code:: bash zenml orchestrator register ORCHESTRATOR_NAME --type ORCHESTRATOR_TYPE [--ORCHESTRATOR_OPTIONS] If you wish to list the orchestrators that have already been registered within your ZenML project / repository, type: .. code:: bash zenml orchestrator list If you wish to delete a particular orchestrator, pass the name of the orchestrator into the CLI with the following command: .. code:: bash zenml orchestrator delete ORCHESTRATOR_NAME","title":"Customizing your Orchestrator"},{"location":"cli/#customizing-your-container-registry","text":"The container registry is where all the images that are used by a container-based orchestrator are stored. By default, a default ZenML local stack will not register a container registry. If you wish to register a new container registry, do so with the register command: zenml container-registry register REGISTRY_NAME --type REGISTRY_TYPE [--REGISTRY_OPTIONS] If you want the name of the current container registry, use the get command: zenml container-registry get To list all container registries available and registered for use, use the list command: zenml container-registry list For details about a particular container registry, use the describe command. By default (without a specific registry name passed in) it will describe the active or currently used container registry: zenml container-registry describe [REGISTRY_NAME] To delete a container registry (and all of its contents), use the delete command: zenml container-registry delete","title":"Customizing your Container Registry"},{"location":"cli/#administering-the-stack","text":"The stack is a grouping of your artifact store, your metadata store and your orchestrator. With the ZenML tool, switching from a local stack to a distributed cloud environment can be accomplished with just a few CLI commands. To register a new stack, you must already have registered the individual components of the stack using the commands listed above. Use the zenml stack register command to register your stack. It takes four arguments as in the following example: .. code:: bash zenml stack register STACK_NAME -m METADATA_STORE_NAME -a ARTIFACT_STORE_NAME -o ORCHESTRATOR_NAME Each corresponding argument should be the name you passed in as an identifier for the artifact store, metadata store or orchestrator when you originally registered it. To list the stacks that you have registered within your current ZenML project, type: .. code:: bash zenml stack list To delete a stack that you have previously registered, type: .. code:: bash zenml stack delete STACK_NAME By default, ZenML uses a local stack whereby all pipelines run on your local computer. If you wish to set a different stack as the current active stack to be used when running your pipeline, type: .. code:: bash zenml stack set STACK_NAME This changes a configuration property within your local environment. To see which stack is currently set as the default active stack, type: .. code:: bash zenml stack get","title":"Administering the Stack"},{"location":"cli/#global-variables","text":"TYPE_CHECKING click INITIALIZE_REPO OPT_IN_ANALYTICS OPT_OUT_ANALYTICS zenml_version_installed GIT_REPO_URL RUN_EXAMPLE EXAMPLES_GITHUB_REPO EXAMPLES_RUN_SCRIPT SHELL_EXECUTABLE ascii_arts This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"cli.metadata_store/","text":"module cli.metadata_store Global Variables TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Cli.metadata store"},{"location":"cli.metadata_store/#module-climetadata_store","text":"","title":"module cli.metadata_store"},{"location":"cli.metadata_store/#global-variables","text":"TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"cli.orchestrator/","text":"module cli.orchestrator Global Variables TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Cli.orchestrator"},{"location":"cli.orchestrator/#module-cliorchestrator","text":"","title":"module cli.orchestrator"},{"location":"cli.orchestrator/#global-variables","text":"TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"cli.pipeline/","text":"module cli.pipeline CLI to interact with pipelines. This file was automatically generated via lazydocs .","title":"Cli.pipeline"},{"location":"cli.pipeline/#module-clipipeline","text":"CLI to interact with pipelines. This file was automatically generated via lazydocs .","title":"module cli.pipeline"},{"location":"cli.stack/","text":"module cli.stack CLI for manipulating ZenML local and global config file. This file was automatically generated via lazydocs .","title":"Cli.stack"},{"location":"cli.stack/#module-clistack","text":"CLI for manipulating ZenML local and global config file. This file was automatically generated via lazydocs .","title":"module cli.stack"},{"location":"cli.utils/","text":"module cli.utils Global Variables TYPE_CHECKING function title title(text: str) \u2192 None Echo a title formatted string on the CLI. Args: text : Input text string. function confirmation confirmation(text: str, *args: Any, **kwargs: Any) \u2192 bool Echo a confirmation string on the CLI. Args: text : Input text string. *args : Args to be passed to click.confirm(). **kwargs : Kwargs to be passed to click.confirm(). Returns: Boolean based on user response. function declare declare(text: str) \u2192 None Echo a declaration on the CLI. Args: text : Input text string. function error error(text: str) \u2192 None Echo an error string on the CLI. Args: text : Input text string. Raises: click.ClickException when called. function warning warning(text: str) \u2192 None Echo a warning string on the CLI. Args: text : Input text string. function pretty_print pretty_print(obj: Any) \u2192 None Pretty print an object on the CLI. Args: obj : Any object with a str method defined. function print_table print_table(obj: List[Dict[str, Any]]) \u2192 None Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Args: obj : A List containing dictionaries. function format_component_list format_component_list( component_list: Mapping[str, ForwardRef('BaseComponent')], active_component: str ) \u2192 List[Dict[str, str]] Formats a list of components into a List of Dicts. This list of dicts can then be printed in a table style using cli_utils.print_table. Args: component_list : The component_list is a mapping of component key to component class with its relevant attributes active_component : The component that is currently active Returns: list_of_dicts : A list of all components with each component as a dict function print_component_properties print_component_properties(properties: Dict[str, str]) \u2192 None Prints the properties of a component. Args: properties : A dictionary of properties. function format_date format_date(dt: datetime, format: str = '%Y-%m-%d %H:%M:%S') \u2192 str Format a date into a string. Args: dt : Datetime object to be formatted. format : The format in string you want the datetime formatted to. Returns: Formatted string according to specification. function parse_unknown_options parse_unknown_options(args: List[str]) \u2192 Dict[str, Any] Parse unknown options from the CLI. Args: args : A list of strings from the CLI. Returns: Dict of parsed args. function activate_integrations activate_integrations(func: ~F) \u2192 ~F Decorator that activates all ZenML integrations. function install_package install_package(package: str) \u2192 None Installs pypi package into the current environment with pip function uninstall_package uninstall_package(package: str) \u2192 None Uninstalls pypi package from the current environment with pip This file was automatically generated via lazydocs .","title":"Cli.utils"},{"location":"cli.utils/#module-cliutils","text":"","title":"module cli.utils"},{"location":"cli.utils/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"cli.utils/#function-title","text":"title(text: str) \u2192 None Echo a title formatted string on the CLI. Args: text : Input text string.","title":"function title"},{"location":"cli.utils/#function-confirmation","text":"confirmation(text: str, *args: Any, **kwargs: Any) \u2192 bool Echo a confirmation string on the CLI. Args: text : Input text string. *args : Args to be passed to click.confirm(). **kwargs : Kwargs to be passed to click.confirm(). Returns: Boolean based on user response.","title":"function confirmation"},{"location":"cli.utils/#function-declare","text":"declare(text: str) \u2192 None Echo a declaration on the CLI. Args: text : Input text string.","title":"function declare"},{"location":"cli.utils/#function-error","text":"error(text: str) \u2192 None Echo an error string on the CLI. Args: text : Input text string. Raises: click.ClickException when called.","title":"function error"},{"location":"cli.utils/#function-warning","text":"warning(text: str) \u2192 None Echo a warning string on the CLI. Args: text : Input text string.","title":"function warning"},{"location":"cli.utils/#function-pretty_print","text":"pretty_print(obj: Any) \u2192 None Pretty print an object on the CLI. Args: obj : Any object with a str method defined.","title":"function pretty_print"},{"location":"cli.utils/#function-print_table","text":"print_table(obj: List[Dict[str, Any]]) \u2192 None Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Args: obj : A List containing dictionaries.","title":"function print_table"},{"location":"cli.utils/#function-format_component_list","text":"format_component_list( component_list: Mapping[str, ForwardRef('BaseComponent')], active_component: str ) \u2192 List[Dict[str, str]] Formats a list of components into a List of Dicts. This list of dicts can then be printed in a table style using cli_utils.print_table. Args: component_list : The component_list is a mapping of component key to component class with its relevant attributes active_component : The component that is currently active Returns: list_of_dicts : A list of all components with each component as a dict","title":"function format_component_list"},{"location":"cli.utils/#function-print_component_properties","text":"print_component_properties(properties: Dict[str, str]) \u2192 None Prints the properties of a component. Args: properties : A dictionary of properties.","title":"function print_component_properties"},{"location":"cli.utils/#function-format_date","text":"format_date(dt: datetime, format: str = '%Y-%m-%d %H:%M:%S') \u2192 str Format a date into a string. Args: dt : Datetime object to be formatted. format : The format in string you want the datetime formatted to. Returns: Formatted string according to specification.","title":"function format_date"},{"location":"cli.utils/#function-parse_unknown_options","text":"parse_unknown_options(args: List[str]) \u2192 Dict[str, Any] Parse unknown options from the CLI. Args: args : A list of strings from the CLI. Returns: Dict of parsed args.","title":"function parse_unknown_options"},{"location":"cli.utils/#function-activate_integrations","text":"activate_integrations(func: ~F) \u2192 ~F Decorator that activates all ZenML integrations.","title":"function activate_integrations"},{"location":"cli.utils/#function-install_package","text":"install_package(package: str) \u2192 None Installs pypi package into the current environment with pip","title":"function install_package"},{"location":"cli.utils/#function-uninstall_package","text":"uninstall_package(package: str) \u2192 None Uninstalls pypi package from the current environment with pip This file was automatically generated via lazydocs .","title":"function uninstall_package"},{"location":"cli.version/","text":"module cli.version Global Variables ascii_arts This file was automatically generated via lazydocs .","title":"Cli.version"},{"location":"cli.version/#module-cliversion","text":"","title":"module cli.version"},{"location":"cli.version/#global-variables","text":"ascii_arts This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"config.config_keys/","text":"module config.config_keys class ConfigKeys Class to validate dictionary configurations. classmethod get_keys get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. classmethod key_check key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key. class PipelineConfigurationKeys Keys for a pipeline configuration dict. classmethod get_keys get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. classmethod key_check key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key. class StepConfigurationKeys Keys for a step configuration dict. classmethod get_keys get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. classmethod key_check key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key. This file was automatically generated via lazydocs .","title":"Config.config keys"},{"location":"config.config_keys/#module-configconfig_keys","text":"","title":"module config.config_keys"},{"location":"config.config_keys/#class-configkeys","text":"Class to validate dictionary configurations.","title":"class ConfigKeys"},{"location":"config.config_keys/#classmethod-get_keys","text":"get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class.","title":"classmethod get_keys"},{"location":"config.config_keys/#classmethod-key_check","text":"key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key.","title":"classmethod key_check"},{"location":"config.config_keys/#class-pipelineconfigurationkeys","text":"Keys for a pipeline configuration dict.","title":"class PipelineConfigurationKeys"},{"location":"config.config_keys/#classmethod-get_keys_1","text":"get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class.","title":"classmethod get_keys"},{"location":"config.config_keys/#classmethod-key_check_1","text":"key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key.","title":"classmethod key_check"},{"location":"config.config_keys/#class-stepconfigurationkeys","text":"Keys for a step configuration dict.","title":"class StepConfigurationKeys"},{"location":"config.config_keys/#classmethod-get_keys_2","text":"get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class.","title":"classmethod get_keys"},{"location":"config.config_keys/#classmethod-key_check_2","text":"key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key. This file was automatically generated via lazydocs .","title":"classmethod key_check"},{"location":"config.constants/","text":"module config.constants Global Variables GLOBAL_CONFIG_NAME This file was automatically generated via lazydocs .","title":"Config.constants"},{"location":"config.constants/#module-configconstants","text":"","title":"module config.constants"},{"location":"config.constants/#global-variables","text":"GLOBAL_CONFIG_NAME This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"config.global_config/","text":"module config.global_config Global config for the ZenML installation. Global Variables GLOBAL_CONFIG_NAME class GlobalConfig Class definition for the global config. Defines global data such as unique user ID and whether they opted in for analytics. method __init__ __init__(**data: Any) We persist the attributes in the config file. For the global config, we want to persist the data as soon as it is initialized for the first time. method get_serialization_file_name get_serialization_file_name() \u2192 str Gets the global config dir for installed package. This file was automatically generated via lazydocs .","title":"Config.global config"},{"location":"config.global_config/#module-configglobal_config","text":"Global config for the ZenML installation.","title":"module config.global_config"},{"location":"config.global_config/#global-variables","text":"GLOBAL_CONFIG_NAME","title":"Global Variables"},{"location":"config.global_config/#class-globalconfig","text":"Class definition for the global config. Defines global data such as unique user ID and whether they opted in for analytics.","title":"class GlobalConfig"},{"location":"config.global_config/#method-__init__","text":"__init__(**data: Any) We persist the attributes in the config file. For the global config, we want to persist the data as soon as it is initialized for the first time.","title":"method __init__"},{"location":"config.global_config/#method-get_serialization_file_name","text":"get_serialization_file_name() \u2192 str Gets the global config dir for installed package. This file was automatically generated via lazydocs .","title":"method get_serialization_file_name"},{"location":"config/","text":"module config The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions. This file was automatically generated via lazydocs .","title":"Config"},{"location":"config/#module-config","text":"The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions. This file was automatically generated via lazydocs .","title":"module config"},{"location":"constants/","text":"module constants Global Variables APP_NAME CONFIG_VERSION GIT_REPO_URL ENV_ZENML_DEBUG ENV_ZENML_LOGGING_VERBOSITY ENV_ABSL_LOGGING_VERBOSITY ENV_ZENML_REPOSITORY_PATH ENV_ZENML_PREVENT_PIPELINE_EXECUTION IS_DEBUG_ENV ZENML_LOGGING_VERBOSITY ABSL_LOGGING_VERBOSITY ZENML_REGISTRY ZENML_BASE_IMAGE_NAME ZENML_TRAINER_IMAGE_NAME ZENML_DATAFLOW_IMAGE_NAME COMPARISON_NOTEBOOK EVALUATION_NOTEBOOK PREPROCESSING_FN TRAINER_FN GCP_ENTRYPOINT AWS_ENTRYPOINT K8S_ENTRYPOINT VALID_OPERATING_SYSTEMS REMOTE_FS_PREFIX SEGMENT_KEY_DEV SEGMENT_KEY_PROD SHOULD_PREVENT_PIPELINE_EXECUTION function handle_bool_env_var handle_bool_env_var(var: str, default: bool = False) \u2192 bool Converts normal env var to boolean function handle_int_env_var handle_int_env_var(var: str, default: int = 0) \u2192 int Converts normal env var to int This file was automatically generated via lazydocs .","title":"Constants"},{"location":"constants/#module-constants","text":"","title":"module constants"},{"location":"constants/#global-variables","text":"APP_NAME CONFIG_VERSION GIT_REPO_URL ENV_ZENML_DEBUG ENV_ZENML_LOGGING_VERBOSITY ENV_ABSL_LOGGING_VERBOSITY ENV_ZENML_REPOSITORY_PATH ENV_ZENML_PREVENT_PIPELINE_EXECUTION IS_DEBUG_ENV ZENML_LOGGING_VERBOSITY ABSL_LOGGING_VERBOSITY ZENML_REGISTRY ZENML_BASE_IMAGE_NAME ZENML_TRAINER_IMAGE_NAME ZENML_DATAFLOW_IMAGE_NAME COMPARISON_NOTEBOOK EVALUATION_NOTEBOOK PREPROCESSING_FN TRAINER_FN GCP_ENTRYPOINT AWS_ENTRYPOINT K8S_ENTRYPOINT VALID_OPERATING_SYSTEMS REMOTE_FS_PREFIX SEGMENT_KEY_DEV SEGMENT_KEY_PROD SHOULD_PREVENT_PIPELINE_EXECUTION","title":"Global Variables"},{"location":"constants/#function-handle_bool_env_var","text":"handle_bool_env_var(var: str, default: bool = False) \u2192 bool Converts normal env var to boolean","title":"function handle_bool_env_var"},{"location":"constants/#function-handle_int_env_var","text":"handle_int_env_var(var: str, default: int = 0) \u2192 int Converts normal env var to int This file was automatically generated via lazydocs .","title":"function handle_int_env_var"},{"location":"container_registries.base_container_registry/","text":"module container_registries.base_container_registry Base class for all container registries. class BaseContainerRegistry Base class for all ZenML container registries. method __init__ __init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseContainerRegistry instance. Args: repo_path : Path to the repository of this container registry. This file was automatically generated via lazydocs .","title":"Container registries.base container registry"},{"location":"container_registries.base_container_registry/#module-container_registriesbase_container_registry","text":"Base class for all container registries.","title":"module container_registries.base_container_registry"},{"location":"container_registries.base_container_registry/#class-basecontainerregistry","text":"Base class for all ZenML container registries.","title":"class BaseContainerRegistry"},{"location":"container_registries.base_container_registry/#method-__init__","text":"__init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseContainerRegistry instance. Args: repo_path : Path to the repository of this container registry. This file was automatically generated via lazydocs .","title":"method __init__"},{"location":"container_registries/","text":"module container_registries This file was automatically generated via lazydocs .","title":"Container registries"},{"location":"container_registries/#module-container_registries","text":"This file was automatically generated via lazydocs .","title":"module container_registries"},{"location":"core.base_component/","text":"module core.base_component Global Variables SUPERFLUOUS_OPTIONS_ATTRIBUTE_NAME class BaseComponent Class definition for the base config. The base component class defines the basic serialization / deserialization of various components used in ZenML. The logic of the serialization / deserialization is as follows: If a uuid is passed in, then the object is read from a file, so theconstructor becomes a query for an object that is assumed to already been serialized. * If a 'uuid` is NOT passed, then a new object is created with the default args (and any other args that are passed), and therefore a fresh serialization takes place. method __init__ __init__(serialization_dir: str, **values: Any) classmethod check_superfluous_options check_superfluous_options(values: Dict[str, Any]) \u2192 Dict[str, Any] Detects superfluous config values (usually read from an existing config file after the schema changed) and saves them in the classes _superfluous_options attribute. method delete delete() \u2192 None Deletes the persisted state of this object. method dict dict(**kwargs: Any) \u2192 Dict[str, Any] Removes private attributes from pydantic dict so they don't get stored in our config files. method get_serialization_dir get_serialization_dir() \u2192 str Return the dir where object is serialized. method get_serialization_file_name get_serialization_file_name() \u2192 str Return the name of the file where object is serialized. This has a sane default in cases where uuid is not passed externally, and therefore reading from a serialize file is not an option for the table. However, we still this function to go through without an exception, therefore the sane default. method get_serialization_full_path get_serialization_full_path() \u2192 str Returns the full path of the serialization file. method update update() \u2192 None Persist the current state of the component. Calling this will result in a persistent, stateful change in the system. This file was automatically generated via lazydocs .","title":"Core.base component"},{"location":"core.base_component/#module-corebase_component","text":"","title":"module core.base_component"},{"location":"core.base_component/#global-variables","text":"SUPERFLUOUS_OPTIONS_ATTRIBUTE_NAME","title":"Global Variables"},{"location":"core.base_component/#class-basecomponent","text":"Class definition for the base config. The base component class defines the basic serialization / deserialization of various components used in ZenML. The logic of the serialization / deserialization is as follows: If a uuid is passed in, then the object is read from a file, so theconstructor becomes a query for an object that is assumed to already been serialized. * If a 'uuid` is NOT passed, then a new object is created with the default args (and any other args that are passed), and therefore a fresh serialization takes place.","title":"class BaseComponent"},{"location":"core.base_component/#method-__init__","text":"__init__(serialization_dir: str, **values: Any)","title":"method __init__"},{"location":"core.base_component/#classmethod-check_superfluous_options","text":"check_superfluous_options(values: Dict[str, Any]) \u2192 Dict[str, Any] Detects superfluous config values (usually read from an existing config file after the schema changed) and saves them in the classes _superfluous_options attribute.","title":"classmethod check_superfluous_options"},{"location":"core.base_component/#method-delete","text":"delete() \u2192 None Deletes the persisted state of this object.","title":"method delete"},{"location":"core.base_component/#method-dict","text":"dict(**kwargs: Any) \u2192 Dict[str, Any] Removes private attributes from pydantic dict so they don't get stored in our config files.","title":"method dict"},{"location":"core.base_component/#method-get_serialization_dir","text":"get_serialization_dir() \u2192 str Return the dir where object is serialized.","title":"method get_serialization_dir"},{"location":"core.base_component/#method-get_serialization_file_name","text":"get_serialization_file_name() \u2192 str Return the name of the file where object is serialized. This has a sane default in cases where uuid is not passed externally, and therefore reading from a serialize file is not an option for the table. However, we still this function to go through without an exception, therefore the sane default.","title":"method get_serialization_file_name"},{"location":"core.base_component/#method-get_serialization_full_path","text":"get_serialization_full_path() \u2192 str Returns the full path of the serialization file.","title":"method get_serialization_full_path"},{"location":"core.base_component/#method-update","text":"update() \u2192 None Persist the current state of the component. Calling this will result in a persistent, stateful change in the system. This file was automatically generated via lazydocs .","title":"method update"},{"location":"core.component_factory/","text":"module core.component_factory Factory to register all components. Global Variables artifact_store_factory metadata_store_factory orchestrator_store_factory class ComponentFactory Definition of ComponentFactory to track all BaseComponent subclasses. All BaseComponents (including custom ones) are to be registered here. method __init__ __init__(name: str) Constructor for the factory. Args: name : Unique name for the factory. method get_component_key get_component_key(component: Type[BaseComponent]) \u2192 str Gets the key of a registered component. method get_components get_components() \u2192 Dict[str, Type[BaseComponent]] Return all components method get_single_component get_single_component(key: str) \u2192 Type[BaseComponent] Get a registered component from a key. method register register(name: str) \u2192 Callable[[Type[BaseComponent]], Type[BaseComponent]] Class decorator to register component classes to the internal registry. Args: name : The name of the component. Returns: A function which registers the class at this ComponentFactory. method register_component register_component(key: str, component: Type[BaseComponent]) \u2192 None Registers a single component class for a given key. This file was automatically generated via lazydocs .","title":"Core.component factory"},{"location":"core.component_factory/#module-corecomponent_factory","text":"Factory to register all components.","title":"module core.component_factory"},{"location":"core.component_factory/#global-variables","text":"artifact_store_factory metadata_store_factory orchestrator_store_factory","title":"Global Variables"},{"location":"core.component_factory/#class-componentfactory","text":"Definition of ComponentFactory to track all BaseComponent subclasses. All BaseComponents (including custom ones) are to be registered here.","title":"class ComponentFactory"},{"location":"core.component_factory/#method-__init__","text":"__init__(name: str) Constructor for the factory. Args: name : Unique name for the factory.","title":"method __init__"},{"location":"core.component_factory/#method-get_component_key","text":"get_component_key(component: Type[BaseComponent]) \u2192 str Gets the key of a registered component.","title":"method get_component_key"},{"location":"core.component_factory/#method-get_components","text":"get_components() \u2192 Dict[str, Type[BaseComponent]] Return all components","title":"method get_components"},{"location":"core.component_factory/#method-get_single_component","text":"get_single_component(key: str) \u2192 Type[BaseComponent] Get a registered component from a key.","title":"method get_single_component"},{"location":"core.component_factory/#method-register","text":"register(name: str) \u2192 Callable[[Type[BaseComponent]], Type[BaseComponent]] Class decorator to register component classes to the internal registry. Args: name : The name of the component. Returns: A function which registers the class at this ComponentFactory.","title":"method register"},{"location":"core.component_factory/#method-register_component","text":"register_component(key: str, component: Type[BaseComponent]) \u2192 None Registers a single component class for a given key. This file was automatically generated via lazydocs .","title":"method register_component"},{"location":"core.constants/","text":"module core.constants Global Variables ZENML_DIR_NAME This file was automatically generated via lazydocs .","title":"Core.constants"},{"location":"core.constants/#module-coreconstants","text":"","title":"module core.constants"},{"location":"core.constants/#global-variables","text":"ZENML_DIR_NAME This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"core.git_wrapper/","text":"module core.git_wrapper Wrapper class to handle Git integration Global Variables APP_NAME GIT_FOLDER_NAME class GitWrapper Wrapper class for Git. This class is responsible for handling git interactions, primarily handling versioning of different steps in pipelines. method __init__ __init__(repo_path: str) Initialize GitWrapper. Should be initialized by ZenML Repository. Args: repo_path: Raises: InvalidGitRepositoryError : If repository is not a git repository. NoSuchPathError : If the repo_path does not exist. method check_file_committed check_file_committed(file_path: str) \u2192 bool Checks file is committed. If yes, return True, else False. Args: file_path (str): Path to any file within the ZenML repo. method check_module_clean check_module_clean(source: str) \u2192 bool Returns True if all files within source's module are committed. Args: source : relative module path pointing to a Class. method checkout checkout( sha_or_branch: Optional[str] = None, directory: Optional[str] = None ) \u2192 None Wrapper for git checkout Args: sha_or_branch : hex string of len 40 representing git sha OR name of branch directory : relative path to directory to scope checkout method get_current_sha get_current_sha() \u2192 str Finds the git sha that each file within the module is currently on. method is_valid_source is_valid_source(source: str) \u2192 bool Checks whether the source_path is valid or not. Args: source (str): class_source e.g. this.module.Class[@pin]. method load_source_path_class load_source_path_class(source: str) \u2192 Type[Any] Loads a Python class from the source. Args: source : class_source e.g. this.module.Class[@sha] method reset reset(directory: Optional[str] = None) \u2192 None Wrapper for git reset HEAD <directory> . Args: directory : Relative path to directory to scope checkout method resolve_class resolve_class(class_: Type[Any]) \u2192 str Resolves a class into a serializable source string. Args: class_ : A Python Class reference. Returns: source_path e.g. this.module.Class[@pin]. method resolve_class_source resolve_class_source(class_source: str) \u2192 str Resolves class_source with an optional pin. Takes source (e.g. this.module.ClassName), and appends relevant sha to it if the files within module are all committed. If even one file is not committed, then returns source unchanged. Args: class_source (str): class_source e.g. this.module.Class method stash stash() \u2192 None Wrapper for git stash method stash_pop stash_pop() \u2192 None Wrapper for git stash pop. Only pops if there's something to pop. This file was automatically generated via lazydocs .","title":"Core.git wrapper"},{"location":"core.git_wrapper/#module-coregit_wrapper","text":"Wrapper class to handle Git integration","title":"module core.git_wrapper"},{"location":"core.git_wrapper/#global-variables","text":"APP_NAME GIT_FOLDER_NAME","title":"Global Variables"},{"location":"core.git_wrapper/#class-gitwrapper","text":"Wrapper class for Git. This class is responsible for handling git interactions, primarily handling versioning of different steps in pipelines.","title":"class GitWrapper"},{"location":"core.git_wrapper/#method-__init__","text":"__init__(repo_path: str) Initialize GitWrapper. Should be initialized by ZenML Repository. Args: repo_path: Raises: InvalidGitRepositoryError : If repository is not a git repository. NoSuchPathError : If the repo_path does not exist.","title":"method __init__"},{"location":"core.git_wrapper/#method-check_file_committed","text":"check_file_committed(file_path: str) \u2192 bool Checks file is committed. If yes, return True, else False. Args: file_path (str): Path to any file within the ZenML repo.","title":"method check_file_committed"},{"location":"core.git_wrapper/#method-check_module_clean","text":"check_module_clean(source: str) \u2192 bool Returns True if all files within source's module are committed. Args: source : relative module path pointing to a Class.","title":"method check_module_clean"},{"location":"core.git_wrapper/#method-checkout","text":"checkout( sha_or_branch: Optional[str] = None, directory: Optional[str] = None ) \u2192 None Wrapper for git checkout Args: sha_or_branch : hex string of len 40 representing git sha OR name of branch directory : relative path to directory to scope checkout","title":"method checkout"},{"location":"core.git_wrapper/#method-get_current_sha","text":"get_current_sha() \u2192 str Finds the git sha that each file within the module is currently on.","title":"method get_current_sha"},{"location":"core.git_wrapper/#method-is_valid_source","text":"is_valid_source(source: str) \u2192 bool Checks whether the source_path is valid or not. Args: source (str): class_source e.g. this.module.Class[@pin].","title":"method is_valid_source"},{"location":"core.git_wrapper/#method-load_source_path_class","text":"load_source_path_class(source: str) \u2192 Type[Any] Loads a Python class from the source. Args: source : class_source e.g. this.module.Class[@sha]","title":"method load_source_path_class"},{"location":"core.git_wrapper/#method-reset","text":"reset(directory: Optional[str] = None) \u2192 None Wrapper for git reset HEAD <directory> . Args: directory : Relative path to directory to scope checkout","title":"method reset"},{"location":"core.git_wrapper/#method-resolve_class","text":"resolve_class(class_: Type[Any]) \u2192 str Resolves a class into a serializable source string. Args: class_ : A Python Class reference. Returns: source_path e.g. this.module.Class[@pin].","title":"method resolve_class"},{"location":"core.git_wrapper/#method-resolve_class_source","text":"resolve_class_source(class_source: str) \u2192 str Resolves class_source with an optional pin. Takes source (e.g. this.module.ClassName), and appends relevant sha to it if the files within module are all committed. If even one file is not committed, then returns source unchanged. Args: class_source (str): class_source e.g. this.module.Class","title":"method resolve_class_source"},{"location":"core.git_wrapper/#method-stash","text":"stash() \u2192 None Wrapper for git stash","title":"method stash"},{"location":"core.git_wrapper/#method-stash_pop","text":"stash_pop() \u2192 None Wrapper for git stash pop. Only pops if there's something to pop. This file was automatically generated via lazydocs .","title":"method stash_pop"},{"location":"core.local_service/","text":"module core.local_service Global Variables TYPE_CHECKING REGISTERED_ARTIFACT_STORE REGISTERED_CONTAINER_REGISTRY REGISTERED_METADATA_STORE REGISTERED_ORCHESTRATOR REGISTERED_STACK class LocalService Definition of a local service that keeps track of all ZenML components. method __init__ __init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a LocalService instance. Args: repo_path : Path to the repository of this service. property artifact_stores Returns all registered artifact stores. property container_registries Returns all registered container registries. property metadata_stores Returns all registered metadata stores. property orchestrators Returns all registered orchestrators. method delete delete() \u2192 None Deletes the entire service. Dangerous operation method delete_artifact_store delete_artifact_store(key: str) \u2192 None Delete an artifact_store. Args: key : Unique key of artifact_store. method delete_container_registry delete_container_registry(key: str) \u2192 None Delete a container registry. Args: key : Unique key of the container registry. method delete_metadata_store delete_metadata_store(key: str) \u2192 None Delete a metadata store. Args: key : Unique key of metadata store. method delete_orchestrator delete_orchestrator(key: str) \u2192 None Delete a orchestrator. Args: key : Unique key of orchestrator. method delete_stack delete_stack(key: str) \u2192 None Delete a stack specified with a key. Args: key : Unique key of stack. method get_active_stack_key get_active_stack_key() \u2192 str Returns the active stack key. method get_artifact_store get_artifact_store(key: str) \u2192 BaseArtifactStore Return a single artifact store based on key. Args: key : Unique key of artifact store. Returns: Stack specified by key. method get_container_registry get_container_registry(key: str) \u2192 BaseContainerRegistry Return a single container registry based on key. Args: key : Unique key of a container registry. Returns: Container registry specified by key. method get_metadata_store get_metadata_store(key: str) \u2192 BaseMetadataStore Return a single metadata store based on key. Args: key : Unique key of metadata store. Returns: Metadata store specified by key. method get_orchestrator get_orchestrator(key: str) \u2192 BaseOrchestrator Return a single orchestrator based on key. Args: key : Unique key of orchestrator. Returns: Orchestrator specified by key. method get_serialization_file_name get_serialization_file_name() \u2192 str Return the name of the file where object is serialized. method get_stack get_stack(key: str) \u2192 BaseStack Return a single stack based on key. Args: key : Unique key of stack. Returns: Stack specified by key. method register_artifact_store register_artifact_store(key: str, artifact_store: 'BaseArtifactStore') \u2192 None Register an artifact store. Args: artifact_store : Artifact store to be registered. key : Unique key for the artifact store. method register_metadata_store register_metadata_store(key: str, metadata_store: 'BaseMetadataStore') \u2192 None Register a metadata store. Args: metadata_store : Metadata store to be registered. key : Unique key for the metadata store. method register_orchestrator register_orchestrator(key: str, orchestrator: 'BaseOrchestrator') \u2192 None Register an orchestrator. Args: orchestrator : Orchestrator to be registered. key : Unique key for the orchestrator. method set_active_stack_key set_active_stack_key(stack_key: str) \u2192 None Sets the active stack key. This file was automatically generated via lazydocs .","title":"Core.local service"},{"location":"core.local_service/#module-corelocal_service","text":"","title":"module core.local_service"},{"location":"core.local_service/#global-variables","text":"TYPE_CHECKING REGISTERED_ARTIFACT_STORE REGISTERED_CONTAINER_REGISTRY REGISTERED_METADATA_STORE REGISTERED_ORCHESTRATOR REGISTERED_STACK","title":"Global Variables"},{"location":"core.local_service/#class-localservice","text":"Definition of a local service that keeps track of all ZenML components.","title":"class LocalService"},{"location":"core.local_service/#method-__init__","text":"__init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a LocalService instance. Args: repo_path : Path to the repository of this service.","title":"method __init__"},{"location":"core.local_service/#property-artifact_stores","text":"Returns all registered artifact stores.","title":"property artifact_stores"},{"location":"core.local_service/#property-container_registries","text":"Returns all registered container registries.","title":"property container_registries"},{"location":"core.local_service/#property-metadata_stores","text":"Returns all registered metadata stores.","title":"property metadata_stores"},{"location":"core.local_service/#property-orchestrators","text":"Returns all registered orchestrators.","title":"property orchestrators"},{"location":"core.local_service/#method-delete","text":"delete() \u2192 None Deletes the entire service. Dangerous operation","title":"method delete"},{"location":"core.local_service/#method-delete_artifact_store","text":"delete_artifact_store(key: str) \u2192 None Delete an artifact_store. Args: key : Unique key of artifact_store.","title":"method delete_artifact_store"},{"location":"core.local_service/#method-delete_container_registry","text":"delete_container_registry(key: str) \u2192 None Delete a container registry. Args: key : Unique key of the container registry.","title":"method delete_container_registry"},{"location":"core.local_service/#method-delete_metadata_store","text":"delete_metadata_store(key: str) \u2192 None Delete a metadata store. Args: key : Unique key of metadata store.","title":"method delete_metadata_store"},{"location":"core.local_service/#method-delete_orchestrator","text":"delete_orchestrator(key: str) \u2192 None Delete a orchestrator. Args: key : Unique key of orchestrator.","title":"method delete_orchestrator"},{"location":"core.local_service/#method-delete_stack","text":"delete_stack(key: str) \u2192 None Delete a stack specified with a key. Args: key : Unique key of stack.","title":"method delete_stack"},{"location":"core.local_service/#method-get_active_stack_key","text":"get_active_stack_key() \u2192 str Returns the active stack key.","title":"method get_active_stack_key"},{"location":"core.local_service/#method-get_artifact_store","text":"get_artifact_store(key: str) \u2192 BaseArtifactStore Return a single artifact store based on key. Args: key : Unique key of artifact store. Returns: Stack specified by key.","title":"method get_artifact_store"},{"location":"core.local_service/#method-get_container_registry","text":"get_container_registry(key: str) \u2192 BaseContainerRegistry Return a single container registry based on key. Args: key : Unique key of a container registry. Returns: Container registry specified by key.","title":"method get_container_registry"},{"location":"core.local_service/#method-get_metadata_store","text":"get_metadata_store(key: str) \u2192 BaseMetadataStore Return a single metadata store based on key. Args: key : Unique key of metadata store. Returns: Metadata store specified by key.","title":"method get_metadata_store"},{"location":"core.local_service/#method-get_orchestrator","text":"get_orchestrator(key: str) \u2192 BaseOrchestrator Return a single orchestrator based on key. Args: key : Unique key of orchestrator. Returns: Orchestrator specified by key.","title":"method get_orchestrator"},{"location":"core.local_service/#method-get_serialization_file_name","text":"get_serialization_file_name() \u2192 str Return the name of the file where object is serialized.","title":"method get_serialization_file_name"},{"location":"core.local_service/#method-get_stack","text":"get_stack(key: str) \u2192 BaseStack Return a single stack based on key. Args: key : Unique key of stack. Returns: Stack specified by key.","title":"method get_stack"},{"location":"core.local_service/#method-register_artifact_store","text":"register_artifact_store(key: str, artifact_store: 'BaseArtifactStore') \u2192 None Register an artifact store. Args: artifact_store : Artifact store to be registered. key : Unique key for the artifact store.","title":"method register_artifact_store"},{"location":"core.local_service/#method-register_metadata_store","text":"register_metadata_store(key: str, metadata_store: 'BaseMetadataStore') \u2192 None Register a metadata store. Args: metadata_store : Metadata store to be registered. key : Unique key for the metadata store.","title":"method register_metadata_store"},{"location":"core.local_service/#method-register_orchestrator","text":"register_orchestrator(key: str, orchestrator: 'BaseOrchestrator') \u2192 None Register an orchestrator. Args: orchestrator : Orchestrator to be registered. key : Unique key for the orchestrator.","title":"method register_orchestrator"},{"location":"core.local_service/#method-set_active_stack_key","text":"set_active_stack_key(stack_key: str) \u2192 None Sets the active stack key. This file was automatically generated via lazydocs .","title":"method set_active_stack_key"},{"location":"core.mapping_utils/","text":"module core.mapping_utils function get_key_from_uuid get_key_from_uuid(uuid: UUID, mapping: Dict[str, UUIDSourceTuple]) \u2192 str Return the key that points to a certain uuid in a mapping. Args: uuid : uuid to query. mapping : Dict mapping keys to UUIDs and source information. Returns: Returns the key from the mapping. function get_component_from_key get_component_from_key( key: str, mapping: Dict[str, UUIDSourceTuple], repo_path: str ) \u2192 BaseComponent Given a key and a mapping, return an initialized component. Args: key : Unique key. mapping : Dict of type str -> UUIDSourceTuple. repo_path : Path to the repo from which to load the component. Returns: An object which is a subclass of type BaseComponent. function get_components_from_store get_components_from_store( store_name: str, mapping: Dict[str, UUIDSourceTuple], repo_path: str ) \u2192 Dict[str, BaseComponent] Returns a list of components from a store. Args: store_name : Name of the store. mapping : Dict of type str -> UUIDSourceTuple. repo_path : Path to the repo from which to load the components. Returns: A dict of objects which are a subclass of type BaseComponent. class UUIDSourceTuple Container used to store UUID and source information of a single BaseComponent subclass. Attributes: uuid : Identifier of the BaseComponent source : Contains the fully qualified class name and information about a git hash/tag. E.g. foo.bar.BaseComponentSubclass@git_tag This file was automatically generated via lazydocs .","title":"Core.mapping utils"},{"location":"core.mapping_utils/#module-coremapping_utils","text":"","title":"module core.mapping_utils"},{"location":"core.mapping_utils/#function-get_key_from_uuid","text":"get_key_from_uuid(uuid: UUID, mapping: Dict[str, UUIDSourceTuple]) \u2192 str Return the key that points to a certain uuid in a mapping. Args: uuid : uuid to query. mapping : Dict mapping keys to UUIDs and source information. Returns: Returns the key from the mapping.","title":"function get_key_from_uuid"},{"location":"core.mapping_utils/#function-get_component_from_key","text":"get_component_from_key( key: str, mapping: Dict[str, UUIDSourceTuple], repo_path: str ) \u2192 BaseComponent Given a key and a mapping, return an initialized component. Args: key : Unique key. mapping : Dict of type str -> UUIDSourceTuple. repo_path : Path to the repo from which to load the component. Returns: An object which is a subclass of type BaseComponent.","title":"function get_component_from_key"},{"location":"core.mapping_utils/#function-get_components_from_store","text":"get_components_from_store( store_name: str, mapping: Dict[str, UUIDSourceTuple], repo_path: str ) \u2192 Dict[str, BaseComponent] Returns a list of components from a store. Args: store_name : Name of the store. mapping : Dict of type str -> UUIDSourceTuple. repo_path : Path to the repo from which to load the components. Returns: A dict of objects which are a subclass of type BaseComponent.","title":"function get_components_from_store"},{"location":"core.mapping_utils/#class-uuidsourcetuple","text":"Container used to store UUID and source information of a single BaseComponent subclass. Attributes: uuid : Identifier of the BaseComponent source : Contains the fully qualified class name and information about a git hash/tag. E.g. foo.bar.BaseComponentSubclass@git_tag This file was automatically generated via lazydocs .","title":"class UUIDSourceTuple"},{"location":"core/","text":"module core The core module is where all the base ZenML functionality is defined, including a Pydantic base class for components, a git wrapper and a class for ZenML's own repository methods. This module is also where the local service functionality (which keeps track of all the ZenML components) is defined. Every ZenML project has its own ZenML repository, and the repo module is where associated methods are defined. The repo.init_repo method is where all our functionality is kickstarted when you first initialize everything through the `zenml init CLI command. This file was automatically generated via lazydocs .","title":"Core"},{"location":"core/#module-core","text":"The core module is where all the base ZenML functionality is defined, including a Pydantic base class for components, a git wrapper and a class for ZenML's own repository methods. This module is also where the local service functionality (which keeps track of all the ZenML components) is defined. Every ZenML project has its own ZenML repository, and the repo module is where associated methods are defined. The repo.init_repo method is where all our functionality is kickstarted when you first initialize everything through the `zenml init CLI command. This file was automatically generated via lazydocs .","title":"module core"},{"location":"core.repo/","text":"module core.repo Base ZenML repository Global Variables ZENML_DIR_NAME GET_PIPELINE GET_PIPELINES SET_STACK class Repository ZenML repository definition. Every ZenML project exists inside a ZenML repository. method __init__ __init__(path: Optional[str] = None) Construct reference to a ZenML repository. Args: path (str): Path to root of repository method clean clean() \u2192 None Deletes associated metadata store, pipelines dir and artifacts method get_active_stack get_active_stack() \u2192 BaseStack Get the active stack from global config. Returns: Currently active stack. method get_active_stack_key get_active_stack_key() \u2192 str Get the active stack key from global config. Returns: Currently active stacks key. method get_git_wrapper get_git_wrapper() \u2192 GitWrapper Returns the git wrapper for the repo. method get_service get_service() \u2192 LocalService Returns the active service. For now, always local. method init_repo init_repo(path: str = '/home/apenner/PycharmProjects/zenml') \u2192 None Initializes a ZenML repository. Args: path : Path where the ZenML repository should be created. Raises: InitializationException : If a ZenML repository already exists at the given path. This file was automatically generated via lazydocs .","title":"Core.repo"},{"location":"core.repo/#module-corerepo","text":"Base ZenML repository","title":"module core.repo"},{"location":"core.repo/#global-variables","text":"ZENML_DIR_NAME GET_PIPELINE GET_PIPELINES SET_STACK","title":"Global Variables"},{"location":"core.repo/#class-repository","text":"ZenML repository definition. Every ZenML project exists inside a ZenML repository.","title":"class Repository"},{"location":"core.repo/#method-__init__","text":"__init__(path: Optional[str] = None) Construct reference to a ZenML repository. Args: path (str): Path to root of repository","title":"method __init__"},{"location":"core.repo/#method-clean","text":"clean() \u2192 None Deletes associated metadata store, pipelines dir and artifacts","title":"method clean"},{"location":"core.repo/#method-get_active_stack","text":"get_active_stack() \u2192 BaseStack Get the active stack from global config. Returns: Currently active stack.","title":"method get_active_stack"},{"location":"core.repo/#method-get_active_stack_key","text":"get_active_stack_key() \u2192 str Get the active stack key from global config. Returns: Currently active stacks key.","title":"method get_active_stack_key"},{"location":"core.repo/#method-get_git_wrapper","text":"get_git_wrapper() \u2192 GitWrapper Returns the git wrapper for the repo.","title":"method get_git_wrapper"},{"location":"core.repo/#method-get_service","text":"get_service() \u2192 LocalService Returns the active service. For now, always local.","title":"method get_service"},{"location":"core.repo/#method-init_repo","text":"init_repo(path: str = '/home/apenner/PycharmProjects/zenml') \u2192 None Initializes a ZenML repository. Args: path : Path where the ZenML repository should be created. Raises: InitializationException : If a ZenML repository already exists at the given path. This file was automatically generated via lazydocs .","title":"method init_repo"},{"location":"core.utils/","text":"module core.utils function define_json_config_settings_source define_json_config_settings_source( config_dir: str, config_name: str ) \u2192 Callable[[ForwardRef('BaseSettings')], Dict[str, Any]] Define a function to essentially deserialize a model from a serialized json config. Args: config_dir : A path to a dir where we want the config file to exist. config_name : Full name of config file. Returns: A json_config_settings_source callable reading from the passed path. function generate_customise_sources generate_customise_sources( file_dir: str, file_name: str ) \u2192 Callable[[Type[Config], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]]], Tuple[Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], ]] Generate a customise_sources function as defined here: https://pydantic-docs.helpmanual.io/usage/settings/. This function generates a function that configures the priorities of the sources through which the model is loaded. The important thing to note here is that the define_json_config_settings_source is dynamically generated with the provided file_dir and file_name. This allows us to dynamically generate a file name for the serialization and deserialization of the model. Args: file_dir : Dir where file is stored. file_name : Name of the file to persist. Returns: A customise_sources class method to be defined the a Pydantic BaseSettings inner Config class. This file was automatically generated via lazydocs .","title":"Core.utils"},{"location":"core.utils/#module-coreutils","text":"","title":"module core.utils"},{"location":"core.utils/#function-define_json_config_settings_source","text":"define_json_config_settings_source( config_dir: str, config_name: str ) \u2192 Callable[[ForwardRef('BaseSettings')], Dict[str, Any]] Define a function to essentially deserialize a model from a serialized json config. Args: config_dir : A path to a dir where we want the config file to exist. config_name : Full name of config file. Returns: A json_config_settings_source callable reading from the passed path.","title":"function define_json_config_settings_source"},{"location":"core.utils/#function-generate_customise_sources","text":"generate_customise_sources( file_dir: str, file_name: str ) \u2192 Callable[[Type[Config], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]]], Tuple[Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], ]] Generate a customise_sources function as defined here: https://pydantic-docs.helpmanual.io/usage/settings/. This function generates a function that configures the priorities of the sources through which the model is loaded. The important thing to note here is that the define_json_config_settings_source is dynamically generated with the provided file_dir and file_name. This allows us to dynamically generate a file name for the serialization and deserialization of the model. Args: file_dir : Dir where file is stored. file_name : Name of the file to persist. Returns: A customise_sources class method to be defined the a Pydantic BaseSettings inner Config class. This file was automatically generated via lazydocs .","title":"function generate_customise_sources"},{"location":"enums/","text":"module enums class ArtifactStoreTypes All supported Artifact Store types. class MLMetadataTypes All supported ML Metadata types. class OrchestratorTypes All supported Orchestrator types class StackTypes All supported Stack types. class ExecutionStatus Enum that represents the current status of a step or pipeline run. class LoggingLevels Enum for logging levels. This file was automatically generated via lazydocs .","title":"Enums"},{"location":"enums/#module-enums","text":"","title":"module enums"},{"location":"enums/#class-artifactstoretypes","text":"All supported Artifact Store types.","title":"class ArtifactStoreTypes"},{"location":"enums/#class-mlmetadatatypes","text":"All supported ML Metadata types.","title":"class MLMetadataTypes"},{"location":"enums/#class-orchestratortypes","text":"All supported Orchestrator types","title":"class OrchestratorTypes"},{"location":"enums/#class-stacktypes","text":"All supported Stack types.","title":"class StackTypes"},{"location":"enums/#class-executionstatus","text":"Enum that represents the current status of a step or pipeline run.","title":"class ExecutionStatus"},{"location":"enums/#class-logginglevels","text":"Enum for logging levels. This file was automatically generated via lazydocs .","title":"class LoggingLevels"},{"location":"exceptions/","text":"module exceptions ZenML specific exception definitions Global Variables TYPE_CHECKING class InitializationException Raises exception when a function is run before zenml initialization. method __init__ __init__(message: str = 'ZenML config is none. Did you do `zenml init`?') class EmptyDatasourceException Raises exception when a datasource data is accessed without running an associated pipeline. method __init__ __init__( message: str = 'This datasource has not been used in any pipelines, therefore the associated data has no versions. Please use this datasource in any ZenML pipeline with `pipeline.add_datasource(datasource)`' ) class DoesNotExistException Raises exception when the entity does not exist in the system but an action is being done that requires it to be present. method __init__ __init__(message: str) class AlreadyExistsException Raises exception when the name already exist in the system but an action is trying to create a resource with the same name. method __init__ __init__(message: Optional[str] = None, name: str = '', resource_type: str = '') class PipelineNotSucceededException Raises exception when trying to fetch artifacts from a not succeeded pipeline. method __init__ __init__(name: str = '', message: str = '{} is not yet completed successfully.') class GitException Raises exception when a problem occurs in git resolution. method __init__ __init__( message: str = 'There is a problem with git resolution. Please make sure that all relevant files are committed.' ) class StepInterfaceError Raises exception when interacting with the Step interface in an unsupported way. class StepContextError Raises exception when interacting with a StepContext in an unsupported way. class PipelineInterfaceError Raises exception when interacting with the Pipeline interface in an unsupported way. class ArtifactInterfaceError Raises exception when interacting with the Artifact interface in an unsupported way. class PipelineConfigurationError Raises exceptions when a pipeline configuration contains invalid values. class MissingStepParameterError Raises exceptions when a step parameter is missing when running a pipeline. method __init__ __init__( step_name: str, missing_parameters: List[str], config_class: Type[ForwardRef('BaseStepConfig')] ) Initializes a MissingStepParameterError object. Args: step_name : Name of the step for which one or more parameters are missing. missing_parameters : Names of all parameters which are missing. config_class : Class of the configuration object for which the parameters are missing. class IntegrationError Raises exceptions when a requested integration can not be activated. class DuplicateRunNameError Raises exception when a run with the same name already exists. method __init__ __init__( message: str = 'Unable to run a pipeline with a run name that already exists.' ) This file was automatically generated via lazydocs .","title":"Exceptions"},{"location":"exceptions/#module-exceptions","text":"ZenML specific exception definitions","title":"module exceptions"},{"location":"exceptions/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"exceptions/#class-initializationexception","text":"Raises exception when a function is run before zenml initialization.","title":"class InitializationException"},{"location":"exceptions/#method-__init__","text":"__init__(message: str = 'ZenML config is none. Did you do `zenml init`?')","title":"method __init__"},{"location":"exceptions/#class-emptydatasourceexception","text":"Raises exception when a datasource data is accessed without running an associated pipeline.","title":"class EmptyDatasourceException"},{"location":"exceptions/#method-__init___1","text":"__init__( message: str = 'This datasource has not been used in any pipelines, therefore the associated data has no versions. Please use this datasource in any ZenML pipeline with `pipeline.add_datasource(datasource)`' )","title":"method __init__"},{"location":"exceptions/#class-doesnotexistexception","text":"Raises exception when the entity does not exist in the system but an action is being done that requires it to be present.","title":"class DoesNotExistException"},{"location":"exceptions/#method-__init___2","text":"__init__(message: str)","title":"method __init__"},{"location":"exceptions/#class-alreadyexistsexception","text":"Raises exception when the name already exist in the system but an action is trying to create a resource with the same name.","title":"class AlreadyExistsException"},{"location":"exceptions/#method-__init___3","text":"__init__(message: Optional[str] = None, name: str = '', resource_type: str = '')","title":"method __init__"},{"location":"exceptions/#class-pipelinenotsucceededexception","text":"Raises exception when trying to fetch artifacts from a not succeeded pipeline.","title":"class PipelineNotSucceededException"},{"location":"exceptions/#method-__init___4","text":"__init__(name: str = '', message: str = '{} is not yet completed successfully.')","title":"method __init__"},{"location":"exceptions/#class-gitexception","text":"Raises exception when a problem occurs in git resolution.","title":"class GitException"},{"location":"exceptions/#method-__init___5","text":"__init__( message: str = 'There is a problem with git resolution. Please make sure that all relevant files are committed.' )","title":"method __init__"},{"location":"exceptions/#class-stepinterfaceerror","text":"Raises exception when interacting with the Step interface in an unsupported way.","title":"class StepInterfaceError"},{"location":"exceptions/#class-stepcontexterror","text":"Raises exception when interacting with a StepContext in an unsupported way.","title":"class StepContextError"},{"location":"exceptions/#class-pipelineinterfaceerror","text":"Raises exception when interacting with the Pipeline interface in an unsupported way.","title":"class PipelineInterfaceError"},{"location":"exceptions/#class-artifactinterfaceerror","text":"Raises exception when interacting with the Artifact interface in an unsupported way.","title":"class ArtifactInterfaceError"},{"location":"exceptions/#class-pipelineconfigurationerror","text":"Raises exceptions when a pipeline configuration contains invalid values.","title":"class PipelineConfigurationError"},{"location":"exceptions/#class-missingstepparametererror","text":"Raises exceptions when a step parameter is missing when running a pipeline.","title":"class MissingStepParameterError"},{"location":"exceptions/#method-__init___6","text":"__init__( step_name: str, missing_parameters: List[str], config_class: Type[ForwardRef('BaseStepConfig')] ) Initializes a MissingStepParameterError object. Args: step_name : Name of the step for which one or more parameters are missing. missing_parameters : Names of all parameters which are missing. config_class : Class of the configuration object for which the parameters are missing.","title":"method __init__"},{"location":"exceptions/#class-integrationerror","text":"Raises exceptions when a requested integration can not be activated.","title":"class IntegrationError"},{"location":"exceptions/#class-duplicaterunnameerror","text":"Raises exception when a run with the same name already exists.","title":"class DuplicateRunNameError"},{"location":"exceptions/#method-__init___7","text":"__init__( message: str = 'Unable to run a pipeline with a run name that already exists.' ) This file was automatically generated via lazydocs .","title":"method __init__"},{"location":"integrations.airflow/","text":"module integrations.airflow The Airflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Airflow orchestrator with the CLI tool, then bootstrap using the zenml orchestrator up command. Global Variables AIRFLOW class AirflowIntegration Definition of Airflow Integration for ZenML. classmethod activate activate() Activates all classes required for the airflow integration. This file was automatically generated via lazydocs .","title":"Integrations.airflow"},{"location":"integrations.airflow/#module-integrationsairflow","text":"The Airflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Airflow orchestrator with the CLI tool, then bootstrap using the zenml orchestrator up command.","title":"module integrations.airflow"},{"location":"integrations.airflow/#global-variables","text":"AIRFLOW","title":"Global Variables"},{"location":"integrations.airflow/#class-airflowintegration","text":"Definition of Airflow Integration for ZenML.","title":"class AirflowIntegration"},{"location":"integrations.airflow/#classmethod-activate","text":"activate() Activates all classes required for the airflow integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"integrations.airflow.orchestrators.airflow_component/","text":"module integrations.airflow.orchestrators.airflow_component Definition for Airflow component for TFX. class AirflowComponent Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow. method __init__ __init__( parent_dag: DAG, pipeline_node: PipelineNode, mlmd_connection: Metadata, pipeline_info: PipelineInfo, pipeline_runtime_spec: PipelineRuntimeSpec, executor_spec: Optional[Message] = None, custom_driver_spec: Optional[Message] = None ) \u2192 None Constructs an Airflow implementation of TFX component. Args: parent_dag : The airflow DAG that this component is contained in. pipeline_node : The specification of the node to launch. mlmd_connection : ML metadata connection info. pipeline_info : The information of the pipeline that this node runs in. pipeline_runtime_spec : The runtime information of the pipeline that this node runs in. executor_spec : Specification for the executor of the node. custom_driver_spec : Specification for custom driver. property dag Returns the Operator's DAG if set, otherwise raises an error property dag_id Returns dag id if it has one or an adhoc + owner property downstream_list @property: list of tasks directly downstream property downstream_task_ids @property: set of ids of tasks directly downstream property inherits_from_dummy_operator Used to determine if an Operator is inherited from DummyOperator property leaves Required by TaskMixin property log Returns a logger. property output Returns reference to XCom pushed by current operator property priority_weight_total Total priority weight for the task. It might include all upstream or downstream tasks. depending on the weight rule. WeightRule.ABSOLUTE - only own weight WeightRule.DOWNSTREAM - adds priority weight of all downstream tasks WeightRule.UPSTREAM - adds priority weight of all upstream tasks property roots Required by TaskMixin property task_type @property: type of the task property upstream_list @property: list of tasks directly upstream property upstream_task_ids @property: set of ids of tasks directly upstream This file was automatically generated via lazydocs .","title":"Integrations.airflow.orchestrators.airflow component"},{"location":"integrations.airflow.orchestrators.airflow_component/#module-integrationsairfloworchestratorsairflow_component","text":"Definition for Airflow component for TFX.","title":"module integrations.airflow.orchestrators.airflow_component"},{"location":"integrations.airflow.orchestrators.airflow_component/#class-airflowcomponent","text":"Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow.","title":"class AirflowComponent"},{"location":"integrations.airflow.orchestrators.airflow_component/#method-__init__","text":"__init__( parent_dag: DAG, pipeline_node: PipelineNode, mlmd_connection: Metadata, pipeline_info: PipelineInfo, pipeline_runtime_spec: PipelineRuntimeSpec, executor_spec: Optional[Message] = None, custom_driver_spec: Optional[Message] = None ) \u2192 None Constructs an Airflow implementation of TFX component. Args: parent_dag : The airflow DAG that this component is contained in. pipeline_node : The specification of the node to launch. mlmd_connection : ML metadata connection info. pipeline_info : The information of the pipeline that this node runs in. pipeline_runtime_spec : The runtime information of the pipeline that this node runs in. executor_spec : Specification for the executor of the node. custom_driver_spec : Specification for custom driver.","title":"method __init__"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-dag","text":"Returns the Operator's DAG if set, otherwise raises an error","title":"property dag"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-dag_id","text":"Returns dag id if it has one or an adhoc + owner","title":"property dag_id"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-downstream_list","text":"@property: list of tasks directly downstream","title":"property downstream_list"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-downstream_task_ids","text":"@property: set of ids of tasks directly downstream","title":"property downstream_task_ids"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-inherits_from_dummy_operator","text":"Used to determine if an Operator is inherited from DummyOperator","title":"property inherits_from_dummy_operator"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-leaves","text":"Required by TaskMixin","title":"property leaves"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-log","text":"Returns a logger.","title":"property log"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-output","text":"Returns reference to XCom pushed by current operator","title":"property output"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-priority_weight_total","text":"Total priority weight for the task. It might include all upstream or downstream tasks. depending on the weight rule. WeightRule.ABSOLUTE - only own weight WeightRule.DOWNSTREAM - adds priority weight of all downstream tasks WeightRule.UPSTREAM - adds priority weight of all upstream tasks","title":"property priority_weight_total"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-roots","text":"Required by TaskMixin","title":"property roots"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-task_type","text":"@property: type of the task","title":"property task_type"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-upstream_list","text":"@property: list of tasks directly upstream","title":"property upstream_list"},{"location":"integrations.airflow.orchestrators.airflow_component/#property-upstream_task_ids","text":"@property: set of ids of tasks directly upstream This file was automatically generated via lazydocs .","title":"property upstream_task_ids"},{"location":"integrations.airflow.orchestrators.airflow_dag_runner/","text":"module integrations.airflow.orchestrators.airflow_dag_runner Definition of Airflow TFX runner. This is an unmodified copy from the TFX source code (outside of superficial, stylistic changes) Global Variables TYPE_CHECKING class AirflowPipelineConfig Pipeline config for AirflowDagRunner. method __init__ __init__(airflow_dag_config: Optional[Dict[str, Any]] = None, **kwargs: Any) Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config : Configs of Airflow DAG model. See https : //airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs : keyword args for PipelineConfig. class AirflowDagRunner Tfx runner on Airflow. method __init__ __init__(config: Optional[Dict[str, Any], AirflowPipelineConfig] = None) Creates an instance of AirflowDagRunner. Args: config : Optional Airflow pipeline config for customizing the launching of each component. property config method run run(pipeline: Pipeline, run_name: str = '') \u2192 DAG Deploys given logical pipeline on Airflow. Args: pipeline : Logical pipeline containing pipeline args and comps. run_name : Optional name for the run. Returns: An Airflow DAG. This file was automatically generated via lazydocs .","title":"Integrations.airflow.orchestrators.airflow dag runner"},{"location":"integrations.airflow.orchestrators.airflow_dag_runner/#module-integrationsairfloworchestratorsairflow_dag_runner","text":"Definition of Airflow TFX runner. This is an unmodified copy from the TFX source code (outside of superficial, stylistic changes)","title":"module integrations.airflow.orchestrators.airflow_dag_runner"},{"location":"integrations.airflow.orchestrators.airflow_dag_runner/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"integrations.airflow.orchestrators.airflow_dag_runner/#class-airflowpipelineconfig","text":"Pipeline config for AirflowDagRunner.","title":"class AirflowPipelineConfig"},{"location":"integrations.airflow.orchestrators.airflow_dag_runner/#method-__init__","text":"__init__(airflow_dag_config: Optional[Dict[str, Any]] = None, **kwargs: Any) Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config : Configs of Airflow DAG model. See https : //airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs : keyword args for PipelineConfig.","title":"method __init__"},{"location":"integrations.airflow.orchestrators.airflow_dag_runner/#class-airflowdagrunner","text":"Tfx runner on Airflow.","title":"class AirflowDagRunner"},{"location":"integrations.airflow.orchestrators.airflow_dag_runner/#method-__init___1","text":"__init__(config: Optional[Dict[str, Any], AirflowPipelineConfig] = None) Creates an instance of AirflowDagRunner. Args: config : Optional Airflow pipeline config for customizing the launching of each component.","title":"method __init__"},{"location":"integrations.airflow.orchestrators.airflow_dag_runner/#property-config","text":"","title":"property config"},{"location":"integrations.airflow.orchestrators.airflow_dag_runner/#method-run","text":"run(pipeline: Pipeline, run_name: str = '') \u2192 DAG Deploys given logical pipeline on Airflow. Args: pipeline : Logical pipeline containing pipeline args and comps. run_name : Optional name for the run. Returns: An Airflow DAG. This file was automatically generated via lazydocs .","title":"method run"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/","text":"module integrations.airflow.orchestrators.airflow_orchestrator Global Variables TYPE_CHECKING AIRFLOW_ROOT_DIR class AirflowOrchestrator Orchestrator responsible for running pipelines using Airflow. method __init__ __init__(**values: Any) Sets environment variables to configure airflow. property dags_directory Returns path to the airflow dags directory. property is_running Returns whether the airflow daemon is currently running. property log_file Returns path to the airflow log file. property password_file Returns path to the webserver password file. property pid_file Returns path to the daemon PID file. method down down() \u2192 None Stops the airflow daemon if necessary and tears down resources. method pre_run pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Args: pipeline : Pipeline that will be run. caller_filepath : Path to the file in which pipeline.run() was called. This contains the airflow DAG that is returned by the run() method. Raises: RuntimeError : If airflow is not running. method run run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 DAG Prepares the pipeline so it can be run in Airflow. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Unused argument to conform with base class signature. classmethod set_airflow_home set_airflow_home(values: Dict[str, Any]) \u2192 Dict[str, Any] Sets airflow home according to orchestrator UUID. method up up() \u2192 None Ensures that Airflow is running. This file was automatically generated via lazydocs .","title":"Integrations.airflow.orchestrators.airflow orchestrator"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#module-integrationsairfloworchestratorsairflow_orchestrator","text":"","title":"module integrations.airflow.orchestrators.airflow_orchestrator"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#global-variables","text":"TYPE_CHECKING AIRFLOW_ROOT_DIR","title":"Global Variables"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#class-airfloworchestrator","text":"Orchestrator responsible for running pipelines using Airflow.","title":"class AirflowOrchestrator"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#method-__init__","text":"__init__(**values: Any) Sets environment variables to configure airflow.","title":"method __init__"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#property-dags_directory","text":"Returns path to the airflow dags directory.","title":"property dags_directory"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#property-is_running","text":"Returns whether the airflow daemon is currently running.","title":"property is_running"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#property-log_file","text":"Returns path to the airflow log file.","title":"property log_file"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#property-password_file","text":"Returns path to the webserver password file.","title":"property password_file"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#property-pid_file","text":"Returns path to the daemon PID file.","title":"property pid_file"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#method-down","text":"down() \u2192 None Stops the airflow daemon if necessary and tears down resources.","title":"method down"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#method-pre_run","text":"pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Args: pipeline : Pipeline that will be run. caller_filepath : Path to the file in which pipeline.run() was called. This contains the airflow DAG that is returned by the run() method. Raises: RuntimeError : If airflow is not running.","title":"method pre_run"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#method-run","text":"run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 DAG Prepares the pipeline so it can be run in Airflow. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Unused argument to conform with base class signature.","title":"method run"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#classmethod-set_airflow_home","text":"set_airflow_home(values: Dict[str, Any]) \u2192 Dict[str, Any] Sets airflow home according to orchestrator UUID.","title":"classmethod set_airflow_home"},{"location":"integrations.airflow.orchestrators.airflow_orchestrator/#method-up","text":"up() \u2192 None Ensures that Airflow is running. This file was automatically generated via lazydocs .","title":"method up"},{"location":"integrations.airflow.orchestrators/","text":"module integrations.airflow.orchestrators This file was automatically generated via lazydocs .","title":"Integrations.airflow.orchestrators"},{"location":"integrations.airflow.orchestrators/#module-integrationsairfloworchestrators","text":"This file was automatically generated via lazydocs .","title":"module integrations.airflow.orchestrators"},{"location":"integrations.constants/","text":"module integrations.constants Global Variables AIRFLOW DASH EVIDENTLY FACETS GCP GRAPHVIZ KUBEFLOW MLFLOW PLOTLY PYTORCH PYTORCH_L SKLEARN TENSORFLOW This file was automatically generated via lazydocs .","title":"Integrations.constants"},{"location":"integrations.constants/#module-integrationsconstants","text":"","title":"module integrations.constants"},{"location":"integrations.constants/#global-variables","text":"AIRFLOW DASH EVIDENTLY FACETS GCP GRAPHVIZ KUBEFLOW MLFLOW PLOTLY PYTORCH PYTORCH_L SKLEARN TENSORFLOW This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"integrations.dash/","text":"module integrations.dash Global Variables DASH class DashIntegration Definition of Dash integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.dash"},{"location":"integrations.dash/#module-integrationsdash","text":"","title":"module integrations.dash"},{"location":"integrations.dash/#global-variables","text":"DASH","title":"Global Variables"},{"location":"integrations.dash/#class-dashintegration","text":"Definition of Dash integration for ZenML. This file was automatically generated via lazydocs .","title":"class DashIntegration"},{"location":"integrations.dash.visualizers/","text":"module integrations.dash.visualizers This file was automatically generated via lazydocs .","title":"Integrations.dash.visualizers"},{"location":"integrations.dash.visualizers/#module-integrationsdashvisualizers","text":"This file was automatically generated via lazydocs .","title":"module integrations.dash.visualizers"},{"location":"integrations.dash.visualizers.pipeline_run_lineage_visualizer/","text":"module integrations.dash.visualizers.pipeline_run_lineage_visualizer Global Variables OVERALL_STYLE COLOR_RED COLOR_BLUE COLOR_YELLOW COLOR_GREEN STYLESHEET class PipelineRunLineageVisualizer Implementation of a lineage diagram via the dash and dash-cyctoscape library. method visualize visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 Dash Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. This file was automatically generated via lazydocs .","title":"Integrations.dash.visualizers.pipeline run lineage visualizer"},{"location":"integrations.dash.visualizers.pipeline_run_lineage_visualizer/#module-integrationsdashvisualizerspipeline_run_lineage_visualizer","text":"","title":"module integrations.dash.visualizers.pipeline_run_lineage_visualizer"},{"location":"integrations.dash.visualizers.pipeline_run_lineage_visualizer/#global-variables","text":"OVERALL_STYLE COLOR_RED COLOR_BLUE COLOR_YELLOW COLOR_GREEN STYLESHEET","title":"Global Variables"},{"location":"integrations.dash.visualizers.pipeline_run_lineage_visualizer/#class-pipelinerunlineagevisualizer","text":"Implementation of a lineage diagram via the dash and dash-cyctoscape library.","title":"class PipelineRunLineageVisualizer"},{"location":"integrations.dash.visualizers.pipeline_run_lineage_visualizer/#method-visualize","text":"visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 Dash Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"integrations.evidently/","text":"module integrations.evidently The Evidently integration provides a way to monitor your models in production. It includes a way to detect data drift and different kinds of model performance issues. The results of Evidently calculations can either be exported as an interactive dashboard (visualized as an html file or in your Jupyter notebook), or as a JSON file. Global Variables EVIDENTLY class EvidentlyIntegration Definition of Evidently integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.evidently"},{"location":"integrations.evidently/#module-integrationsevidently","text":"The Evidently integration provides a way to monitor your models in production. It includes a way to detect data drift and different kinds of model performance issues. The results of Evidently calculations can either be exported as an interactive dashboard (visualized as an html file or in your Jupyter notebook), or as a JSON file.","title":"module integrations.evidently"},{"location":"integrations.evidently/#global-variables","text":"EVIDENTLY","title":"Global Variables"},{"location":"integrations.evidently/#class-evidentlyintegration","text":"Definition of Evidently integration for ZenML. This file was automatically generated via lazydocs .","title":"class EvidentlyIntegration"},{"location":"integrations.evidently.steps.evidently_profile/","text":"module integrations.evidently.steps.evidently_profile Global Variables profile_mapper dashboard_mapper class EvidentlyProfileConfig Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" method get_profile_sections_and_tabs get_profile_sections_and_tabs() \u2192 Tuple[List[ProfileSection], List[Tab]] class EvidentlyProfileStep Simple step implementation which implements Evidently's functionality for creating a profile. property component Returns a TFX component. method entrypoint entrypoint( reference_dataset: DataArtifact, comparison_dataset: DataArtifact, config: EvidentlyProfileConfig, context: StepContext ) \u2192 <Output object at 0x7fe62ea81490> Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset : a Pandas dataframe comparison_dataset : a Pandas dataframe of new data you wish to compare against the reference data config : the configuration for the step context : the context of the step Returns: profile : dictionary report extracted from an Evidently Profile generated for the data drift dashboard : HTML report extracted from an Evidently Dashboard generated for the data drift This file was automatically generated via lazydocs .","title":"Integrations.evidently.steps.evidently profile"},{"location":"integrations.evidently.steps.evidently_profile/#module-integrationsevidentlystepsevidently_profile","text":"","title":"module integrations.evidently.steps.evidently_profile"},{"location":"integrations.evidently.steps.evidently_profile/#global-variables","text":"profile_mapper dashboard_mapper","title":"Global Variables"},{"location":"integrations.evidently.steps.evidently_profile/#class-evidentlyprofileconfig","text":"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\"","title":"class EvidentlyProfileConfig"},{"location":"integrations.evidently.steps.evidently_profile/#method-get_profile_sections_and_tabs","text":"get_profile_sections_and_tabs() \u2192 Tuple[List[ProfileSection], List[Tab]]","title":"method get_profile_sections_and_tabs"},{"location":"integrations.evidently.steps.evidently_profile/#class-evidentlyprofilestep","text":"Simple step implementation which implements Evidently's functionality for creating a profile.","title":"class EvidentlyProfileStep"},{"location":"integrations.evidently.steps.evidently_profile/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"integrations.evidently.steps.evidently_profile/#method-entrypoint","text":"entrypoint( reference_dataset: DataArtifact, comparison_dataset: DataArtifact, config: EvidentlyProfileConfig, context: StepContext ) \u2192 <Output object at 0x7fe62ea81490> Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset : a Pandas dataframe comparison_dataset : a Pandas dataframe of new data you wish to compare against the reference data config : the configuration for the step context : the context of the step Returns: profile : dictionary report extracted from an Evidently Profile generated for the data drift dashboard : HTML report extracted from an Evidently Dashboard generated for the data drift This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"integrations.evidently.steps/","text":"module integrations.evidently.steps This file was automatically generated via lazydocs .","title":"Integrations.evidently.steps"},{"location":"integrations.evidently.steps/#module-integrationsevidentlysteps","text":"This file was automatically generated via lazydocs .","title":"module integrations.evidently.steps"},{"location":"integrations.evidently.visualizers.evidently_visualizer/","text":"module integrations.evidently.visualizers.evidently_visualizer class EvidentlyVisualizer The implementation of an Evidently Visualizer. method generate_facet generate_facet(html_: str) \u2192 None Generate a Facet Overview Args: h : HTML represented as a string. method visualize visualize(object: StepView, *args: Any, **kwargs: Any) \u2192 None Method to visualize components Args: object : StepView fetched from run.get_step(). This file was automatically generated via lazydocs .","title":"Integrations.evidently.visualizers.evidently visualizer"},{"location":"integrations.evidently.visualizers.evidently_visualizer/#module-integrationsevidentlyvisualizersevidently_visualizer","text":"","title":"module integrations.evidently.visualizers.evidently_visualizer"},{"location":"integrations.evidently.visualizers.evidently_visualizer/#class-evidentlyvisualizer","text":"The implementation of an Evidently Visualizer.","title":"class EvidentlyVisualizer"},{"location":"integrations.evidently.visualizers.evidently_visualizer/#method-generate_facet","text":"generate_facet(html_: str) \u2192 None Generate a Facet Overview Args: h : HTML represented as a string.","title":"method generate_facet"},{"location":"integrations.evidently.visualizers.evidently_visualizer/#method-visualize","text":"visualize(object: StepView, *args: Any, **kwargs: Any) \u2192 None Method to visualize components Args: object : StepView fetched from run.get_step(). This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"integrations.evidently.visualizers/","text":"module integrations.evidently.visualizers This file was automatically generated via lazydocs .","title":"Integrations.evidently.visualizers"},{"location":"integrations.evidently.visualizers/#module-integrationsevidentlyvisualizers","text":"This file was automatically generated via lazydocs .","title":"module integrations.evidently.visualizers"},{"location":"integrations.facets/","text":"module integrations.facets The Facets integration provides a simple way to visualize post-execution objects like PipelineView , PipelineRunView and StepView . These objects can be extended using the BaseVisualization class. This integration requires facets-overview be installed in your Python environment. Global Variables FACETS class FacetsIntegration Definition of Facet integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.facets"},{"location":"integrations.facets/#module-integrationsfacets","text":"The Facets integration provides a simple way to visualize post-execution objects like PipelineView , PipelineRunView and StepView . These objects can be extended using the BaseVisualization class. This integration requires facets-overview be installed in your Python environment.","title":"module integrations.facets"},{"location":"integrations.facets/#global-variables","text":"FACETS","title":"Global Variables"},{"location":"integrations.facets/#class-facetsintegration","text":"Definition of Facet integration for ZenML. This file was automatically generated via lazydocs .","title":"class FacetsIntegration"},{"location":"integrations.facets.visualizers.facet_statistics_visualizer/","text":"module integrations.facets.visualizers.facet_statistics_visualizer class FacetStatisticsVisualizer The base implementation of a ZenML Visualizer. method generate_facet generate_facet(html_: str, magic: bool = False) \u2192 None Generate a Facet Overview Args: h : HTML represented as a string. magic : Whether to magically materialize facet in a notebook. method generate_html generate_html(datasets: List[Dict[str, DataFrame]]) \u2192 str Generates html for facet. Args: datasets : List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded. method visualize visualize( object: StepView, magic: bool = False, *args: Any, **kwargs: Any ) \u2192 None Method to visualize components Args: object : StepView fetched from run.get_step(). magic : Whether to render in a Jupyter notebook or not. This file was automatically generated via lazydocs .","title":"Integrations.facets.visualizers.facet statistics visualizer"},{"location":"integrations.facets.visualizers.facet_statistics_visualizer/#module-integrationsfacetsvisualizersfacet_statistics_visualizer","text":"","title":"module integrations.facets.visualizers.facet_statistics_visualizer"},{"location":"integrations.facets.visualizers.facet_statistics_visualizer/#class-facetstatisticsvisualizer","text":"The base implementation of a ZenML Visualizer.","title":"class FacetStatisticsVisualizer"},{"location":"integrations.facets.visualizers.facet_statistics_visualizer/#method-generate_facet","text":"generate_facet(html_: str, magic: bool = False) \u2192 None Generate a Facet Overview Args: h : HTML represented as a string. magic : Whether to magically materialize facet in a notebook.","title":"method generate_facet"},{"location":"integrations.facets.visualizers.facet_statistics_visualizer/#method-generate_html","text":"generate_html(datasets: List[Dict[str, DataFrame]]) \u2192 str Generates html for facet. Args: datasets : List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded.","title":"method generate_html"},{"location":"integrations.facets.visualizers.facet_statistics_visualizer/#method-visualize","text":"visualize( object: StepView, magic: bool = False, *args: Any, **kwargs: Any ) \u2192 None Method to visualize components Args: object : StepView fetched from run.get_step(). magic : Whether to render in a Jupyter notebook or not. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"integrations.facets.visualizers/","text":"module integrations.facets.visualizers This file was automatically generated via lazydocs .","title":"Integrations.facets.visualizers"},{"location":"integrations.facets.visualizers/#module-integrationsfacetsvisualizers","text":"This file was automatically generated via lazydocs .","title":"module integrations.facets.visualizers"},{"location":"integrations.gcp.artifact_stores.gcp_artifact_store/","text":"module integrations.gcp.artifact_stores.gcp_artifact_store class GCPArtifactStore Artifact Store for Google Cloud Storage based artifacts. classmethod must_be_gcs_path must_be_gcs_path(v: str) \u2192 str Validates that the path is a valid gcs path. This file was automatically generated via lazydocs .","title":"Integrations.gcp.artifact stores.gcp artifact store"},{"location":"integrations.gcp.artifact_stores.gcp_artifact_store/#module-integrationsgcpartifact_storesgcp_artifact_store","text":"","title":"module integrations.gcp.artifact_stores.gcp_artifact_store"},{"location":"integrations.gcp.artifact_stores.gcp_artifact_store/#class-gcpartifactstore","text":"Artifact Store for Google Cloud Storage based artifacts.","title":"class GCPArtifactStore"},{"location":"integrations.gcp.artifact_stores.gcp_artifact_store/#classmethod-must_be_gcs_path","text":"must_be_gcs_path(v: str) \u2192 str Validates that the path is a valid gcs path. This file was automatically generated via lazydocs .","title":"classmethod must_be_gcs_path"},{"location":"integrations.gcp.artifact_stores/","text":"module integrations.gcp.artifact_stores This file was automatically generated via lazydocs .","title":"Integrations.gcp.artifact stores"},{"location":"integrations.gcp.artifact_stores/#module-integrationsgcpartifact_stores","text":"This file was automatically generated via lazydocs .","title":"module integrations.gcp.artifact_stores"},{"location":"integrations.gcp.io.gcs_plugin/","text":"module integrations.gcp.io.gcs_plugin Plugin which is created to add Google Cloud Store support to ZenML. It inherits from the base Filesystem created by TFX and overwrites the corresponding functions thanks to gcsfs. class ZenGCS Filesystem that delegates to Google Cloud Store using gcsfs. Note : To allow TFX to check for various error conditions, we need to raise their custom NotFoundError instead of the builtin python FileNotFoundError. method copy copy( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Copy a file. Args: src : The path to copy from. dst : The path to copy to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileNotFoundError : If the source file does not exist. FileExistsError : If a file already exists at the destination and overwrite is not set to True . method exists exists(path: Union[bytes, str]) \u2192 bool Check whether a path exists. method glob glob(pattern: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return all paths that match the given glob pattern. The glob pattern may include: - ' ' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - ' ' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/ */some_file) Args: pattern : The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern. method isdir isdir(path: Union[bytes, str]) \u2192 bool Check whether a path is a directory. method listdir listdir(path: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return a list of files in a directory. method makedirs makedirs(path: Union[bytes, str]) \u2192 None Create a directory at the given path. If needed also create missing parent directories. method mkdir mkdir(path: Union[bytes, str]) \u2192 None Create a directory at the given path. method open open(path: Union[bytes, str], mode: str = 'r') \u2192 Any Open a file at the given path. Args: path : Path of the file to open. mode : Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. method remove remove(path: Union[bytes, str]) \u2192 None Remove the file at the given path. method rename rename( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Rename source file to destination file. Args: src : The path of the file to rename. dst : The path to rename the source file to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileNotFoundError : If the source file does not exist. FileExistsError : If a file already exists at the destination and overwrite is not set to True . method rmtree rmtree(path: Union[bytes, str]) \u2192 None Remove the given directory. method stat stat(path: Union[bytes, str]) \u2192 Dict[str, Any] Return stat info for the given path. method walk walk( top: Union[bytes, str], topdown: bool = True, onerror: Optional[Callable[], NoneType] = None ) \u2192 Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] Return an iterator that walks the contents of the given directory. Args: top : Path of directory to walk. topdown : Unused argument to conform to interface. onerror : Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. This file was automatically generated via lazydocs .","title":"Integrations.gcp.io.gcs plugin"},{"location":"integrations.gcp.io.gcs_plugin/#module-integrationsgcpiogcs_plugin","text":"Plugin which is created to add Google Cloud Store support to ZenML. It inherits from the base Filesystem created by TFX and overwrites the corresponding functions thanks to gcsfs.","title":"module integrations.gcp.io.gcs_plugin"},{"location":"integrations.gcp.io.gcs_plugin/#class-zengcs","text":"Filesystem that delegates to Google Cloud Store using gcsfs. Note : To allow TFX to check for various error conditions, we need to raise their custom NotFoundError instead of the builtin python FileNotFoundError.","title":"class ZenGCS"},{"location":"integrations.gcp.io.gcs_plugin/#method-copy","text":"copy( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Copy a file. Args: src : The path to copy from. dst : The path to copy to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileNotFoundError : If the source file does not exist. FileExistsError : If a file already exists at the destination and overwrite is not set to True .","title":"method copy"},{"location":"integrations.gcp.io.gcs_plugin/#method-exists","text":"exists(path: Union[bytes, str]) \u2192 bool Check whether a path exists.","title":"method exists"},{"location":"integrations.gcp.io.gcs_plugin/#method-glob","text":"glob(pattern: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return all paths that match the given glob pattern. The glob pattern may include: - ' ' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - ' ' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/ */some_file) Args: pattern : The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern.","title":"method glob"},{"location":"integrations.gcp.io.gcs_plugin/#method-isdir","text":"isdir(path: Union[bytes, str]) \u2192 bool Check whether a path is a directory.","title":"method isdir"},{"location":"integrations.gcp.io.gcs_plugin/#method-listdir","text":"listdir(path: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return a list of files in a directory.","title":"method listdir"},{"location":"integrations.gcp.io.gcs_plugin/#method-makedirs","text":"makedirs(path: Union[bytes, str]) \u2192 None Create a directory at the given path. If needed also create missing parent directories.","title":"method makedirs"},{"location":"integrations.gcp.io.gcs_plugin/#method-mkdir","text":"mkdir(path: Union[bytes, str]) \u2192 None Create a directory at the given path.","title":"method mkdir"},{"location":"integrations.gcp.io.gcs_plugin/#method-open","text":"open(path: Union[bytes, str], mode: str = 'r') \u2192 Any Open a file at the given path. Args: path : Path of the file to open. mode : Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported.","title":"method open"},{"location":"integrations.gcp.io.gcs_plugin/#method-remove","text":"remove(path: Union[bytes, str]) \u2192 None Remove the file at the given path.","title":"method remove"},{"location":"integrations.gcp.io.gcs_plugin/#method-rename","text":"rename( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Rename source file to destination file. Args: src : The path of the file to rename. dst : The path to rename the source file to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileNotFoundError : If the source file does not exist. FileExistsError : If a file already exists at the destination and overwrite is not set to True .","title":"method rename"},{"location":"integrations.gcp.io.gcs_plugin/#method-rmtree","text":"rmtree(path: Union[bytes, str]) \u2192 None Remove the given directory.","title":"method rmtree"},{"location":"integrations.gcp.io.gcs_plugin/#method-stat","text":"stat(path: Union[bytes, str]) \u2192 Dict[str, Any] Return stat info for the given path.","title":"method stat"},{"location":"integrations.gcp.io.gcs_plugin/#method-walk","text":"walk( top: Union[bytes, str], topdown: bool = True, onerror: Optional[Callable[], NoneType] = None ) \u2192 Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] Return an iterator that walks the contents of the given directory. Args: top : Path of directory to walk. topdown : Unused argument to conform to interface. onerror : Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. This file was automatically generated via lazydocs .","title":"method walk"},{"location":"integrations.gcp.io/","text":"module integrations.gcp.io This file was automatically generated via lazydocs .","title":"Integrations.gcp.io"},{"location":"integrations.gcp.io/#module-integrationsgcpio","text":"This file was automatically generated via lazydocs .","title":"module integrations.gcp.io"},{"location":"integrations.gcp/","text":"module integrations.gcp The GCP integration submodule provides a way to run ZenML pipelines in a cloud environment. Specifically, it allows the use of cloud artifact stores, metadata stores, and an io module to handle file operations on Google Cloud Storage (GCS). Global Variables GCP class GcpIntegration Definition of Google Cloud Platform integration for ZenML. classmethod activate activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"Integrations.gcp"},{"location":"integrations.gcp/#module-integrationsgcp","text":"The GCP integration submodule provides a way to run ZenML pipelines in a cloud environment. Specifically, it allows the use of cloud artifact stores, metadata stores, and an io module to handle file operations on Google Cloud Storage (GCS).","title":"module integrations.gcp"},{"location":"integrations.gcp/#global-variables","text":"GCP","title":"Global Variables"},{"location":"integrations.gcp/#class-gcpintegration","text":"Definition of Google Cloud Platform integration for ZenML.","title":"class GcpIntegration"},{"location":"integrations.gcp/#classmethod-activate","text":"activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"integrations.graphviz/","text":"module integrations.graphviz Global Variables GRAPHVIZ class GraphvizIntegration Definition of Graphviz integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.graphviz"},{"location":"integrations.graphviz/#module-integrationsgraphviz","text":"","title":"module integrations.graphviz"},{"location":"integrations.graphviz/#global-variables","text":"GRAPHVIZ","title":"Global Variables"},{"location":"integrations.graphviz/#class-graphvizintegration","text":"Definition of Graphviz integration for ZenML. This file was automatically generated via lazydocs .","title":"class GraphvizIntegration"},{"location":"integrations.graphviz.visualizers/","text":"module integrations.graphviz.visualizers This file was automatically generated via lazydocs .","title":"Integrations.graphviz.visualizers"},{"location":"integrations.graphviz.visualizers/#module-integrationsgraphvizvisualizers","text":"This file was automatically generated via lazydocs .","title":"module integrations.graphviz.visualizers"},{"location":"integrations.graphviz.visualizers.pipeline_run_dag_visualizer/","text":"module integrations.graphviz.visualizers.pipeline_run_dag_visualizer class PipelineRunDagVisualizer Visualize the lineage of runs in a pipeline. method visualize visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 Digraph Creates a pipeline lineage diagram using graphviz. This file was automatically generated via lazydocs .","title":"Integrations.graphviz.visualizers.pipeline run dag visualizer"},{"location":"integrations.graphviz.visualizers.pipeline_run_dag_visualizer/#module-integrationsgraphvizvisualizerspipeline_run_dag_visualizer","text":"","title":"module integrations.graphviz.visualizers.pipeline_run_dag_visualizer"},{"location":"integrations.graphviz.visualizers.pipeline_run_dag_visualizer/#class-pipelinerundagvisualizer","text":"Visualize the lineage of runs in a pipeline.","title":"class PipelineRunDagVisualizer"},{"location":"integrations.graphviz.visualizers.pipeline_run_dag_visualizer/#method-visualize","text":"visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 Digraph Creates a pipeline lineage diagram using graphviz. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"integrations.integration/","text":"module integrations.integration class IntegrationMeta Metaclass responsible for registering different Integration subclasses class Integration Base class for integration in ZenML method activate activate() \u2192 None Abstract method to activate the integration classmethod check_installation check_installation() \u2192 bool Method to check whether the required packages are installed This file was automatically generated via lazydocs .","title":"Integrations.integration"},{"location":"integrations.integration/#module-integrationsintegration","text":"","title":"module integrations.integration"},{"location":"integrations.integration/#class-integrationmeta","text":"Metaclass responsible for registering different Integration subclasses","title":"class IntegrationMeta"},{"location":"integrations.integration/#class-integration","text":"Base class for integration in ZenML","title":"class Integration"},{"location":"integrations.integration/#method-activate","text":"activate() \u2192 None Abstract method to activate the integration","title":"method activate"},{"location":"integrations.integration/#classmethod-check_installation","text":"check_installation() \u2192 bool Method to check whether the required packages are installed This file was automatically generated via lazydocs .","title":"classmethod check_installation"},{"location":"integrations.kubeflow.container_entrypoint/","text":"module integrations.kubeflow.container_entrypoint Main entrypoint for containers with Kubeflow TFX component executors. function main main() \u2192 None Runs a single step defined by the command line arguments. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.container entrypoint"},{"location":"integrations.kubeflow.container_entrypoint/#module-integrationskubeflowcontainer_entrypoint","text":"Main entrypoint for containers with Kubeflow TFX component executors.","title":"module integrations.kubeflow.container_entrypoint"},{"location":"integrations.kubeflow.container_entrypoint/#function-main","text":"main() \u2192 None Runs a single step defined by the command line arguments. This file was automatically generated via lazydocs .","title":"function main"},{"location":"integrations.kubeflow.docker_utils/","text":"module integrations.kubeflow.docker_utils Global Variables DEFAULT_BASE_IMAGE function generate_dockerfile_contents generate_dockerfile_contents( base_image: str, command: Optional[str] = None, requirements: Optional[List[str]] = None ) \u2192 str Generates a Dockerfile. Args: base_image : The image to use as base for the dockerfile. command : The default command that gets executed when running a container of an image created by this dockerfile. requirements : Optional list of pip requirements to install. Returns: Content of a dockerfile. function create_custom_build_context create_custom_build_context( build_context_path: str, dockerfile_contents: str, dockerignore_path: Optional[str] = None ) \u2192 Any Creates a docker build context. Args: build_context_path : Path to a directory that will be sent to the docker daemon as build context. dockerfile_contents : File contents of the Dockerfile to use for the build. dockerignore_path : Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. Returns: Docker build context that can be passed when building a docker image. function get_current_environment_requirements get_current_environment_requirements() \u2192 Dict[str, str] Returns a dict of package requirements for the environment that the current python process is running in. function build_docker_image build_docker_image( build_context_path: str, image_name: str, dockerfile_path: Optional[str] = None, dockerignore_path: Optional[str] = None, requirements: Optional[List[str]] = None, use_local_requirements: bool = False, base_image: Optional[str] = None ) \u2192 None Builds a docker image. Args: build_context_path : Path to a directory that will be sent to the docker daemon as build context. image_name : The name to use for the created docker image. dockerfile_path : Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. dockerignore_path : Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. requirements : Optional list of pip requirements to install. This will only be used if no value is given for dockerfile_path . use_local_requirements : If True and no values are given for dockerfile_path and requirements , then the packages installed in the environment of the current python processed will be installed in the docker image. base_image : The image to use as base for the docker image. function push_docker_image push_docker_image(image_name: str) \u2192 None Pushes a docker image to a container registry. Args: image_name : The full name (including a tag) of the image to push. function get_image_digest get_image_digest(image_name: str) \u2192 Union[str, NoneType] Gets the digest of a docker image. Args: image_name : Name of the image to get the digest for. Returns: Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns None . This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.docker utils"},{"location":"integrations.kubeflow.docker_utils/#module-integrationskubeflowdocker_utils","text":"","title":"module integrations.kubeflow.docker_utils"},{"location":"integrations.kubeflow.docker_utils/#global-variables","text":"DEFAULT_BASE_IMAGE","title":"Global Variables"},{"location":"integrations.kubeflow.docker_utils/#function-generate_dockerfile_contents","text":"generate_dockerfile_contents( base_image: str, command: Optional[str] = None, requirements: Optional[List[str]] = None ) \u2192 str Generates a Dockerfile. Args: base_image : The image to use as base for the dockerfile. command : The default command that gets executed when running a container of an image created by this dockerfile. requirements : Optional list of pip requirements to install. Returns: Content of a dockerfile.","title":"function generate_dockerfile_contents"},{"location":"integrations.kubeflow.docker_utils/#function-create_custom_build_context","text":"create_custom_build_context( build_context_path: str, dockerfile_contents: str, dockerignore_path: Optional[str] = None ) \u2192 Any Creates a docker build context. Args: build_context_path : Path to a directory that will be sent to the docker daemon as build context. dockerfile_contents : File contents of the Dockerfile to use for the build. dockerignore_path : Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. Returns: Docker build context that can be passed when building a docker image.","title":"function create_custom_build_context"},{"location":"integrations.kubeflow.docker_utils/#function-get_current_environment_requirements","text":"get_current_environment_requirements() \u2192 Dict[str, str] Returns a dict of package requirements for the environment that the current python process is running in.","title":"function get_current_environment_requirements"},{"location":"integrations.kubeflow.docker_utils/#function-build_docker_image","text":"build_docker_image( build_context_path: str, image_name: str, dockerfile_path: Optional[str] = None, dockerignore_path: Optional[str] = None, requirements: Optional[List[str]] = None, use_local_requirements: bool = False, base_image: Optional[str] = None ) \u2192 None Builds a docker image. Args: build_context_path : Path to a directory that will be sent to the docker daemon as build context. image_name : The name to use for the created docker image. dockerfile_path : Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. dockerignore_path : Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. requirements : Optional list of pip requirements to install. This will only be used if no value is given for dockerfile_path . use_local_requirements : If True and no values are given for dockerfile_path and requirements , then the packages installed in the environment of the current python processed will be installed in the docker image. base_image : The image to use as base for the docker image.","title":"function build_docker_image"},{"location":"integrations.kubeflow.docker_utils/#function-push_docker_image","text":"push_docker_image(image_name: str) \u2192 None Pushes a docker image to a container registry. Args: image_name : The full name (including a tag) of the image to push.","title":"function push_docker_image"},{"location":"integrations.kubeflow.docker_utils/#function-get_image_digest","text":"get_image_digest(image_name: str) \u2192 Union[str, NoneType] Gets the digest of a docker image. Args: image_name : Name of the image to get the digest for. Returns: Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns None . This file was automatically generated via lazydocs .","title":"function get_image_digest"},{"location":"integrations.kubeflow/","text":"module integrations.kubeflow The Kubeflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Kubeflow orchestrator with the CLI tool. Global Variables KUBEFLOW class KubeflowIntegration Definition of Kubeflow Integration for ZenML. classmethod activate activate() \u2192 None Activates all classes required for the airflow integration. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow"},{"location":"integrations.kubeflow/#module-integrationskubeflow","text":"The Kubeflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Kubeflow orchestrator with the CLI tool.","title":"module integrations.kubeflow"},{"location":"integrations.kubeflow/#global-variables","text":"KUBEFLOW","title":"Global Variables"},{"location":"integrations.kubeflow/#class-kubeflowintegration","text":"Definition of Kubeflow Integration for ZenML.","title":"class KubeflowIntegration"},{"location":"integrations.kubeflow/#classmethod-activate","text":"activate() \u2192 None Activates all classes required for the airflow integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"integrations.kubeflow.metadata.kubeflow_metadata_store/","text":"module integrations.kubeflow.metadata.kubeflow_metadata_store class KubeflowMetadataStore Kubeflow MySQL backend for ZenML metadata store. property step_type_mapping Maps type_id's to step names. property store General property that hooks into TFX metadata store. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.metadata.kubeflow metadata store"},{"location":"integrations.kubeflow.metadata.kubeflow_metadata_store/#module-integrationskubeflowmetadatakubeflow_metadata_store","text":"","title":"module integrations.kubeflow.metadata.kubeflow_metadata_store"},{"location":"integrations.kubeflow.metadata.kubeflow_metadata_store/#class-kubeflowmetadatastore","text":"Kubeflow MySQL backend for ZenML metadata store.","title":"class KubeflowMetadataStore"},{"location":"integrations.kubeflow.metadata.kubeflow_metadata_store/#property-step_type_mapping","text":"Maps type_id's to step names.","title":"property step_type_mapping"},{"location":"integrations.kubeflow.metadata.kubeflow_metadata_store/#property-store","text":"General property that hooks into TFX metadata store. This file was automatically generated via lazydocs .","title":"property store"},{"location":"integrations.kubeflow.metadata/","text":"module integrations.kubeflow.metadata This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.metadata"},{"location":"integrations.kubeflow.metadata/#module-integrationskubeflowmetadata","text":"This file was automatically generated via lazydocs .","title":"module integrations.kubeflow.metadata"},{"location":"integrations.kubeflow.orchestrators.kubeflow_component/","text":"module integrations.kubeflow.orchestrators.kubeflow_component Kubeflow Pipelines based implementation of TFX components. These components are lightweight wrappers around the KFP DSL's ContainerOp, and ensure that the container gets called with the right set of input arguments. It also ensures that each component exports named output attributes that are consistent with those provided by the native TFX components, thus ensuring that both types of pipeline definitions are compatible. Note: This requires Kubeflow Pipelines SDK to be installed. Global Variables ENV_ZENML_PREVENT_PIPELINE_EXECUTION CONTAINER_ENTRYPOINT_COMMAND class KubeflowComponent Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components. method __init__ __init__( component: BaseComponent, depends_on: Set[ContainerOp], image: str, tfx_ir: Pipeline, pod_labels_to_attach: Dict[str, str], main_module: str, step_module: str, step_function_name: str, runtime_parameters: List[RuntimeParameter], metadata_ui_path: str = '/tmp/mlpipeline-ui-metadata.json' ) Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component : The logical TFX component to wrap. depends_on : The set of upstream KFP ContainerOp components that this component will depend on. image : The container image to use for this component. tfx_ir : The TFX intermedia representation of the pipeline. pod_labels_to_attach : Dict of pod labels to attach to the GKE pod. runtime_parameters : Runtime parameters of the pipeline. metadata_ui_path : File location for metadata-ui-metadata.json file. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators.kubeflow component"},{"location":"integrations.kubeflow.orchestrators.kubeflow_component/#module-integrationskubefloworchestratorskubeflow_component","text":"Kubeflow Pipelines based implementation of TFX components. These components are lightweight wrappers around the KFP DSL's ContainerOp, and ensure that the container gets called with the right set of input arguments. It also ensures that each component exports named output attributes that are consistent with those provided by the native TFX components, thus ensuring that both types of pipeline definitions are compatible. Note: This requires Kubeflow Pipelines SDK to be installed.","title":"module integrations.kubeflow.orchestrators.kubeflow_component"},{"location":"integrations.kubeflow.orchestrators.kubeflow_component/#global-variables","text":"ENV_ZENML_PREVENT_PIPELINE_EXECUTION CONTAINER_ENTRYPOINT_COMMAND","title":"Global Variables"},{"location":"integrations.kubeflow.orchestrators.kubeflow_component/#class-kubeflowcomponent","text":"Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components.","title":"class KubeflowComponent"},{"location":"integrations.kubeflow.orchestrators.kubeflow_component/#method-__init__","text":"__init__( component: BaseComponent, depends_on: Set[ContainerOp], image: str, tfx_ir: Pipeline, pod_labels_to_attach: Dict[str, str], main_module: str, step_module: str, step_function_name: str, runtime_parameters: List[RuntimeParameter], metadata_ui_path: str = '/tmp/mlpipeline-ui-metadata.json' ) Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component : The logical TFX component to wrap. depends_on : The set of upstream KFP ContainerOp components that this component will depend on. image : The container image to use for this component. tfx_ir : The TFX intermedia representation of the pipeline. pod_labels_to_attach : Dict of pod labels to attach to the GKE pod. runtime_parameters : Runtime parameters of the pipeline. metadata_ui_path : File location for metadata-ui-metadata.json file. This file was automatically generated via lazydocs .","title":"method __init__"},{"location":"integrations.kubeflow.orchestrators.kubeflow_dag_runner/","text":"module integrations.kubeflow.orchestrators.kubeflow_dag_runner The below code is copied from the TFX source repo with minor changes. All credits goes to the TFX team for the core implementation function get_default_pipeline_operator_funcs get_default_pipeline_operator_funcs( use_gcp_sa: bool = False ) \u2192 List[Callable[[ContainerOp], Union[ContainerOp, NoneType]]] Returns a default list of pipeline operator functions. Args: use_gcp_sa : If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. Returns: A list of functions with type OpFunc. function get_default_pod_labels get_default_pod_labels() \u2192 Dict[str, str] Returns the default pod label dict for Kubeflow. class KubeflowDagRunnerConfig Runtime configuration parameters specific to execution on Kubeflow. method __init__ __init__( image: str, pipeline_operator_funcs: Optional[List[Callable[[ContainerOp], Union[ContainerOp]]], NoneType] = None, supported_launcher_classes: Optional[List[Type[BaseComponentLauncher]]] = None, metadata_ui_path: str = '/tmp/mlpipeline-ui-metadata.json', **kwargs: Any ) Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image : The docker image to use in the pipeline. pipeline_operator_funcs : A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes : A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path : File location for metadata-ui-metadata.json file. **kwargs : keyword args for PipelineConfig. class KubeflowDagRunner Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline. method __init__ __init__( config: KubeflowDagRunnerConfig, output_path: str, pod_labels_to_attach: Optional[Dict[str, str]] = None ) Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config : A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path : Path where the pipeline definition file will be stored. pod_labels_to_attach : Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env : true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. property config method run run(pipeline: Pipeline) \u2192 None Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline : The logical TFX pipeline to use when building the Kubeflow pipeline. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators.kubeflow dag runner"},{"location":"integrations.kubeflow.orchestrators.kubeflow_dag_runner/#module-integrationskubefloworchestratorskubeflow_dag_runner","text":"The below code is copied from the TFX source repo with minor changes. All credits goes to the TFX team for the core implementation","title":"module integrations.kubeflow.orchestrators.kubeflow_dag_runner"},{"location":"integrations.kubeflow.orchestrators.kubeflow_dag_runner/#function-get_default_pipeline_operator_funcs","text":"get_default_pipeline_operator_funcs( use_gcp_sa: bool = False ) \u2192 List[Callable[[ContainerOp], Union[ContainerOp, NoneType]]] Returns a default list of pipeline operator functions. Args: use_gcp_sa : If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. Returns: A list of functions with type OpFunc.","title":"function get_default_pipeline_operator_funcs"},{"location":"integrations.kubeflow.orchestrators.kubeflow_dag_runner/#function-get_default_pod_labels","text":"get_default_pod_labels() \u2192 Dict[str, str] Returns the default pod label dict for Kubeflow.","title":"function get_default_pod_labels"},{"location":"integrations.kubeflow.orchestrators.kubeflow_dag_runner/#class-kubeflowdagrunnerconfig","text":"Runtime configuration parameters specific to execution on Kubeflow.","title":"class KubeflowDagRunnerConfig"},{"location":"integrations.kubeflow.orchestrators.kubeflow_dag_runner/#method-__init__","text":"__init__( image: str, pipeline_operator_funcs: Optional[List[Callable[[ContainerOp], Union[ContainerOp]]], NoneType] = None, supported_launcher_classes: Optional[List[Type[BaseComponentLauncher]]] = None, metadata_ui_path: str = '/tmp/mlpipeline-ui-metadata.json', **kwargs: Any ) Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image : The docker image to use in the pipeline. pipeline_operator_funcs : A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes : A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path : File location for metadata-ui-metadata.json file. **kwargs : keyword args for PipelineConfig.","title":"method __init__"},{"location":"integrations.kubeflow.orchestrators.kubeflow_dag_runner/#class-kubeflowdagrunner","text":"Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline.","title":"class KubeflowDagRunner"},{"location":"integrations.kubeflow.orchestrators.kubeflow_dag_runner/#method-__init___1","text":"__init__( config: KubeflowDagRunnerConfig, output_path: str, pod_labels_to_attach: Optional[Dict[str, str]] = None ) Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config : A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path : Path where the pipeline definition file will be stored. pod_labels_to_attach : Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env : true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking.","title":"method __init__"},{"location":"integrations.kubeflow.orchestrators.kubeflow_dag_runner/#property-config","text":"","title":"property config"},{"location":"integrations.kubeflow.orchestrators.kubeflow_dag_runner/#method-run","text":"run(pipeline: Pipeline) \u2192 None Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline : The logical TFX pipeline to use when building the Kubeflow pipeline. This file was automatically generated via lazydocs .","title":"method run"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/","text":"module integrations.kubeflow.orchestrators.kubeflow_orchestrator Global Variables TYPE_CHECKING KFP_VERSION DEFAULT_KFP_UI_PORT class KubeflowOrchestrator Orchestrator responsible for running pipelines using Kubeflow. property is_running Returns whether the orchestrator is running. property log_file Path of the daemon log file. property pipeline_directory Returns path to a directory in which the kubeflow pipeline files are stored. property root_directory Returns path to the root directory for all files concerning this orchestrator. method down down() \u2192 None Tears down a local Kubeflow Pipelines deployment. method get_docker_image_name get_docker_image_name(pipeline_name: str) \u2192 str Returns the full docker image name including registry and tag. method list_manual_setup_steps list_manual_setup_steps( container_registry_name: str, container_registry_path: str ) \u2192 None Logs manual steps needed to setup the Kubeflow local orchestrator. method pre_run pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Builds a docker image for the current environment and uploads it to a container registry if configured. method run run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 None Runs the pipeline on Kubeflow. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Unused kwargs to conform with base signature method up up() \u2192 None Spins up a local Kubeflow Pipelines deployment. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators.kubeflow orchestrator"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#module-integrationskubefloworchestratorskubeflow_orchestrator","text":"","title":"module integrations.kubeflow.orchestrators.kubeflow_orchestrator"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#global-variables","text":"TYPE_CHECKING KFP_VERSION DEFAULT_KFP_UI_PORT","title":"Global Variables"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#class-kubefloworchestrator","text":"Orchestrator responsible for running pipelines using Kubeflow.","title":"class KubeflowOrchestrator"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#property-is_running","text":"Returns whether the orchestrator is running.","title":"property is_running"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#property-log_file","text":"Path of the daemon log file.","title":"property log_file"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#property-pipeline_directory","text":"Returns path to a directory in which the kubeflow pipeline files are stored.","title":"property pipeline_directory"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#property-root_directory","text":"Returns path to the root directory for all files concerning this orchestrator.","title":"property root_directory"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-down","text":"down() \u2192 None Tears down a local Kubeflow Pipelines deployment.","title":"method down"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-get_docker_image_name","text":"get_docker_image_name(pipeline_name: str) \u2192 str Returns the full docker image name including registry and tag.","title":"method get_docker_image_name"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-list_manual_setup_steps","text":"list_manual_setup_steps( container_registry_name: str, container_registry_path: str ) \u2192 None Logs manual steps needed to setup the Kubeflow local orchestrator.","title":"method list_manual_setup_steps"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-pre_run","text":"pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Builds a docker image for the current environment and uploads it to a container registry if configured.","title":"method pre_run"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-run","text":"run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 None Runs the pipeline on Kubeflow. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Unused kwargs to conform with base signature","title":"method run"},{"location":"integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-up","text":"up() \u2192 None Spins up a local Kubeflow Pipelines deployment. This file was automatically generated via lazydocs .","title":"method up"},{"location":"integrations.kubeflow.orchestrators.kubeflow_utils/","text":"module integrations.kubeflow.orchestrators.kubeflow_utils Common utility for Kubeflow-based orchestrator. function replace_placeholder replace_placeholder(component: BaseNode) \u2192 None Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators.kubeflow utils"},{"location":"integrations.kubeflow.orchestrators.kubeflow_utils/#module-integrationskubefloworchestratorskubeflow_utils","text":"Common utility for Kubeflow-based orchestrator.","title":"module integrations.kubeflow.orchestrators.kubeflow_utils"},{"location":"integrations.kubeflow.orchestrators.kubeflow_utils/#function-replace_placeholder","text":"replace_placeholder(component: BaseNode) \u2192 None Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. This file was automatically generated via lazydocs .","title":"function replace_placeholder"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/","text":"module integrations.kubeflow.orchestrators.local_deployment_utils Global Variables KFP_VERSION function check_prerequisites check_prerequisites() \u2192 bool Checks whether all prerequisites for a local kubeflow pipelines deployment are installed. function write_local_registry_yaml write_local_registry_yaml( yaml_path: str, registry_name: str, registry_uri: str ) \u2192 None Writes a K3D registry config file. Args: yaml_path : Path where the config file should be written to. registry_name : Name of the registry. registry_uri : URI of the registry. function k3d_cluster_exists k3d_cluster_exists(cluster_name: str) \u2192 bool Checks whether there exists a K3D cluster with the given name. function create_k3d_cluster create_k3d_cluster( cluster_name: str, registry_name: str, registry_config_path: str ) \u2192 None Creates a K3D cluster. Args: cluster_name : Name of the cluster to create. registry_name : Name of the registry to create for this cluster. registry_config_path : Path to the registry config file. function delete_k3d_cluster delete_k3d_cluster(cluster_name: str) \u2192 None Deletes a K3D cluster with the given name. function kubeflow_pipelines_ready kubeflow_pipelines_ready(kubernetes_context: str) \u2192 bool Returns whether all Kubeflow Pipelines pods are ready. Args: kubernetes_context : The kubernetes context in which the pods should be checked. function deploy_kubeflow_pipelines deploy_kubeflow_pipelines(kubernetes_context: str) \u2192 None Deploys Kubeflow Pipelines. Args: kubernetes_context : The kubernetes context on which Kubeflow Pipelines should be deployed. function start_kfp_ui_daemon start_kfp_ui_daemon(pid_file_path: str, log_file_path: str, port: int) \u2192 None Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Args: pid_file_path : Path where the file with the daemons process ID should be written. log_file_path : Path to a file where the daemon logs should be written. port : Port on which the UI should be accessible. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators.local deployment utils"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/#module-integrationskubefloworchestratorslocal_deployment_utils","text":"","title":"module integrations.kubeflow.orchestrators.local_deployment_utils"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/#global-variables","text":"KFP_VERSION","title":"Global Variables"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/#function-check_prerequisites","text":"check_prerequisites() \u2192 bool Checks whether all prerequisites for a local kubeflow pipelines deployment are installed.","title":"function check_prerequisites"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/#function-write_local_registry_yaml","text":"write_local_registry_yaml( yaml_path: str, registry_name: str, registry_uri: str ) \u2192 None Writes a K3D registry config file. Args: yaml_path : Path where the config file should be written to. registry_name : Name of the registry. registry_uri : URI of the registry.","title":"function write_local_registry_yaml"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/#function-k3d_cluster_exists","text":"k3d_cluster_exists(cluster_name: str) \u2192 bool Checks whether there exists a K3D cluster with the given name.","title":"function k3d_cluster_exists"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/#function-create_k3d_cluster","text":"create_k3d_cluster( cluster_name: str, registry_name: str, registry_config_path: str ) \u2192 None Creates a K3D cluster. Args: cluster_name : Name of the cluster to create. registry_name : Name of the registry to create for this cluster. registry_config_path : Path to the registry config file.","title":"function create_k3d_cluster"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/#function-delete_k3d_cluster","text":"delete_k3d_cluster(cluster_name: str) \u2192 None Deletes a K3D cluster with the given name.","title":"function delete_k3d_cluster"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/#function-kubeflow_pipelines_ready","text":"kubeflow_pipelines_ready(kubernetes_context: str) \u2192 bool Returns whether all Kubeflow Pipelines pods are ready. Args: kubernetes_context : The kubernetes context in which the pods should be checked.","title":"function kubeflow_pipelines_ready"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/#function-deploy_kubeflow_pipelines","text":"deploy_kubeflow_pipelines(kubernetes_context: str) \u2192 None Deploys Kubeflow Pipelines. Args: kubernetes_context : The kubernetes context on which Kubeflow Pipelines should be deployed.","title":"function deploy_kubeflow_pipelines"},{"location":"integrations.kubeflow.orchestrators.local_deployment_utils/#function-start_kfp_ui_daemon","text":"start_kfp_ui_daemon(pid_file_path: str, log_file_path: str, port: int) \u2192 None Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Args: pid_file_path : Path where the file with the daemons process ID should be written. log_file_path : Path to a file where the daemon logs should be written. port : Port on which the UI should be accessible. This file was automatically generated via lazydocs .","title":"function start_kfp_ui_daemon"},{"location":"integrations.kubeflow.orchestrators/","text":"module integrations.kubeflow.orchestrators This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators"},{"location":"integrations.kubeflow.orchestrators/#module-integrationskubefloworchestrators","text":"This file was automatically generated via lazydocs .","title":"module integrations.kubeflow.orchestrators"},{"location":"integrations/","text":"module integrations The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch. This file was automatically generated via lazydocs .","title":"Integrations"},{"location":"integrations/#module-integrations","text":"The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch. This file was automatically generated via lazydocs .","title":"module integrations"},{"location":"integrations.mlflow/","text":"module integrations.mlflow The mlflow integrations currently enables you to use mlflow tracking as a convenient way to visualize your experiment runs within the mlflow ui Global Variables MLFLOW class MlflowIntegration Definition of Plotly integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.mlflow"},{"location":"integrations.mlflow/#module-integrationsmlflow","text":"The mlflow integrations currently enables you to use mlflow tracking as a convenient way to visualize your experiment runs within the mlflow ui","title":"module integrations.mlflow"},{"location":"integrations.mlflow/#global-variables","text":"MLFLOW","title":"Global Variables"},{"location":"integrations.mlflow/#class-mlflowintegration","text":"Definition of Plotly integration for ZenML. This file was automatically generated via lazydocs .","title":"class MlflowIntegration"},{"location":"integrations.mlflow.mlflow_utils/","text":"module integrations.mlflow.mlflow_utils function local_mlflow_backend local_mlflow_backend() \u2192 str Returns the local mlflow backend inside the global zenml directory function setup_mlflow setup_mlflow( backend_store_uri: Optional[str] = None, experiment_name: str = 'default' ) \u2192 None Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Args: backend_store_uri : The mlflow backend to log to experiment_name : The experiment name under which all runs will be tracked function enable_mlflow_init enable_mlflow_init( original_init: Callable[[BasePipeline, BaseStep, Any], NoneType], experiment: Optional[str] = None ) \u2192 Callable[, NoneType] Outer decorator function for extending the init method for pipelines that should be run using mlflow Args: original_init : The init method that should be extended experiment : The users chosen experiment name to use for mlflow Returns: the inner decorator which extends the init method function enable_mlflow_run enable_mlflow_run(run: Callable[, Any]) \u2192 Callable[, Any] Outer decorator function for extending the run method for pipelines that should be run using mlflow Args: run : The run method that should be extended Returns: the inner decorator which extends the run method function enable_mlflow enable_mlflow( _pipeline: Type[BasePipeline], experiment_name: Optional[str] = None ) \u2192 Type[BasePipeline] Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a pipeline run. For this, the init and run method need to be extended accordingly. Args: _pipeline : The decorated pipeline experiment_name : Experiment name to use for mlflow Returns: the inner decorator which has a pipeline with the two methods extended This file was automatically generated via lazydocs .","title":"Integrations.mlflow.mlflow utils"},{"location":"integrations.mlflow.mlflow_utils/#module-integrationsmlflowmlflow_utils","text":"","title":"module integrations.mlflow.mlflow_utils"},{"location":"integrations.mlflow.mlflow_utils/#function-local_mlflow_backend","text":"local_mlflow_backend() \u2192 str Returns the local mlflow backend inside the global zenml directory","title":"function local_mlflow_backend"},{"location":"integrations.mlflow.mlflow_utils/#function-setup_mlflow","text":"setup_mlflow( backend_store_uri: Optional[str] = None, experiment_name: str = 'default' ) \u2192 None Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Args: backend_store_uri : The mlflow backend to log to experiment_name : The experiment name under which all runs will be tracked","title":"function setup_mlflow"},{"location":"integrations.mlflow.mlflow_utils/#function-enable_mlflow_init","text":"enable_mlflow_init( original_init: Callable[[BasePipeline, BaseStep, Any], NoneType], experiment: Optional[str] = None ) \u2192 Callable[, NoneType] Outer decorator function for extending the init method for pipelines that should be run using mlflow Args: original_init : The init method that should be extended experiment : The users chosen experiment name to use for mlflow Returns: the inner decorator which extends the init method","title":"function enable_mlflow_init"},{"location":"integrations.mlflow.mlflow_utils/#function-enable_mlflow_run","text":"enable_mlflow_run(run: Callable[, Any]) \u2192 Callable[, Any] Outer decorator function for extending the run method for pipelines that should be run using mlflow Args: run : The run method that should be extended Returns: the inner decorator which extends the run method","title":"function enable_mlflow_run"},{"location":"integrations.mlflow.mlflow_utils/#function-enable_mlflow","text":"enable_mlflow( _pipeline: Type[BasePipeline], experiment_name: Optional[str] = None ) \u2192 Type[BasePipeline] Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a pipeline run. For this, the init and run method need to be extended accordingly. Args: _pipeline : The decorated pipeline experiment_name : Experiment name to use for mlflow Returns: the inner decorator which has a pipeline with the two methods extended This file was automatically generated via lazydocs .","title":"function enable_mlflow"},{"location":"integrations.plotly/","text":"module integrations.plotly Global Variables PLOTLY class PlotlyIntegration Definition of Plotly integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.plotly"},{"location":"integrations.plotly/#module-integrationsplotly","text":"","title":"module integrations.plotly"},{"location":"integrations.plotly/#global-variables","text":"PLOTLY","title":"Global Variables"},{"location":"integrations.plotly/#class-plotlyintegration","text":"Definition of Plotly integration for ZenML. This file was automatically generated via lazydocs .","title":"class PlotlyIntegration"},{"location":"integrations.plotly.visualizers/","text":"module integrations.plotly.visualizers This file was automatically generated via lazydocs .","title":"Integrations.plotly.visualizers"},{"location":"integrations.plotly.visualizers/#module-integrationsplotlyvisualizers","text":"This file was automatically generated via lazydocs .","title":"module integrations.plotly.visualizers"},{"location":"integrations.plotly.visualizers.pipeline_lineage_visualizer/","text":"module integrations.plotly.visualizers.pipeline_lineage_visualizer class PipelineLineageVisualizer Visualize the lineage of runs in a pipeline using plotly. method visualize visualize(object: PipelineView, *args: Any, **kwargs: Any) \u2192 Figure Creates a pipeline lineage diagram using plotly. This file was automatically generated via lazydocs .","title":"Integrations.plotly.visualizers.pipeline lineage visualizer"},{"location":"integrations.plotly.visualizers.pipeline_lineage_visualizer/#module-integrationsplotlyvisualizerspipeline_lineage_visualizer","text":"","title":"module integrations.plotly.visualizers.pipeline_lineage_visualizer"},{"location":"integrations.plotly.visualizers.pipeline_lineage_visualizer/#class-pipelinelineagevisualizer","text":"Visualize the lineage of runs in a pipeline using plotly.","title":"class PipelineLineageVisualizer"},{"location":"integrations.plotly.visualizers.pipeline_lineage_visualizer/#method-visualize","text":"visualize(object: PipelineView, *args: Any, **kwargs: Any) \u2192 Figure Creates a pipeline lineage diagram using plotly. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"integrations.pytorch.materializers/","text":"module integrations.pytorch.materializers This file was automatically generated via lazydocs .","title":"Integrations.pytorch.materializers"},{"location":"integrations.pytorch.materializers/#module-integrationspytorchmaterializers","text":"This file was automatically generated via lazydocs .","title":"module integrations.pytorch.materializers"},{"location":"integrations.pytorch.materializers.pytorch_materializer/","text":"module integrations.pytorch.materializers.pytorch_materializer Global Variables DEFAULT_FILENAME class PyTorchMaterializer Materializer to read/write Pytorch models. method handle_input handle_input(data_type: Type[Any]) \u2192 Union[Module, TorchDict] Reads and returns a PyTorch model. Returns: A loaded pytorch model. method handle_return handle_return(model: Union[Module, TorchDict]) \u2192 None Writes a PyTorch model. Args: model : A torch.nn.Module or a dict to pass into model.save This file was automatically generated via lazydocs .","title":"Integrations.pytorch.materializers.pytorch materializer"},{"location":"integrations.pytorch.materializers.pytorch_materializer/#module-integrationspytorchmaterializerspytorch_materializer","text":"","title":"module integrations.pytorch.materializers.pytorch_materializer"},{"location":"integrations.pytorch.materializers.pytorch_materializer/#global-variables","text":"DEFAULT_FILENAME","title":"Global Variables"},{"location":"integrations.pytorch.materializers.pytorch_materializer/#class-pytorchmaterializer","text":"Materializer to read/write Pytorch models.","title":"class PyTorchMaterializer"},{"location":"integrations.pytorch.materializers.pytorch_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Union[Module, TorchDict] Reads and returns a PyTorch model. Returns: A loaded pytorch model.","title":"method handle_input"},{"location":"integrations.pytorch.materializers.pytorch_materializer/#method-handle_return","text":"handle_return(model: Union[Module, TorchDict]) \u2192 None Writes a PyTorch model. Args: model : A torch.nn.Module or a dict to pass into model.save This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"integrations.pytorch.materializers.pytorch_types/","text":"module integrations.pytorch.materializers.pytorch_types class TorchDict A type of dict that represents saving a model. This file was automatically generated via lazydocs .","title":"Integrations.pytorch.materializers.pytorch types"},{"location":"integrations.pytorch.materializers.pytorch_types/#module-integrationspytorchmaterializerspytorch_types","text":"","title":"module integrations.pytorch.materializers.pytorch_types"},{"location":"integrations.pytorch.materializers.pytorch_types/#class-torchdict","text":"A type of dict that represents saving a model. This file was automatically generated via lazydocs .","title":"class TorchDict"},{"location":"integrations.pytorch/","text":"module integrations.pytorch Global Variables PYTORCH class PytorchIntegration Definition of PyTorch integration for ZenML. classmethod activate activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"Integrations.pytorch"},{"location":"integrations.pytorch/#module-integrationspytorch","text":"","title":"module integrations.pytorch"},{"location":"integrations.pytorch/#global-variables","text":"PYTORCH","title":"Global Variables"},{"location":"integrations.pytorch/#class-pytorchintegration","text":"Definition of PyTorch integration for ZenML.","title":"class PytorchIntegration"},{"location":"integrations.pytorch/#classmethod-activate","text":"activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"integrations.pytorch_lightning.materializers/","text":"module integrations.pytorch_lightning.materializers This file was automatically generated via lazydocs .","title":"Integrations.pytorch lightning.materializers"},{"location":"integrations.pytorch_lightning.materializers/#module-integrationspytorch_lightningmaterializers","text":"This file was automatically generated via lazydocs .","title":"module integrations.pytorch_lightning.materializers"},{"location":"integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/","text":"module integrations.pytorch_lightning.materializers.pytorch_lightning_materializer Global Variables CHECKPOINT_NAME class PyTorchLightningMaterializer Materializer to read/write Pytorch models. method handle_input handle_input(data_type: Type[Any]) \u2192 Trainer Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object. method handle_return handle_return(trainer: Trainer) \u2192 None Writes a PyTorch Lightning trainer. Args: trainer : A PyTorch Lightning trainer object. This file was automatically generated via lazydocs .","title":"Integrations.pytorch lightning.materializers.pytorch lightning materializer"},{"location":"integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/#module-integrationspytorch_lightningmaterializerspytorch_lightning_materializer","text":"","title":"module integrations.pytorch_lightning.materializers.pytorch_lightning_materializer"},{"location":"integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/#global-variables","text":"CHECKPOINT_NAME","title":"Global Variables"},{"location":"integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/#class-pytorchlightningmaterializer","text":"Materializer to read/write Pytorch models.","title":"class PyTorchLightningMaterializer"},{"location":"integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Trainer Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object.","title":"method handle_input"},{"location":"integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/#method-handle_return","text":"handle_return(trainer: Trainer) \u2192 None Writes a PyTorch Lightning trainer. Args: trainer : A PyTorch Lightning trainer object. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"integrations.pytorch_lightning/","text":"module integrations.pytorch_lightning Global Variables PYTORCH_L class PytorchLightningIntegration Definition of PyTorch Lightning integration for ZenML. classmethod activate activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"Integrations.pytorch lightning"},{"location":"integrations.pytorch_lightning/#module-integrationspytorch_lightning","text":"","title":"module integrations.pytorch_lightning"},{"location":"integrations.pytorch_lightning/#global-variables","text":"PYTORCH_L","title":"Global Variables"},{"location":"integrations.pytorch_lightning/#class-pytorchlightningintegration","text":"Definition of PyTorch Lightning integration for ZenML.","title":"class PytorchLightningIntegration"},{"location":"integrations.pytorch_lightning/#classmethod-activate","text":"activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"integrations.registry/","text":"module integrations.registry Global Variables TYPE_CHECKING integration_registry class IntegrationRegistry Registry to keep track of ZenML Integrations method __init__ __init__() \u2192 None Initializing the integration registry property integrations Method to get integrations dictionary. Returns: A dict of integration key to type of Integration . property list_integration_names Get a list of all possible integrations method activate_integrations activate_integrations() \u2192 None Method to activate the integrations with are registered in the registry method is_installed is_installed(integration_name: Optional[str] = None) \u2192 bool Checks if all requirements for an integration are installed method register_integration register_integration(key: str, type_: Type[ForwardRef('Integration')]) \u2192 None Method to register an integration with a given name method select_integration_requirements select_integration_requirements( integration_name: Optional[str] = None ) \u2192 List[str] Select the requirements for a given integration or all integrations This file was automatically generated via lazydocs .","title":"Integrations.registry"},{"location":"integrations.registry/#module-integrationsregistry","text":"","title":"module integrations.registry"},{"location":"integrations.registry/#global-variables","text":"TYPE_CHECKING integration_registry","title":"Global Variables"},{"location":"integrations.registry/#class-integrationregistry","text":"Registry to keep track of ZenML Integrations","title":"class IntegrationRegistry"},{"location":"integrations.registry/#method-__init__","text":"__init__() \u2192 None Initializing the integration registry","title":"method __init__"},{"location":"integrations.registry/#property-integrations","text":"Method to get integrations dictionary. Returns: A dict of integration key to type of Integration .","title":"property integrations"},{"location":"integrations.registry/#property-list_integration_names","text":"Get a list of all possible integrations","title":"property list_integration_names"},{"location":"integrations.registry/#method-activate_integrations","text":"activate_integrations() \u2192 None Method to activate the integrations with are registered in the registry","title":"method activate_integrations"},{"location":"integrations.registry/#method-is_installed","text":"is_installed(integration_name: Optional[str] = None) \u2192 bool Checks if all requirements for an integration are installed","title":"method is_installed"},{"location":"integrations.registry/#method-register_integration","text":"register_integration(key: str, type_: Type[ForwardRef('Integration')]) \u2192 None Method to register an integration with a given name","title":"method register_integration"},{"location":"integrations.registry/#method-select_integration_requirements","text":"select_integration_requirements( integration_name: Optional[str] = None ) \u2192 List[str] Select the requirements for a given integration or all integrations This file was automatically generated via lazydocs .","title":"method select_integration_requirements"},{"location":"integrations.sklearn.helpers.digits/","text":"module integrations.sklearn.helpers.digits function get_digits get_digits() \u2192 Tuple[ndarray, ndarray, ndarray, ndarray] Returns the digits dataset in the form of a tuple of numpy arrays. function get_digits_model get_digits_model() \u2192 ClassifierMixin Creates a support vector classifier for digits dataset. This file was automatically generated via lazydocs .","title":"Integrations.sklearn.helpers.digits"},{"location":"integrations.sklearn.helpers.digits/#module-integrationssklearnhelpersdigits","text":"","title":"module integrations.sklearn.helpers.digits"},{"location":"integrations.sklearn.helpers.digits/#function-get_digits","text":"get_digits() \u2192 Tuple[ndarray, ndarray, ndarray, ndarray] Returns the digits dataset in the form of a tuple of numpy arrays.","title":"function get_digits"},{"location":"integrations.sklearn.helpers.digits/#function-get_digits_model","text":"get_digits_model() \u2192 ClassifierMixin Creates a support vector classifier for digits dataset. This file was automatically generated via lazydocs .","title":"function get_digits_model"},{"location":"integrations.sklearn.helpers/","text":"module integrations.sklearn.helpers This file was automatically generated via lazydocs .","title":"Integrations.sklearn.helpers"},{"location":"integrations.sklearn.helpers/#module-integrationssklearnhelpers","text":"This file was automatically generated via lazydocs .","title":"module integrations.sklearn.helpers"},{"location":"integrations.sklearn.materializers/","text":"module integrations.sklearn.materializers This file was automatically generated via lazydocs .","title":"Integrations.sklearn.materializers"},{"location":"integrations.sklearn.materializers/#module-integrationssklearnmaterializers","text":"This file was automatically generated via lazydocs .","title":"module integrations.sklearn.materializers"},{"location":"integrations.sklearn.materializers.sklearn_materializer/","text":"module integrations.sklearn.materializers.sklearn_materializer Global Variables DEFAULT_FILENAME class SklearnMaterializer Materializer to read data to and from sklearn. method handle_input handle_input( data_type: Type[Any] ) \u2192 Union[BaseEstimator, ClassifierMixin, ClusterMixin, BiclusterMixin, OutlierMixin, RegressorMixin, MetaEstimatorMixin, MultiOutputMixin, DensityMixin, TransformerMixin] Reads a base sklearn model from a pickle file. method handle_return handle_return( clf: Union[BaseEstimator, ClassifierMixin, ClusterMixin, BiclusterMixin, OutlierMixin, RegressorMixin, MetaEstimatorMixin, MultiOutputMixin, DensityMixin, TransformerMixin] ) \u2192 None Creates a pickle for a sklearn model. Args: clf : A sklearn model. This file was automatically generated via lazydocs .","title":"Integrations.sklearn.materializers.sklearn materializer"},{"location":"integrations.sklearn.materializers.sklearn_materializer/#module-integrationssklearnmaterializerssklearn_materializer","text":"","title":"module integrations.sklearn.materializers.sklearn_materializer"},{"location":"integrations.sklearn.materializers.sklearn_materializer/#global-variables","text":"DEFAULT_FILENAME","title":"Global Variables"},{"location":"integrations.sklearn.materializers.sklearn_materializer/#class-sklearnmaterializer","text":"Materializer to read data to and from sklearn.","title":"class SklearnMaterializer"},{"location":"integrations.sklearn.materializers.sklearn_materializer/#method-handle_input","text":"handle_input( data_type: Type[Any] ) \u2192 Union[BaseEstimator, ClassifierMixin, ClusterMixin, BiclusterMixin, OutlierMixin, RegressorMixin, MetaEstimatorMixin, MultiOutputMixin, DensityMixin, TransformerMixin] Reads a base sklearn model from a pickle file.","title":"method handle_input"},{"location":"integrations.sklearn.materializers.sklearn_materializer/#method-handle_return","text":"handle_return( clf: Union[BaseEstimator, ClassifierMixin, ClusterMixin, BiclusterMixin, OutlierMixin, RegressorMixin, MetaEstimatorMixin, MultiOutputMixin, DensityMixin, TransformerMixin] ) \u2192 None Creates a pickle for a sklearn model. Args: clf : A sklearn model. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"integrations.sklearn/","text":"module integrations.sklearn Global Variables SKLEARN class SklearnIntegration Definition of sklearn integration for ZenML. classmethod activate activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"Integrations.sklearn"},{"location":"integrations.sklearn/#module-integrationssklearn","text":"","title":"module integrations.sklearn"},{"location":"integrations.sklearn/#global-variables","text":"SKLEARN","title":"Global Variables"},{"location":"integrations.sklearn/#class-sklearnintegration","text":"Definition of sklearn integration for ZenML.","title":"class SklearnIntegration"},{"location":"integrations.sklearn/#classmethod-activate","text":"activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"integrations.sklearn.steps/","text":"module integrations.sklearn.steps This file was automatically generated via lazydocs .","title":"Integrations.sklearn.steps"},{"location":"integrations.sklearn.steps/#module-integrationssklearnsteps","text":"This file was automatically generated via lazydocs .","title":"module integrations.sklearn.steps"},{"location":"integrations.sklearn.steps.sklearn_evaluator/","text":"module integrations.sklearn.steps.sklearn_evaluator class SklearnEvaluatorConfig Config class for the sklearn evaluator class SklearnEvaluator A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset property component Returns a TFX component. method entrypoint entrypoint( dataset: DataFrame, model: Model, config: SklearnEvaluatorConfig ) \u2192 dict Method which is responsible for the computation of the evaluation Args: dataset : a pandas Dataframe which represents the test dataset model : a trained tensorflow Keras model config : the configuration for the step Returns: a dictionary which has the evaluation report This file was automatically generated via lazydocs .","title":"Integrations.sklearn.steps.sklearn evaluator"},{"location":"integrations.sklearn.steps.sklearn_evaluator/#module-integrationssklearnstepssklearn_evaluator","text":"","title":"module integrations.sklearn.steps.sklearn_evaluator"},{"location":"integrations.sklearn.steps.sklearn_evaluator/#class-sklearnevaluatorconfig","text":"Config class for the sklearn evaluator","title":"class SklearnEvaluatorConfig"},{"location":"integrations.sklearn.steps.sklearn_evaluator/#class-sklearnevaluator","text":"A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset","title":"class SklearnEvaluator"},{"location":"integrations.sklearn.steps.sklearn_evaluator/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"integrations.sklearn.steps.sklearn_evaluator/#method-entrypoint","text":"entrypoint( dataset: DataFrame, model: Model, config: SklearnEvaluatorConfig ) \u2192 dict Method which is responsible for the computation of the evaluation Args: dataset : a pandas Dataframe which represents the test dataset model : a trained tensorflow Keras model config : the configuration for the step Returns: a dictionary which has the evaluation report This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"integrations.sklearn.steps.sklearn_splitter/","text":"module integrations.sklearn.steps.sklearn_splitter class SklearnSplitterConfig Config class for the sklearn splitter class SklearnSplitter A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits property component Returns a TFX component. method entrypoint entrypoint( dataset: DataFrame, config: SklearnSplitterConfig ) \u2192 <Output object at 0x7fe5c7f80700> Method which is responsible for the splitting logic Args: dataset : a pandas Dataframe which entire dataset config : the configuration for the step Returns: three dataframes representing the splits This file was automatically generated via lazydocs .","title":"Integrations.sklearn.steps.sklearn splitter"},{"location":"integrations.sklearn.steps.sklearn_splitter/#module-integrationssklearnstepssklearn_splitter","text":"","title":"module integrations.sklearn.steps.sklearn_splitter"},{"location":"integrations.sklearn.steps.sklearn_splitter/#class-sklearnsplitterconfig","text":"Config class for the sklearn splitter","title":"class SklearnSplitterConfig"},{"location":"integrations.sklearn.steps.sklearn_splitter/#class-sklearnsplitter","text":"A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits","title":"class SklearnSplitter"},{"location":"integrations.sklearn.steps.sklearn_splitter/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"integrations.sklearn.steps.sklearn_splitter/#method-entrypoint","text":"entrypoint( dataset: DataFrame, config: SklearnSplitterConfig ) \u2192 <Output object at 0x7fe5c7f80700> Method which is responsible for the splitting logic Args: dataset : a pandas Dataframe which entire dataset config : the configuration for the step Returns: three dataframes representing the splits This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"integrations.sklearn.steps.sklearn_standard_scaler/","text":"module integrations.sklearn.steps.sklearn_standard_scaler class SklearnStandardScalerConfig Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset class SklearnStandardScaler Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame property component Returns a TFX component. method entrypoint entrypoint( train_dataset: DataFrame, test_dataset: DataFrame, validation_dataset: DataFrame, statistics: DataFrame, schema: DataFrame, config: SklearnStandardScalerConfig ) \u2192 <Output object at 0x7fe5c7f80e50> Main entrypoint function for the StandardScaler Args: train_dataset : pd.DataFrame, the training dataset test_dataset : pd.DataFrame, the test dataset validation_dataset : pd.DataFrame, the validation dataset statistics : pd.DataFrame, the statistics over the train dataset schema : pd.DataFrame, the detected schema of the dataset config : the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames This file was automatically generated via lazydocs .","title":"Integrations.sklearn.steps.sklearn standard scaler"},{"location":"integrations.sklearn.steps.sklearn_standard_scaler/#module-integrationssklearnstepssklearn_standard_scaler","text":"","title":"module integrations.sklearn.steps.sklearn_standard_scaler"},{"location":"integrations.sklearn.steps.sklearn_standard_scaler/#class-sklearnstandardscalerconfig","text":"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset","title":"class SklearnStandardScalerConfig"},{"location":"integrations.sklearn.steps.sklearn_standard_scaler/#class-sklearnstandardscaler","text":"Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame","title":"class SklearnStandardScaler"},{"location":"integrations.sklearn.steps.sklearn_standard_scaler/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"integrations.sklearn.steps.sklearn_standard_scaler/#method-entrypoint","text":"entrypoint( train_dataset: DataFrame, test_dataset: DataFrame, validation_dataset: DataFrame, statistics: DataFrame, schema: DataFrame, config: SklearnStandardScalerConfig ) \u2192 <Output object at 0x7fe5c7f80e50> Main entrypoint function for the StandardScaler Args: train_dataset : pd.DataFrame, the training dataset test_dataset : pd.DataFrame, the test dataset validation_dataset : pd.DataFrame, the validation dataset statistics : pd.DataFrame, the statistics over the train dataset schema : pd.DataFrame, the detected schema of the dataset config : the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"integrations.tensorflow.materializers.keras_materializer/","text":"module integrations.tensorflow.materializers.keras_materializer Global Variables DEFAULT_FILENAME class KerasMaterializer Materializer to read/write Keras models. method handle_input handle_input(data_type: Type[Any]) \u2192 Model Reads and returns a Keras model. Returns: A tf.keras.Model model. method handle_return handle_return(model: Model) \u2192 None Writes a keras model. Args: model : A tf.keras.Model model. This file was automatically generated via lazydocs .","title":"Integrations.tensorflow.materializers.keras materializer"},{"location":"integrations.tensorflow.materializers.keras_materializer/#module-integrationstensorflowmaterializerskeras_materializer","text":"","title":"module integrations.tensorflow.materializers.keras_materializer"},{"location":"integrations.tensorflow.materializers.keras_materializer/#global-variables","text":"DEFAULT_FILENAME","title":"Global Variables"},{"location":"integrations.tensorflow.materializers.keras_materializer/#class-kerasmaterializer","text":"Materializer to read/write Keras models.","title":"class KerasMaterializer"},{"location":"integrations.tensorflow.materializers.keras_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Model Reads and returns a Keras model. Returns: A tf.keras.Model model.","title":"method handle_input"},{"location":"integrations.tensorflow.materializers.keras_materializer/#method-handle_return","text":"handle_return(model: Model) \u2192 None Writes a keras model. Args: model : A tf.keras.Model model. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"integrations.tensorflow.materializers/","text":"module integrations.tensorflow.materializers This file was automatically generated via lazydocs .","title":"Integrations.tensorflow.materializers"},{"location":"integrations.tensorflow.materializers/#module-integrationstensorflowmaterializers","text":"This file was automatically generated via lazydocs .","title":"module integrations.tensorflow.materializers"},{"location":"integrations.tensorflow.materializers.tf_dataset_materializer/","text":"module integrations.tensorflow.materializers.tf_dataset_materializer Global Variables DEFAULT_FILENAME class TensorflowDatasetMaterializer Materializer to read data to and from tf.data.Dataset. method handle_input handle_input(data_type: Type[Any]) \u2192 Any Reads data into tf.data.Dataset method handle_return handle_return(dataset: DatasetV2) \u2192 None Persists a tf.data.Dataset object. This file was automatically generated via lazydocs .","title":"Integrations.tensorflow.materializers.tf dataset materializer"},{"location":"integrations.tensorflow.materializers.tf_dataset_materializer/#module-integrationstensorflowmaterializerstf_dataset_materializer","text":"","title":"module integrations.tensorflow.materializers.tf_dataset_materializer"},{"location":"integrations.tensorflow.materializers.tf_dataset_materializer/#global-variables","text":"DEFAULT_FILENAME","title":"Global Variables"},{"location":"integrations.tensorflow.materializers.tf_dataset_materializer/#class-tensorflowdatasetmaterializer","text":"Materializer to read data to and from tf.data.Dataset.","title":"class TensorflowDatasetMaterializer"},{"location":"integrations.tensorflow.materializers.tf_dataset_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Any Reads data into tf.data.Dataset","title":"method handle_input"},{"location":"integrations.tensorflow.materializers.tf_dataset_materializer/#method-handle_return","text":"handle_return(dataset: DatasetV2) \u2192 None Persists a tf.data.Dataset object. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"integrations.tensorflow/","text":"module integrations.tensorflow Global Variables TENSORFLOW class TensorflowIntegration Definition of Tensorflow integration for ZenML. classmethod activate activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"Integrations.tensorflow"},{"location":"integrations.tensorflow/#module-integrationstensorflow","text":"","title":"module integrations.tensorflow"},{"location":"integrations.tensorflow/#global-variables","text":"TENSORFLOW","title":"Global Variables"},{"location":"integrations.tensorflow/#class-tensorflowintegration","text":"Definition of Tensorflow integration for ZenML.","title":"class TensorflowIntegration"},{"location":"integrations.tensorflow/#classmethod-activate","text":"activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"integrations.tensorflow.steps/","text":"module integrations.tensorflow.steps This file was automatically generated via lazydocs .","title":"Integrations.tensorflow.steps"},{"location":"integrations.tensorflow.steps/#module-integrationstensorflowsteps","text":"This file was automatically generated via lazydocs .","title":"module integrations.tensorflow.steps"},{"location":"integrations.tensorflow.steps.tensorflow_trainer/","text":"module integrations.tensorflow.steps.tensorflow_trainer class TensorflowBinaryClassifierConfig Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch class TensorflowBinaryClassifier Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset property component Returns a TFX component. method entrypoint entrypoint( train_dataset: DataFrame, validation_dataset: DataFrame, config: TensorflowBinaryClassifierConfig ) \u2192 Model Main entrypoint for the tensorflow trainer Args: train_dataset : pd.DataFrame, the training dataset validation_dataset : pd.DataFrame, the validation dataset config : the configuration of the step Returns: the trained tf.keras.Model This file was automatically generated via lazydocs .","title":"Integrations.tensorflow.steps.tensorflow trainer"},{"location":"integrations.tensorflow.steps.tensorflow_trainer/#module-integrationstensorflowstepstensorflow_trainer","text":"","title":"module integrations.tensorflow.steps.tensorflow_trainer"},{"location":"integrations.tensorflow.steps.tensorflow_trainer/#class-tensorflowbinaryclassifierconfig","text":"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch","title":"class TensorflowBinaryClassifierConfig"},{"location":"integrations.tensorflow.steps.tensorflow_trainer/#class-tensorflowbinaryclassifier","text":"Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset","title":"class TensorflowBinaryClassifier"},{"location":"integrations.tensorflow.steps.tensorflow_trainer/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"integrations.tensorflow.steps.tensorflow_trainer/#method-entrypoint","text":"entrypoint( train_dataset: DataFrame, validation_dataset: DataFrame, config: TensorflowBinaryClassifierConfig ) \u2192 Model Main entrypoint for the tensorflow trainer Args: train_dataset : pd.DataFrame, the training dataset validation_dataset : pd.DataFrame, the validation dataset config : the configuration of the step Returns: the trained tf.keras.Model This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"integrations.utils/","text":"module integrations.utils function get_integration_for_module get_integration_for_module( module_name: str ) \u2192 Union[Type[Integration], NoneType] Gets the integration class for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return None . If it is part of a ZenML integration, it will return the integration class found inside the integration init file. function get_requirements_for_module get_requirements_for_module(module_name: str) \u2192 List[str] Gets requirements for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration init file. This file was automatically generated via lazydocs .","title":"Integrations.utils"},{"location":"integrations.utils/#module-integrationsutils","text":"","title":"module integrations.utils"},{"location":"integrations.utils/#function-get_integration_for_module","text":"get_integration_for_module( module_name: str ) \u2192 Union[Type[Integration], NoneType] Gets the integration class for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return None . If it is part of a ZenML integration, it will return the integration class found inside the integration init file.","title":"function get_integration_for_module"},{"location":"integrations.utils/#function-get_requirements_for_module","text":"get_requirements_for_module(module_name: str) \u2192 List[str] Gets requirements for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration init file. This file was automatically generated via lazydocs .","title":"function get_requirements_for_module"},{"location":"io.fileio/","text":"module io.fileio Global Variables REMOTE_FS_PREFIX function open open(path: Union[bytes, str], mode: str = 'r') \u2192 Any Open a file at the given path. function copy copy( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Copy a file from the source to the destination. function file_exists file_exists(path: Union[bytes, str]) \u2192 bool Returns True if the given path exists. function remove remove(path: Union[bytes, str]) \u2192 None Remove the file at the given path. Dangerous operation. function glob glob(pattern: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return the paths that match a glob pattern. function is_dir is_dir(path: Union[bytes, str]) \u2192 bool Returns whether the given path points to a directory. function is_root is_root(path: str) \u2192 bool Returns true if path has no parent in local filesystem. Args: path : Local path in filesystem. Returns: True if root, else False. function list_dir list_dir(dir_path: str, only_file_names: bool = False) \u2192 List[str] Returns a list of files under dir. Args: dir_path : Path in filesystem. only_file_names : Returns only file names if True. Returns: List of full qualified paths. function make_dirs make_dirs(path: Union[bytes, str]) \u2192 None Make a directory at the given path, recursively creating parents. function mkdir mkdir(path: Union[bytes, str]) \u2192 None Make a directory at the given path; parent directory must exist. function rename rename( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Rename source file to destination file. Args: src : The path of the file to rename. dst : The path to rename the source file to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileExistsError : If a file already exists at the destination and overwrite is not set to True . function rm_dir rm_dir(dir_path: str) \u2192 None Deletes dir recursively. Dangerous operation. Args: dir_path : Dir to delete. function stat stat(path: Union[bytes, str]) \u2192 Any Return the stat descriptor for a given file path. function walk walk( top: Union[bytes, str], topdown: bool = True, onerror: Optional[Callable[], NoneType] = None ) \u2192 Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] Return an iterator that walks the contents of the given directory. Args: top : Path of directory to walk. topdown : Whether to walk directories topdown or bottom-up. onerror : Callable that gets called if an error occurs. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. function find_files find_files(dir_path: Union[bytes, str], pattern: str) \u2192 Iterable[str] Find files in a directory that match pattern. Args: dir_path : Path to directory. pattern : pattern like *.png. Yields: All matching filenames if found, else None. function is_remote is_remote(path: str) \u2192 bool Returns True if path exists remotely. Args: path : Any path as a string. Returns: True if remote path, else False. function create_file_if_not_exists create_file_if_not_exists(file_path: str, file_contents: str = '{}') \u2192 None Creates file if it does not exist. Args: file_path : Local path in filesystem. file_contents : Contents of file. function append_file append_file(file_path: str, file_contents: str) \u2192 None Appends file_contents to file. Args: file_path : Local path in filesystem. file_contents : Contents of file. function create_dir_if_not_exists create_dir_if_not_exists(dir_path: str) \u2192 None Creates directory if it does not exist. Args: dir_path (str): Local path in filesystem. function create_dir_recursive_if_not_exists create_dir_recursive_if_not_exists(dir_path: str) \u2192 None Creates directory recursively if it does not exist. Args: dir_path : Local path in filesystem. function resolve_relative_path resolve_relative_path(path: str) \u2192 str Takes relative path and resolves it absolutely. Args: path : Local path in filesystem. Returns: Resolved path. function copy_dir copy_dir(source_dir: str, destination_dir: str, overwrite: bool = False) \u2192 None Copies dir from source to destination. Args: source_dir : Path to copy from. destination_dir : Path to copy to. overwrite : Boolean. If false, function throws an error before overwrite. function move move(source: str, destination: str, overwrite: bool = False) \u2192 None Moves dir or file from source to destination. Can be used to rename. Args: source : Local path to copy from. destination : Local path to copy to. overwrite : boolean, if false, then throws an error before overwrite. function get_grandparent get_grandparent(dir_path: str) \u2192 str Get grandparent of dir. Args: dir_path : Path to directory. Returns: The input paths parents parent. function get_parent get_parent(dir_path: str) \u2192 str Get parent of dir. Args: dir_path (str): Path to directory. Returns: Parent (stem) of the dir as a string. function convert_to_str convert_to_str(path: Union[bytes, str]) \u2192 str Converts a PathType to a str using UTF-8. This file was automatically generated via lazydocs .","title":"Io.fileio"},{"location":"io.fileio/#module-iofileio","text":"","title":"module io.fileio"},{"location":"io.fileio/#global-variables","text":"REMOTE_FS_PREFIX","title":"Global Variables"},{"location":"io.fileio/#function-open","text":"open(path: Union[bytes, str], mode: str = 'r') \u2192 Any Open a file at the given path.","title":"function open"},{"location":"io.fileio/#function-copy","text":"copy( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Copy a file from the source to the destination.","title":"function copy"},{"location":"io.fileio/#function-file_exists","text":"file_exists(path: Union[bytes, str]) \u2192 bool Returns True if the given path exists.","title":"function file_exists"},{"location":"io.fileio/#function-remove","text":"remove(path: Union[bytes, str]) \u2192 None Remove the file at the given path. Dangerous operation.","title":"function remove"},{"location":"io.fileio/#function-glob","text":"glob(pattern: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return the paths that match a glob pattern.","title":"function glob"},{"location":"io.fileio/#function-is_dir","text":"is_dir(path: Union[bytes, str]) \u2192 bool Returns whether the given path points to a directory.","title":"function is_dir"},{"location":"io.fileio/#function-is_root","text":"is_root(path: str) \u2192 bool Returns true if path has no parent in local filesystem. Args: path : Local path in filesystem. Returns: True if root, else False.","title":"function is_root"},{"location":"io.fileio/#function-list_dir","text":"list_dir(dir_path: str, only_file_names: bool = False) \u2192 List[str] Returns a list of files under dir. Args: dir_path : Path in filesystem. only_file_names : Returns only file names if True. Returns: List of full qualified paths.","title":"function list_dir"},{"location":"io.fileio/#function-make_dirs","text":"make_dirs(path: Union[bytes, str]) \u2192 None Make a directory at the given path, recursively creating parents.","title":"function make_dirs"},{"location":"io.fileio/#function-mkdir","text":"mkdir(path: Union[bytes, str]) \u2192 None Make a directory at the given path; parent directory must exist.","title":"function mkdir"},{"location":"io.fileio/#function-rename","text":"rename( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Rename source file to destination file. Args: src : The path of the file to rename. dst : The path to rename the source file to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileExistsError : If a file already exists at the destination and overwrite is not set to True .","title":"function rename"},{"location":"io.fileio/#function-rm_dir","text":"rm_dir(dir_path: str) \u2192 None Deletes dir recursively. Dangerous operation. Args: dir_path : Dir to delete.","title":"function rm_dir"},{"location":"io.fileio/#function-stat","text":"stat(path: Union[bytes, str]) \u2192 Any Return the stat descriptor for a given file path.","title":"function stat"},{"location":"io.fileio/#function-walk","text":"walk( top: Union[bytes, str], topdown: bool = True, onerror: Optional[Callable[], NoneType] = None ) \u2192 Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] Return an iterator that walks the contents of the given directory. Args: top : Path of directory to walk. topdown : Whether to walk directories topdown or bottom-up. onerror : Callable that gets called if an error occurs. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory.","title":"function walk"},{"location":"io.fileio/#function-find_files","text":"find_files(dir_path: Union[bytes, str], pattern: str) \u2192 Iterable[str] Find files in a directory that match pattern. Args: dir_path : Path to directory. pattern : pattern like *.png. Yields: All matching filenames if found, else None.","title":"function find_files"},{"location":"io.fileio/#function-is_remote","text":"is_remote(path: str) \u2192 bool Returns True if path exists remotely. Args: path : Any path as a string. Returns: True if remote path, else False.","title":"function is_remote"},{"location":"io.fileio/#function-create_file_if_not_exists","text":"create_file_if_not_exists(file_path: str, file_contents: str = '{}') \u2192 None Creates file if it does not exist. Args: file_path : Local path in filesystem. file_contents : Contents of file.","title":"function create_file_if_not_exists"},{"location":"io.fileio/#function-append_file","text":"append_file(file_path: str, file_contents: str) \u2192 None Appends file_contents to file. Args: file_path : Local path in filesystem. file_contents : Contents of file.","title":"function append_file"},{"location":"io.fileio/#function-create_dir_if_not_exists","text":"create_dir_if_not_exists(dir_path: str) \u2192 None Creates directory if it does not exist. Args: dir_path (str): Local path in filesystem.","title":"function create_dir_if_not_exists"},{"location":"io.fileio/#function-create_dir_recursive_if_not_exists","text":"create_dir_recursive_if_not_exists(dir_path: str) \u2192 None Creates directory recursively if it does not exist. Args: dir_path : Local path in filesystem.","title":"function create_dir_recursive_if_not_exists"},{"location":"io.fileio/#function-resolve_relative_path","text":"resolve_relative_path(path: str) \u2192 str Takes relative path and resolves it absolutely. Args: path : Local path in filesystem. Returns: Resolved path.","title":"function resolve_relative_path"},{"location":"io.fileio/#function-copy_dir","text":"copy_dir(source_dir: str, destination_dir: str, overwrite: bool = False) \u2192 None Copies dir from source to destination. Args: source_dir : Path to copy from. destination_dir : Path to copy to. overwrite : Boolean. If false, function throws an error before overwrite.","title":"function copy_dir"},{"location":"io.fileio/#function-move","text":"move(source: str, destination: str, overwrite: bool = False) \u2192 None Moves dir or file from source to destination. Can be used to rename. Args: source : Local path to copy from. destination : Local path to copy to. overwrite : boolean, if false, then throws an error before overwrite.","title":"function move"},{"location":"io.fileio/#function-get_grandparent","text":"get_grandparent(dir_path: str) \u2192 str Get grandparent of dir. Args: dir_path : Path to directory. Returns: The input paths parents parent.","title":"function get_grandparent"},{"location":"io.fileio/#function-get_parent","text":"get_parent(dir_path: str) \u2192 str Get parent of dir. Args: dir_path (str): Path to directory. Returns: Parent (stem) of the dir as a string.","title":"function get_parent"},{"location":"io.fileio/#function-convert_to_str","text":"convert_to_str(path: Union[bytes, str]) \u2192 str Converts a PathType to a str using UTF-8. This file was automatically generated via lazydocs .","title":"function convert_to_str"},{"location":"io.fileio_registry/","text":"module io.fileio_registry Filesystem registry managing filesystem plugins. Global Variables default_fileio_registry class FileIORegistry Registry of pluggable filesystem implementations used in TFX components. method __init__ __init__() \u2192 None method get_filesystem_for_path get_filesystem_for_path(path: Union[bytes, str]) \u2192 Type[Filesystem] Get filesystem plugin for given path. method get_filesystem_for_scheme get_filesystem_for_scheme(scheme: Union[bytes, str]) \u2192 Type[Filesystem] Get filesystem plugin for given scheme string. method register register(filesystem_cls: Type[Filesystem]) \u2192 None Register a filesystem implementation. Args: filesystem_cls : Subclass of tfx.dsl.io.filesystem.Filesystem . This file was automatically generated via lazydocs .","title":"Io.fileio registry"},{"location":"io.fileio_registry/#module-iofileio_registry","text":"Filesystem registry managing filesystem plugins.","title":"module io.fileio_registry"},{"location":"io.fileio_registry/#global-variables","text":"default_fileio_registry","title":"Global Variables"},{"location":"io.fileio_registry/#class-fileioregistry","text":"Registry of pluggable filesystem implementations used in TFX components.","title":"class FileIORegistry"},{"location":"io.fileio_registry/#method-__init__","text":"__init__() \u2192 None","title":"method __init__"},{"location":"io.fileio_registry/#method-get_filesystem_for_path","text":"get_filesystem_for_path(path: Union[bytes, str]) \u2192 Type[Filesystem] Get filesystem plugin for given path.","title":"method get_filesystem_for_path"},{"location":"io.fileio_registry/#method-get_filesystem_for_scheme","text":"get_filesystem_for_scheme(scheme: Union[bytes, str]) \u2192 Type[Filesystem] Get filesystem plugin for given scheme string.","title":"method get_filesystem_for_scheme"},{"location":"io.fileio_registry/#method-register","text":"register(filesystem_cls: Type[Filesystem]) \u2192 None Register a filesystem implementation. Args: filesystem_cls : Subclass of tfx.dsl.io.filesystem.Filesystem . This file was automatically generated via lazydocs .","title":"method register"},{"location":"io.filesystem/","text":"module io.filesystem class NotFoundError Auxiliary not found error class FileSystemMeta Metaclass which is responsible for registering the defined filesystem in the default fileio registry. class Filesystem Abstract Filesystem class. This file was automatically generated via lazydocs .","title":"Io.filesystem"},{"location":"io.filesystem/#module-iofilesystem","text":"","title":"module io.filesystem"},{"location":"io.filesystem/#class-notfounderror","text":"Auxiliary not found error","title":"class NotFoundError"},{"location":"io.filesystem/#class-filesystemmeta","text":"Metaclass which is responsible for registering the defined filesystem in the default fileio registry.","title":"class FileSystemMeta"},{"location":"io.filesystem/#class-filesystem","text":"Abstract Filesystem class. This file was automatically generated via lazydocs .","title":"class Filesystem"},{"location":"io/","text":"module io The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx . Global Variables DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END class BufferedIOBase Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one. class IOBase The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!') class RawIOBase Base class for raw binary I/O. class TextIOBase Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor. class UnsupportedOperation This file was automatically generated via lazydocs .","title":"Io"},{"location":"io/#module-io","text":"The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx .","title":"module io"},{"location":"io/#global-variables","text":"DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END","title":"Global Variables"},{"location":"io/#class-bufferediobase","text":"Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one.","title":"class BufferedIOBase"},{"location":"io/#class-iobase","text":"The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!')","title":"class IOBase"},{"location":"io/#class-rawiobase","text":"Base class for raw binary I/O.","title":"class RawIOBase"},{"location":"io/#class-textiobase","text":"Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor.","title":"class TextIOBase"},{"location":"io/#class-unsupportedoperation","text":"This file was automatically generated via lazydocs .","title":"class UnsupportedOperation"},{"location":"io.utils/","text":"module io.utils Global Variables APP_NAME ENV_ZENML_REPOSITORY_PATH ZENML_DIR_NAME function create_tarfile create_tarfile( source_dir: str, output_filename: str = 'zipped.tar.gz', exclude_function: Optional[Callable[[TarInfo], Union[TarInfo]], NoneType] = None ) \u2192 None Create a compressed representation of source_dir. Args: source_dir : Path to source dir. output_filename : Name of outputted gz. exclude_function : Function that determines whether to exclude file. function extract_tarfile extract_tarfile(source_tar: str, output_dir: str) \u2192 None Extracts all files in a compressed tar file to output_dir. Args: source_tar : Path to a tar compressed file. output_dir : Directory where to extract. function is_zenml_dir is_zenml_dir(path: str) \u2192 bool Check if dir is a zenml dir or not. Args: path : Path to the root. Returns: True if path contains a zenml dir, False if not. function get_zenml_config_dir get_zenml_config_dir(path: Optional[str] = None) \u2192 str Recursive function to find the zenml config starting from path. Args: path (Default value = os.getcwd()): Path to check. Returns: The full path with the resolved zenml directory. Raises: InitializationException if directory not found until root of OS. function get_zenml_dir get_zenml_dir(path: Optional[str] = None) \u2192 str Returns path to a ZenML repository directory. Args: path : Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable ZENML_REPOSITORY_PATH (if set) and recursively searching in the parent directories of the current working directory. Returns: Absolute path to a ZenML repository directory. Raises: InitializationException : If no ZenML repository is found. function get_global_config_directory get_global_config_directory() \u2192 str Returns the global config directory for ZenML. function write_file_contents_as_string write_file_contents_as_string(file_path: str, content: str) \u2192 None Writes contents of file. Args: file_path : Path to file. content : Contents of file. function read_file_contents_as_string read_file_contents_as_string(file_path: str) \u2192 str Reads contents of file. Args: file_path : Path to file. function is_gcs_path is_gcs_path(path: str) \u2192 bool Returns True if path is on Google Cloud Storage. Args: path : Any path as a string. Returns: True if gcs path, else False. This file was automatically generated via lazydocs .","title":"Io.utils"},{"location":"io.utils/#module-ioutils","text":"","title":"module io.utils"},{"location":"io.utils/#global-variables","text":"APP_NAME ENV_ZENML_REPOSITORY_PATH ZENML_DIR_NAME","title":"Global Variables"},{"location":"io.utils/#function-create_tarfile","text":"create_tarfile( source_dir: str, output_filename: str = 'zipped.tar.gz', exclude_function: Optional[Callable[[TarInfo], Union[TarInfo]], NoneType] = None ) \u2192 None Create a compressed representation of source_dir. Args: source_dir : Path to source dir. output_filename : Name of outputted gz. exclude_function : Function that determines whether to exclude file.","title":"function create_tarfile"},{"location":"io.utils/#function-extract_tarfile","text":"extract_tarfile(source_tar: str, output_dir: str) \u2192 None Extracts all files in a compressed tar file to output_dir. Args: source_tar : Path to a tar compressed file. output_dir : Directory where to extract.","title":"function extract_tarfile"},{"location":"io.utils/#function-is_zenml_dir","text":"is_zenml_dir(path: str) \u2192 bool Check if dir is a zenml dir or not. Args: path : Path to the root. Returns: True if path contains a zenml dir, False if not.","title":"function is_zenml_dir"},{"location":"io.utils/#function-get_zenml_config_dir","text":"get_zenml_config_dir(path: Optional[str] = None) \u2192 str Recursive function to find the zenml config starting from path. Args: path (Default value = os.getcwd()): Path to check. Returns: The full path with the resolved zenml directory. Raises: InitializationException if directory not found until root of OS.","title":"function get_zenml_config_dir"},{"location":"io.utils/#function-get_zenml_dir","text":"get_zenml_dir(path: Optional[str] = None) \u2192 str Returns path to a ZenML repository directory. Args: path : Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable ZENML_REPOSITORY_PATH (if set) and recursively searching in the parent directories of the current working directory. Returns: Absolute path to a ZenML repository directory. Raises: InitializationException : If no ZenML repository is found.","title":"function get_zenml_dir"},{"location":"io.utils/#function-get_global_config_directory","text":"get_global_config_directory() \u2192 str Returns the global config directory for ZenML.","title":"function get_global_config_directory"},{"location":"io.utils/#function-write_file_contents_as_string","text":"write_file_contents_as_string(file_path: str, content: str) \u2192 None Writes contents of file. Args: file_path : Path to file. content : Contents of file.","title":"function write_file_contents_as_string"},{"location":"io.utils/#function-read_file_contents_as_string","text":"read_file_contents_as_string(file_path: str) \u2192 str Reads contents of file. Args: file_path : Path to file.","title":"function read_file_contents_as_string"},{"location":"io.utils/#function-is_gcs_path","text":"is_gcs_path(path: str) \u2192 bool Returns True if path is on Google Cloud Storage. Args: path : Any path as a string. Returns: True if gcs path, else False. This file was automatically generated via lazydocs .","title":"function is_gcs_path"},{"location":"logger/","text":"module logger Global Variables ZENML_LOGGING_VERBOSITY ABSL_LOGGING_VERBOSITY APP_NAME LOG_FILE function get_logging_level get_logging_level() \u2192 <enum 'LoggingLevels'> Get logging level from the env variable. function set_root_verbosity set_root_verbosity() \u2192 None Set the root verbosity. function get_console_handler get_console_handler() \u2192 Any Get console handler for logging. function get_file_handler get_file_handler() \u2192 Any Return a file handler for logging. function get_logger get_logger(logger_name: str) \u2192 Logger Main function to get logger name,. Args: logger_name : Name of logger to initialize. Returns: A logger object. function init_logging init_logging() \u2192 None Initialize logging with default levels. class CustomFormatter Formats logs according to custom specifications. method format format(record: LogRecord) \u2192 str Converts a log record to a (colored) string Args: record : LogRecord generated by the code. Returns: A string formatted according to specifications. This file was automatically generated via lazydocs .","title":"Logger"},{"location":"logger/#module-logger","text":"","title":"module logger"},{"location":"logger/#global-variables","text":"ZENML_LOGGING_VERBOSITY ABSL_LOGGING_VERBOSITY APP_NAME LOG_FILE","title":"Global Variables"},{"location":"logger/#function-get_logging_level","text":"get_logging_level() \u2192 <enum 'LoggingLevels'> Get logging level from the env variable.","title":"function get_logging_level"},{"location":"logger/#function-set_root_verbosity","text":"set_root_verbosity() \u2192 None Set the root verbosity.","title":"function set_root_verbosity"},{"location":"logger/#function-get_console_handler","text":"get_console_handler() \u2192 Any Get console handler for logging.","title":"function get_console_handler"},{"location":"logger/#function-get_file_handler","text":"get_file_handler() \u2192 Any Return a file handler for logging.","title":"function get_file_handler"},{"location":"logger/#function-get_logger","text":"get_logger(logger_name: str) \u2192 Logger Main function to get logger name,. Args: logger_name : Name of logger to initialize. Returns: A logger object.","title":"function get_logger"},{"location":"logger/#function-init_logging","text":"init_logging() \u2192 None Initialize logging with default levels.","title":"function init_logging"},{"location":"logger/#class-customformatter","text":"Formats logs according to custom specifications.","title":"class CustomFormatter"},{"location":"logger/#method-format","text":"format(record: LogRecord) \u2192 str Converts a log record to a (colored) string Args: record : LogRecord generated by the code. Returns: A string formatted according to specifications. This file was automatically generated via lazydocs .","title":"method format"},{"location":"materializers.base_materializer/","text":"module materializers.base_materializer Global Variables TYPE_CHECKING class BaseMaterializerMeta Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts. class BaseMaterializer Base Materializer to realize artifact data. method __init__ __init__(artifact: 'BaseArtifact') Initializes a materializer with the given artifact. method handle_input handle_input(data_type: Type[Any]) \u2192 Any Write logic here to handle input of the step function. Args: data_type : What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step. method handle_return handle_return(data: Any) \u2192 None Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. This file was automatically generated via lazydocs .","title":"Materializers.base materializer"},{"location":"materializers.base_materializer/#module-materializersbase_materializer","text":"","title":"module materializers.base_materializer"},{"location":"materializers.base_materializer/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"materializers.base_materializer/#class-basematerializermeta","text":"Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts.","title":"class BaseMaterializerMeta"},{"location":"materializers.base_materializer/#class-basematerializer","text":"Base Materializer to realize artifact data.","title":"class BaseMaterializer"},{"location":"materializers.base_materializer/#method-__init__","text":"__init__(artifact: 'BaseArtifact') Initializes a materializer with the given artifact.","title":"method __init__"},{"location":"materializers.base_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Any Write logic here to handle input of the step function. Args: data_type : What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step.","title":"method handle_input"},{"location":"materializers.base_materializer/#method-handle_return","text":"handle_return(data: Any) \u2192 None Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"materializers.beam_materializer/","text":"module materializers.beam_materializer class BeamMaterializer Materializer to read data to and from beam. method handle_input handle_input(data_type: Type[Any]) \u2192 Any Reads all files inside the artifact directory and materializes them as a beam compatible output. method handle_return handle_return(pipeline: Pipeline) \u2192 None Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline : A beam.pipeline object. This file was automatically generated via lazydocs .","title":"Materializers.beam materializer"},{"location":"materializers.beam_materializer/#module-materializersbeam_materializer","text":"","title":"module materializers.beam_materializer"},{"location":"materializers.beam_materializer/#class-beammaterializer","text":"Materializer to read data to and from beam.","title":"class BeamMaterializer"},{"location":"materializers.beam_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Any Reads all files inside the artifact directory and materializes them as a beam compatible output.","title":"method handle_input"},{"location":"materializers.beam_materializer/#method-handle_return","text":"handle_return(pipeline: Pipeline) \u2192 None Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline : A beam.pipeline object. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"materializers.built_in_materializer/","text":"module materializers.built_in_materializer Global Variables DEFAULT_FILENAME class BuiltInMaterializer Read/Write JSON files. method handle_input handle_input(data_type: Type[Any]) \u2192 Any Reads basic primitive types from json. method handle_return handle_return(data: Any) \u2192 None Handles basic built-in types and stores them as json This file was automatically generated via lazydocs .","title":"Materializers.built in materializer"},{"location":"materializers.built_in_materializer/#module-materializersbuilt_in_materializer","text":"","title":"module materializers.built_in_materializer"},{"location":"materializers.built_in_materializer/#global-variables","text":"DEFAULT_FILENAME","title":"Global Variables"},{"location":"materializers.built_in_materializer/#class-builtinmaterializer","text":"Read/Write JSON files.","title":"class BuiltInMaterializer"},{"location":"materializers.built_in_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Any Reads basic primitive types from json.","title":"method handle_input"},{"location":"materializers.built_in_materializer/#method-handle_return","text":"handle_return(data: Any) \u2192 None Handles basic built-in types and stores them as json This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"materializers.default_materializer_registry/","text":"module materializers.default_materializer_registry Global Variables TYPE_CHECKING default_materializer_registry class MaterializerRegistry Matches a python type to a default materializer. method __init__ __init__() \u2192 None method get_materializer_types get_materializer_types() \u2192 Dict[Type[Any], Type[ForwardRef('BaseMaterializer')]] Get all registered materializer types. method is_registered is_registered(key: Type[Any]) \u2192 bool Returns if a materializer class is registered for the given type. method register_and_overwrite_type register_and_overwrite_type( key: Type[Any], type_: Type[ForwardRef('BaseMaterializer')] ) \u2192 None Registers a new materializer and also overwrites a default if set. Args: key : Indicates the type of an object. type_ : A BaseMaterializer subclass. method register_materializer_type register_materializer_type( key: Type[Any], type_: Type[ForwardRef('BaseMaterializer')] ) \u2192 None Registers a new materializer. Args: key : Indicates the type of an object. type_ : A BaseMaterializer subclass. This file was automatically generated via lazydocs .","title":"Materializers.default materializer registry"},{"location":"materializers.default_materializer_registry/#module-materializersdefault_materializer_registry","text":"","title":"module materializers.default_materializer_registry"},{"location":"materializers.default_materializer_registry/#global-variables","text":"TYPE_CHECKING default_materializer_registry","title":"Global Variables"},{"location":"materializers.default_materializer_registry/#class-materializerregistry","text":"Matches a python type to a default materializer.","title":"class MaterializerRegistry"},{"location":"materializers.default_materializer_registry/#method-__init__","text":"__init__() \u2192 None","title":"method __init__"},{"location":"materializers.default_materializer_registry/#method-get_materializer_types","text":"get_materializer_types() \u2192 Dict[Type[Any], Type[ForwardRef('BaseMaterializer')]] Get all registered materializer types.","title":"method get_materializer_types"},{"location":"materializers.default_materializer_registry/#method-is_registered","text":"is_registered(key: Type[Any]) \u2192 bool Returns if a materializer class is registered for the given type.","title":"method is_registered"},{"location":"materializers.default_materializer_registry/#method-register_and_overwrite_type","text":"register_and_overwrite_type( key: Type[Any], type_: Type[ForwardRef('BaseMaterializer')] ) \u2192 None Registers a new materializer and also overwrites a default if set. Args: key : Indicates the type of an object. type_ : A BaseMaterializer subclass.","title":"method register_and_overwrite_type"},{"location":"materializers.default_materializer_registry/#method-register_materializer_type","text":"register_materializer_type( key: Type[Any], type_: Type[ForwardRef('BaseMaterializer')] ) \u2192 None Registers a new materializer. Args: key : Indicates the type of an object. type_ : A BaseMaterializer subclass. This file was automatically generated via lazydocs .","title":"method register_materializer_type"},{"location":"materializers/","text":"module materializers Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class. This file was automatically generated via lazydocs .","title":"Materializers"},{"location":"materializers/#module-materializers","text":"Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class. This file was automatically generated via lazydocs .","title":"module materializers"},{"location":"materializers.numpy_materializer/","text":"module materializers.numpy_materializer Global Variables DATA_FILENAME SHAPE_FILENAME DATA_VAR class NumpyMaterializer Materializer to read data to and from pandas. method handle_input handle_input(data_type: Type[Any]) \u2192 ndarray Reads numpy array from parquet file. method handle_return handle_return(arr: ndarray) \u2192 None Writes a np.ndarray to the artifact store as a parquet file. Args: arr : The numpy array to write. This file was automatically generated via lazydocs .","title":"Materializers.numpy materializer"},{"location":"materializers.numpy_materializer/#module-materializersnumpy_materializer","text":"","title":"module materializers.numpy_materializer"},{"location":"materializers.numpy_materializer/#global-variables","text":"DATA_FILENAME SHAPE_FILENAME DATA_VAR","title":"Global Variables"},{"location":"materializers.numpy_materializer/#class-numpymaterializer","text":"Materializer to read data to and from pandas.","title":"class NumpyMaterializer"},{"location":"materializers.numpy_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 ndarray Reads numpy array from parquet file.","title":"method handle_input"},{"location":"materializers.numpy_materializer/#method-handle_return","text":"handle_return(arr: ndarray) \u2192 None Writes a np.ndarray to the artifact store as a parquet file. Args: arr : The numpy array to write. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"materializers.pandas_materializer/","text":"module materializers.pandas_materializer Global Variables DEFAULT_FILENAME COMPRESSION_TYPE class PandasMaterializer Materializer to read data to and from pandas. method handle_input handle_input(data_type: Type[Any]) \u2192 DataFrame Reads pd.Dataframe from a parquet file. method handle_return handle_return(df: DataFrame) \u2192 None Writes a pandas dataframe to the specified filename. Args: df : The pandas dataframe to write. This file was automatically generated via lazydocs .","title":"Materializers.pandas materializer"},{"location":"materializers.pandas_materializer/#module-materializerspandas_materializer","text":"","title":"module materializers.pandas_materializer"},{"location":"materializers.pandas_materializer/#global-variables","text":"DEFAULT_FILENAME COMPRESSION_TYPE","title":"Global Variables"},{"location":"materializers.pandas_materializer/#class-pandasmaterializer","text":"Materializer to read data to and from pandas.","title":"class PandasMaterializer"},{"location":"materializers.pandas_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 DataFrame Reads pd.Dataframe from a parquet file.","title":"method handle_input"},{"location":"materializers.pandas_materializer/#method-handle_return","text":"handle_return(df: DataFrame) \u2192 None Writes a pandas dataframe to the specified filename. Args: df : The pandas dataframe to write. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"metadata_stores.base_metadata_store/","text":"module metadata_stores.base_metadata_store Global Variables PIPELINE_CONTEXT_TYPE_NAME PIPELINE_RUN_CONTEXT_TYPE_NAME DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY INTERNAL_EXECUTION_PARAMETER_PREFIX PARAM_PIPELINE_PARAMETER_NAME class BaseMetadataStore Metadata store base class to track metadata of zenml first class citizens. method __init__ __init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseMetadataStore instance. Args: repo_path : Path to the repository of this metadata store. property step_type_mapping Maps type_id's to step names. property store General property that hooks into TFX metadata store. method get_pipeline get_pipeline(pipeline_name: str) \u2192 Union[PipelineView, NoneType] Returns a pipeline for the given name. method get_pipeline_run get_pipeline_run( pipeline: PipelineView, run_name: str ) \u2192 Union[PipelineRunView, NoneType] Gets a specific run for the given pipeline. method get_pipeline_run_steps get_pipeline_run_steps(pipeline_run: PipelineRunView) \u2192 Dict[str, StepView] Gets all steps for the given pipeline run. method get_pipeline_runs get_pipeline_runs(pipeline: PipelineView) \u2192 Dict[str, PipelineRunView] Gets all runs for the given pipeline. method get_pipelines get_pipelines() \u2192 List[PipelineView] Returns a list of all pipelines stored in this metadata store. method get_producer_step_from_artifact get_producer_step_from_artifact(artifact: ArtifactView) \u2192 StepView Returns original StepView from an ArtifactView. Args: artifact : ArtifactView to be queried. Returns: Original StepView that produced the artifact. method get_step_artifacts get_step_artifacts( step: StepView ) \u2192 Tuple[Dict[str, ArtifactView], Dict[str, ArtifactView]] Returns input and output artifacts for the given step. Args: step : The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. method get_step_by_id get_step_by_id(step_id: int) \u2192 StepView Gets a StepView by its ID method get_step_status get_step_status(step: StepView) \u2192 <enum 'ExecutionStatus'> Gets the execution status of a single step. method get_tfx_metadata_config get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config. This file was automatically generated via lazydocs .","title":"Metadata stores.base metadata store"},{"location":"metadata_stores.base_metadata_store/#module-metadata_storesbase_metadata_store","text":"","title":"module metadata_stores.base_metadata_store"},{"location":"metadata_stores.base_metadata_store/#global-variables","text":"PIPELINE_CONTEXT_TYPE_NAME PIPELINE_RUN_CONTEXT_TYPE_NAME DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY INTERNAL_EXECUTION_PARAMETER_PREFIX PARAM_PIPELINE_PARAMETER_NAME","title":"Global Variables"},{"location":"metadata_stores.base_metadata_store/#class-basemetadatastore","text":"Metadata store base class to track metadata of zenml first class citizens.","title":"class BaseMetadataStore"},{"location":"metadata_stores.base_metadata_store/#method-__init__","text":"__init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseMetadataStore instance. Args: repo_path : Path to the repository of this metadata store.","title":"method __init__"},{"location":"metadata_stores.base_metadata_store/#property-step_type_mapping","text":"Maps type_id's to step names.","title":"property step_type_mapping"},{"location":"metadata_stores.base_metadata_store/#property-store","text":"General property that hooks into TFX metadata store.","title":"property store"},{"location":"metadata_stores.base_metadata_store/#method-get_pipeline","text":"get_pipeline(pipeline_name: str) \u2192 Union[PipelineView, NoneType] Returns a pipeline for the given name.","title":"method get_pipeline"},{"location":"metadata_stores.base_metadata_store/#method-get_pipeline_run","text":"get_pipeline_run( pipeline: PipelineView, run_name: str ) \u2192 Union[PipelineRunView, NoneType] Gets a specific run for the given pipeline.","title":"method get_pipeline_run"},{"location":"metadata_stores.base_metadata_store/#method-get_pipeline_run_steps","text":"get_pipeline_run_steps(pipeline_run: PipelineRunView) \u2192 Dict[str, StepView] Gets all steps for the given pipeline run.","title":"method get_pipeline_run_steps"},{"location":"metadata_stores.base_metadata_store/#method-get_pipeline_runs","text":"get_pipeline_runs(pipeline: PipelineView) \u2192 Dict[str, PipelineRunView] Gets all runs for the given pipeline.","title":"method get_pipeline_runs"},{"location":"metadata_stores.base_metadata_store/#method-get_pipelines","text":"get_pipelines() \u2192 List[PipelineView] Returns a list of all pipelines stored in this metadata store.","title":"method get_pipelines"},{"location":"metadata_stores.base_metadata_store/#method-get_producer_step_from_artifact","text":"get_producer_step_from_artifact(artifact: ArtifactView) \u2192 StepView Returns original StepView from an ArtifactView. Args: artifact : ArtifactView to be queried. Returns: Original StepView that produced the artifact.","title":"method get_producer_step_from_artifact"},{"location":"metadata_stores.base_metadata_store/#method-get_step_artifacts","text":"get_step_artifacts( step: StepView ) \u2192 Tuple[Dict[str, ArtifactView], Dict[str, ArtifactView]] Returns input and output artifacts for the given step. Args: step : The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively.","title":"method get_step_artifacts"},{"location":"metadata_stores.base_metadata_store/#method-get_step_by_id","text":"get_step_by_id(step_id: int) \u2192 StepView Gets a StepView by its ID","title":"method get_step_by_id"},{"location":"metadata_stores.base_metadata_store/#method-get_step_status","text":"get_step_status(step: StepView) \u2192 <enum 'ExecutionStatus'> Gets the execution status of a single step.","title":"method get_step_status"},{"location":"metadata_stores.base_metadata_store/#method-get_tfx_metadata_config","text":"get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config. This file was automatically generated via lazydocs .","title":"method get_tfx_metadata_config"},{"location":"metadata_stores/","text":"module metadata_stores The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store. This file was automatically generated via lazydocs .","title":"Metadata stores"},{"location":"metadata_stores/#module-metadata_stores","text":"The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store. This file was automatically generated via lazydocs .","title":"module metadata_stores"},{"location":"metadata_stores.mysql_metadata_store/","text":"module metadata_stores.mysql_metadata_store class MySQLMetadataStore MySQL backend for ZenML metadata store. property step_type_mapping Maps type_id's to step names. property store General property that hooks into TFX metadata store. method get_tfx_metadata_config get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config for mysql metadata store. This file was automatically generated via lazydocs .","title":"Metadata stores.mysql metadata store"},{"location":"metadata_stores.mysql_metadata_store/#module-metadata_storesmysql_metadata_store","text":"","title":"module metadata_stores.mysql_metadata_store"},{"location":"metadata_stores.mysql_metadata_store/#class-mysqlmetadatastore","text":"MySQL backend for ZenML metadata store.","title":"class MySQLMetadataStore"},{"location":"metadata_stores.mysql_metadata_store/#property-step_type_mapping","text":"Maps type_id's to step names.","title":"property step_type_mapping"},{"location":"metadata_stores.mysql_metadata_store/#property-store","text":"General property that hooks into TFX metadata store.","title":"property store"},{"location":"metadata_stores.mysql_metadata_store/#method-get_tfx_metadata_config","text":"get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config for mysql metadata store. This file was automatically generated via lazydocs .","title":"method get_tfx_metadata_config"},{"location":"metadata_stores.sqlite_metadata_store/","text":"module metadata_stores.sqlite_metadata_store class SQLiteMetadataStore SQLite backend for ZenML metadata store. method __init__ __init__(**data: Any) Constructor for MySQL MetadataStore for ZenML. property step_type_mapping Maps type_id's to step names. property store General property that hooks into TFX metadata store. method get_tfx_metadata_config get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config for sqlite metadata store. classmethod uri_must_be_local uri_must_be_local(v: str) \u2192 str Validator to ensure uri is local This file was automatically generated via lazydocs .","title":"Metadata stores.sqlite metadata store"},{"location":"metadata_stores.sqlite_metadata_store/#module-metadata_storessqlite_metadata_store","text":"","title":"module metadata_stores.sqlite_metadata_store"},{"location":"metadata_stores.sqlite_metadata_store/#class-sqlitemetadatastore","text":"SQLite backend for ZenML metadata store.","title":"class SQLiteMetadataStore"},{"location":"metadata_stores.sqlite_metadata_store/#method-__init__","text":"__init__(**data: Any) Constructor for MySQL MetadataStore for ZenML.","title":"method __init__"},{"location":"metadata_stores.sqlite_metadata_store/#property-step_type_mapping","text":"Maps type_id's to step names.","title":"property step_type_mapping"},{"location":"metadata_stores.sqlite_metadata_store/#property-store","text":"General property that hooks into TFX metadata store.","title":"property store"},{"location":"metadata_stores.sqlite_metadata_store/#method-get_tfx_metadata_config","text":"get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config for sqlite metadata store.","title":"method get_tfx_metadata_config"},{"location":"metadata_stores.sqlite_metadata_store/#classmethod-uri_must_be_local","text":"uri_must_be_local(v: str) \u2192 str Validator to ensure uri is local This file was automatically generated via lazydocs .","title":"classmethod uri_must_be_local"},{"location":"orchestrators.base_orchestrator/","text":"module orchestrators.base_orchestrator Global Variables TYPE_CHECKING class BaseOrchestrator Base Orchestrator class to orchestrate ZenML pipelines. method __init__ __init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseOrchestrator instance. Args: repo_path : Path to the repository of this orchestrator. property is_running Returns whether the orchestrator is currently running. property log_file Returns path to a log file if available. method down down() \u2192 None Destroys resources for the orchestrator. method post_run post_run() \u2192 None Should be run after the run() to clean up. method pre_run pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Should be run before the run() function to prepare orchestrator. Args: pipeline : Pipeline that will be run. caller_filepath : Path to the file in which pipeline.run() was called. This is necessary for airflow so we know the file in which the DAG is defined. method run run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 Any Abstract method to run a pipeline. Overwrite this in subclasses with a concrete implementation on how to run the given pipeline. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Potential additional parameters used in subclass implementations. method up up() \u2192 None Provisions resources for the orchestrator. This file was automatically generated via lazydocs .","title":"Orchestrators.base orchestrator"},{"location":"orchestrators.base_orchestrator/#module-orchestratorsbase_orchestrator","text":"","title":"module orchestrators.base_orchestrator"},{"location":"orchestrators.base_orchestrator/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"orchestrators.base_orchestrator/#class-baseorchestrator","text":"Base Orchestrator class to orchestrate ZenML pipelines.","title":"class BaseOrchestrator"},{"location":"orchestrators.base_orchestrator/#method-__init__","text":"__init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseOrchestrator instance. Args: repo_path : Path to the repository of this orchestrator.","title":"method __init__"},{"location":"orchestrators.base_orchestrator/#property-is_running","text":"Returns whether the orchestrator is currently running.","title":"property is_running"},{"location":"orchestrators.base_orchestrator/#property-log_file","text":"Returns path to a log file if available.","title":"property log_file"},{"location":"orchestrators.base_orchestrator/#method-down","text":"down() \u2192 None Destroys resources for the orchestrator.","title":"method down"},{"location":"orchestrators.base_orchestrator/#method-post_run","text":"post_run() \u2192 None Should be run after the run() to clean up.","title":"method post_run"},{"location":"orchestrators.base_orchestrator/#method-pre_run","text":"pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Should be run before the run() function to prepare orchestrator. Args: pipeline : Pipeline that will be run. caller_filepath : Path to the file in which pipeline.run() was called. This is necessary for airflow so we know the file in which the DAG is defined.","title":"method pre_run"},{"location":"orchestrators.base_orchestrator/#method-run","text":"run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 Any Abstract method to run a pipeline. Overwrite this in subclasses with a concrete implementation on how to run the given pipeline. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Potential additional parameters used in subclass implementations.","title":"method run"},{"location":"orchestrators.base_orchestrator/#method-up","text":"up() \u2192 None Provisions resources for the orchestrator. This file was automatically generated via lazydocs .","title":"method up"},{"location":"orchestrators.local.local_dag_runner/","text":"module orchestrators.local.local_dag_runner Inspired by local dag runner implementation by Google at: https://github.com/tensorflow/tfx/blob/master/tfx/orchestration /local/local_dag_runner.py Global Variables PIPELINE_RUN_ID_PARAMETER_NAME class LocalDagRunner Local TFX DAG runner. method __init__ __init__() \u2192 None Initializes LocalDagRunner as a TFX orchestrator. method run run(pipeline: Pipeline, run_name: str = '') \u2192 None Runs given logical pipeline locally. Args: pipeline : Logical pipeline containing pipeline args and components. run_name : Name of the pipeline run. This file was automatically generated via lazydocs .","title":"Orchestrators.local.local dag runner"},{"location":"orchestrators.local.local_dag_runner/#module-orchestratorslocallocal_dag_runner","text":"Inspired by local dag runner implementation by Google at: https://github.com/tensorflow/tfx/blob/master/tfx/orchestration /local/local_dag_runner.py","title":"module orchestrators.local.local_dag_runner"},{"location":"orchestrators.local.local_dag_runner/#global-variables","text":"PIPELINE_RUN_ID_PARAMETER_NAME","title":"Global Variables"},{"location":"orchestrators.local.local_dag_runner/#class-localdagrunner","text":"Local TFX DAG runner.","title":"class LocalDagRunner"},{"location":"orchestrators.local.local_dag_runner/#method-__init__","text":"__init__() \u2192 None Initializes LocalDagRunner as a TFX orchestrator.","title":"method __init__"},{"location":"orchestrators.local.local_dag_runner/#method-run","text":"run(pipeline: Pipeline, run_name: str = '') \u2192 None Runs given logical pipeline locally. Args: pipeline : Logical pipeline containing pipeline args and components. run_name : Name of the pipeline run. This file was automatically generated via lazydocs .","title":"method run"},{"location":"orchestrators.local.local_orchestrator/","text":"module orchestrators.local.local_orchestrator Global Variables TYPE_CHECKING class LocalOrchestrator Orchestrator responsible for running pipelines locally. property is_running Returns whether the orchestrator is currently running. property log_file Returns path to a log file if available. method run run(zenml_pipeline: 'BasePipeline', run_name: str, **pipeline_args: Any) \u2192 None Runs a pipeline locally. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **pipeline_args : Unused kwargs to conform with base signature. This file was automatically generated via lazydocs .","title":"Orchestrators.local.local orchestrator"},{"location":"orchestrators.local.local_orchestrator/#module-orchestratorslocallocal_orchestrator","text":"","title":"module orchestrators.local.local_orchestrator"},{"location":"orchestrators.local.local_orchestrator/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"orchestrators.local.local_orchestrator/#class-localorchestrator","text":"Orchestrator responsible for running pipelines locally.","title":"class LocalOrchestrator"},{"location":"orchestrators.local.local_orchestrator/#property-is_running","text":"Returns whether the orchestrator is currently running.","title":"property is_running"},{"location":"orchestrators.local.local_orchestrator/#property-log_file","text":"Returns path to a log file if available.","title":"property log_file"},{"location":"orchestrators.local.local_orchestrator/#method-run","text":"run(zenml_pipeline: 'BasePipeline', run_name: str, **pipeline_args: Any) \u2192 None Runs a pipeline locally. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **pipeline_args : Unused kwargs to conform with base signature. This file was automatically generated via lazydocs .","title":"method run"},{"location":"orchestrators.local/","text":"module orchestrators.local This file was automatically generated via lazydocs .","title":"Orchestrators.local"},{"location":"orchestrators.local/#module-orchestratorslocal","text":"This file was automatically generated via lazydocs .","title":"module orchestrators.local"},{"location":"orchestrators/","text":"module orchestrators An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline. This file was automatically generated via lazydocs .","title":"Orchestrators"},{"location":"orchestrators/#module-orchestrators","text":"An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline. This file was automatically generated via lazydocs .","title":"module orchestrators"},{"location":"orchestrators.utils/","text":"module orchestrators.utils Global Variables TYPE_CHECKING function create_tfx_pipeline create_tfx_pipeline(zenml_pipeline: 'BasePipeline') \u2192 Pipeline Creates a tfx pipeline from a ZenML pipeline. function execute_step execute_step(tfx_launcher: Launcher) \u2192 Union[ExecutionInfo, NoneType] Executes a tfx component. Args: tfx_launcher : A tfx launcher to execute the component. Returns: Optional execution info returned by the launcher. This file was automatically generated via lazydocs .","title":"Orchestrators.utils"},{"location":"orchestrators.utils/#module-orchestratorsutils","text":"","title":"module orchestrators.utils"},{"location":"orchestrators.utils/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"orchestrators.utils/#function-create_tfx_pipeline","text":"create_tfx_pipeline(zenml_pipeline: 'BasePipeline') \u2192 Pipeline Creates a tfx pipeline from a ZenML pipeline.","title":"function create_tfx_pipeline"},{"location":"orchestrators.utils/#function-execute_step","text":"execute_step(tfx_launcher: Launcher) \u2192 Union[ExecutionInfo, NoneType] Executes a tfx component. Args: tfx_launcher : A tfx launcher to execute the component. Returns: Optional execution info returned by the launcher. This file was automatically generated via lazydocs .","title":"function execute_step"},{"location":"pipelines.base_pipeline/","text":"module pipelines.base_pipeline Global Variables ENV_ZENML_PREVENT_PIPELINE_EXECUTION SHOULD_PREVENT_PIPELINE_EXECUTION PIPELINE_INNER_FUNC_NAME PARAM_ENABLE_CACHE PARAM_REQUIREMENTS_FILE PARAM_DOCKERIGNORE_FILE INSTANCE_CONFIGURATION class BasePipelineMeta Pipeline Metaclass responsible for validating the pipeline definition. class BasePipeline Abstract base class for all ZenML pipelines. Attributes: name : The name of this pipeline. enable_cache : A boolean indicating if caching is enabled for this pipeline. requirements_file : Optional path to a pip requirements file that contains all requirements to run the pipeline. method __init__ __init__(*args: BaseStep, **kwargs: Any) \u2192 None property stack Returns the stack for this pipeline. property steps Returns a dictionary of pipeline steps. method connect connect(*args: BaseStep, **kwargs: BaseStep) \u2192 None Function that connects inputs and outputs of the pipeline steps. method run run(run_name: Optional[str] = None) \u2192 Any Runs the pipeline using the orchestrator of the pipeline stack. Args: run_name : Optional name for the run. method with_config with_config( self: ~T, config_file: str, overwrite_step_parameters: bool = False ) \u2192 ~T Configures this pipeline using a yaml file. Args: config_file : Path to a yaml file which contains configuration options for running this pipeline. See https : //docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters : If set to True , values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. This file was automatically generated via lazydocs .","title":"Pipelines.base pipeline"},{"location":"pipelines.base_pipeline/#module-pipelinesbase_pipeline","text":"","title":"module pipelines.base_pipeline"},{"location":"pipelines.base_pipeline/#global-variables","text":"ENV_ZENML_PREVENT_PIPELINE_EXECUTION SHOULD_PREVENT_PIPELINE_EXECUTION PIPELINE_INNER_FUNC_NAME PARAM_ENABLE_CACHE PARAM_REQUIREMENTS_FILE PARAM_DOCKERIGNORE_FILE INSTANCE_CONFIGURATION","title":"Global Variables"},{"location":"pipelines.base_pipeline/#class-basepipelinemeta","text":"Pipeline Metaclass responsible for validating the pipeline definition.","title":"class BasePipelineMeta"},{"location":"pipelines.base_pipeline/#class-basepipeline","text":"Abstract base class for all ZenML pipelines. Attributes: name : The name of this pipeline. enable_cache : A boolean indicating if caching is enabled for this pipeline. requirements_file : Optional path to a pip requirements file that contains all requirements to run the pipeline.","title":"class BasePipeline"},{"location":"pipelines.base_pipeline/#method-__init__","text":"__init__(*args: BaseStep, **kwargs: Any) \u2192 None","title":"method __init__"},{"location":"pipelines.base_pipeline/#property-stack","text":"Returns the stack for this pipeline.","title":"property stack"},{"location":"pipelines.base_pipeline/#property-steps","text":"Returns a dictionary of pipeline steps.","title":"property steps"},{"location":"pipelines.base_pipeline/#method-connect","text":"connect(*args: BaseStep, **kwargs: BaseStep) \u2192 None Function that connects inputs and outputs of the pipeline steps.","title":"method connect"},{"location":"pipelines.base_pipeline/#method-run","text":"run(run_name: Optional[str] = None) \u2192 Any Runs the pipeline using the orchestrator of the pipeline stack. Args: run_name : Optional name for the run.","title":"method run"},{"location":"pipelines.base_pipeline/#method-with_config","text":"with_config( self: ~T, config_file: str, overwrite_step_parameters: bool = False ) \u2192 ~T Configures this pipeline using a yaml file. Args: config_file : Path to a yaml file which contains configuration options for running this pipeline. See https : //docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters : If set to True , values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. This file was automatically generated via lazydocs .","title":"method with_config"},{"location":"pipelines.builtin_pipelines/","text":"module pipelines.builtin_pipelines This file was automatically generated via lazydocs .","title":"Pipelines.builtin pipelines"},{"location":"pipelines.builtin_pipelines/#module-pipelinesbuiltin_pipelines","text":"This file was automatically generated via lazydocs .","title":"module pipelines.builtin_pipelines"},{"location":"pipelines.builtin_pipelines.training_pipeline/","text":"module pipelines.builtin_pipelines.training_pipeline class TrainingPipeline Class for the classic training pipeline implementation property stack Returns the stack for this pipeline. property steps Returns a dictionary of pipeline steps. method connect connect( datasource: BaseDatasourceStep, splitter: BaseSplitStep, analyzer: BaseAnalyzerStep, preprocessor: BasePreprocessorStep, trainer: BaseTrainerStep, evaluator: BaseEvaluatorStep ) \u2192 None Main connect method for the standard training pipelines Args: datasource : the step responsible for the data ingestion splitter : the step responsible for splitting the dataset into train, test, val analyzer : the step responsible for extracting the statistics and the schema preprocessor : the step responsible for preprocessing the data trainer : the step responsible for training a model evaluator : the step responsible for computing the evaluation of the trained model This file was automatically generated via lazydocs .","title":"Pipelines.builtin pipelines.training pipeline"},{"location":"pipelines.builtin_pipelines.training_pipeline/#module-pipelinesbuiltin_pipelinestraining_pipeline","text":"","title":"module pipelines.builtin_pipelines.training_pipeline"},{"location":"pipelines.builtin_pipelines.training_pipeline/#class-trainingpipeline","text":"Class for the classic training pipeline implementation","title":"class TrainingPipeline"},{"location":"pipelines.builtin_pipelines.training_pipeline/#property-stack","text":"Returns the stack for this pipeline.","title":"property stack"},{"location":"pipelines.builtin_pipelines.training_pipeline/#property-steps","text":"Returns a dictionary of pipeline steps.","title":"property steps"},{"location":"pipelines.builtin_pipelines.training_pipeline/#method-connect","text":"connect( datasource: BaseDatasourceStep, splitter: BaseSplitStep, analyzer: BaseAnalyzerStep, preprocessor: BasePreprocessorStep, trainer: BaseTrainerStep, evaluator: BaseEvaluatorStep ) \u2192 None Main connect method for the standard training pipelines Args: datasource : the step responsible for the data ingestion splitter : the step responsible for splitting the dataset into train, test, val analyzer : the step responsible for extracting the statistics and the schema preprocessor : the step responsible for preprocessing the data trainer : the step responsible for training a model evaluator : the step responsible for computing the evaluation of the trained model This file was automatically generated via lazydocs .","title":"method connect"},{"location":"pipelines/","text":"module pipelines A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator. This file was automatically generated via lazydocs .","title":"Pipelines"},{"location":"pipelines/#module-pipelines","text":"A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator. This file was automatically generated via lazydocs .","title":"module pipelines"},{"location":"pipelines.pipeline_decorator/","text":"module pipelines.pipeline_decorator Global Variables INSTANCE_CONFIGURATION PARAM_DOCKERIGNORE_FILE PARAM_ENABLE_CACHE PARAM_REQUIREMENTS_FILE PIPELINE_INNER_FUNC_NAME function pipeline pipeline( _func: Optional[~F] = None, name: Optional[str] = None, enable_cache: bool = True, requirements_file: Optional[str] = None, dockerignore_file: Optional[str] = None ) \u2192 Union[Type[BasePipeline], Callable[[~F], Type[BasePipeline]]] Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Args: _func : The decorated function. name : The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. enable_cache : Whether to use caching or not. requirements_file : Optional path to a pip requirements file that contains requirements to run the pipeline. dockerignore_file : Optional path to a dockerignore file to use when building docker images for running this pipeline. **Note** : If you pass a file, make sure it does not include the .zen directory as it is needed to run ZenML inside the container. Returns: the inner decorator which creates the pipeline class based on the ZenML BasePipeline This file was automatically generated via lazydocs .","title":"Pipelines.pipeline decorator"},{"location":"pipelines.pipeline_decorator/#module-pipelinespipeline_decorator","text":"","title":"module pipelines.pipeline_decorator"},{"location":"pipelines.pipeline_decorator/#global-variables","text":"INSTANCE_CONFIGURATION PARAM_DOCKERIGNORE_FILE PARAM_ENABLE_CACHE PARAM_REQUIREMENTS_FILE PIPELINE_INNER_FUNC_NAME","title":"Global Variables"},{"location":"pipelines.pipeline_decorator/#function-pipeline","text":"pipeline( _func: Optional[~F] = None, name: Optional[str] = None, enable_cache: bool = True, requirements_file: Optional[str] = None, dockerignore_file: Optional[str] = None ) \u2192 Union[Type[BasePipeline], Callable[[~F], Type[BasePipeline]]] Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Args: _func : The decorated function. name : The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. enable_cache : Whether to use caching or not. requirements_file : Optional path to a pip requirements file that contains requirements to run the pipeline. dockerignore_file : Optional path to a dockerignore file to use when building docker images for running this pipeline. **Note** : If you pass a file, make sure it does not include the .zen directory as it is needed to run ZenML inside the container. Returns: the inner decorator which creates the pipeline class based on the ZenML BasePipeline This file was automatically generated via lazydocs .","title":"function pipeline"},{"location":"post_execution.artifact/","text":"module post_execution.artifact Global Variables TYPE_CHECKING class ArtifactView Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution. method __init__ __init__( id_: int, type_: str, uri: str, materializer: str, data_type: str, metadata_store: 'BaseMetadataStore', parent_step_id: int ) Initializes a post-execution artifact object. In most cases ArtifactView objects should not be created manually but retrieved from a StepView via the inputs or outputs properties. Args: id_ : The artifact id. type_ : The type of this artifact. uri : Specifies where the artifact data is stored. materializer : Information needed to restore the materializer that was used to write this artifact. data_type : The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id : The ID of the parent step. property data_type Returns the data type of the artifact. property id Returns the artifact id. property is_cached Returns True if artifact was cached in a previous run, else False. property parent_step_id Returns the ID of the parent step. This need not be equivalent to the ID of the producer step. property producer_step Returns the original StepView that produced the artifact. property type Returns the artifact type. property uri Returns the URI where the artifact data is stored. method read read( output_data_type: Optional[Type[Any]] = None, materializer_class: Optional[Type[ForwardRef('BaseMaterializer')]] = None ) \u2192 Any Materializes the data stored in this artifact. Args: output_data_type : The datatype to which the materializer should read, will be passed to the materializers handle_input method. materializer_class : The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. This file was automatically generated via lazydocs .","title":"Post execution.artifact"},{"location":"post_execution.artifact/#module-post_executionartifact","text":"","title":"module post_execution.artifact"},{"location":"post_execution.artifact/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"post_execution.artifact/#class-artifactview","text":"Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution.","title":"class ArtifactView"},{"location":"post_execution.artifact/#method-__init__","text":"__init__( id_: int, type_: str, uri: str, materializer: str, data_type: str, metadata_store: 'BaseMetadataStore', parent_step_id: int ) Initializes a post-execution artifact object. In most cases ArtifactView objects should not be created manually but retrieved from a StepView via the inputs or outputs properties. Args: id_ : The artifact id. type_ : The type of this artifact. uri : Specifies where the artifact data is stored. materializer : Information needed to restore the materializer that was used to write this artifact. data_type : The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id : The ID of the parent step.","title":"method __init__"},{"location":"post_execution.artifact/#property-data_type","text":"Returns the data type of the artifact.","title":"property data_type"},{"location":"post_execution.artifact/#property-id","text":"Returns the artifact id.","title":"property id"},{"location":"post_execution.artifact/#property-is_cached","text":"Returns True if artifact was cached in a previous run, else False.","title":"property is_cached"},{"location":"post_execution.artifact/#property-parent_step_id","text":"Returns the ID of the parent step. This need not be equivalent to the ID of the producer step.","title":"property parent_step_id"},{"location":"post_execution.artifact/#property-producer_step","text":"Returns the original StepView that produced the artifact.","title":"property producer_step"},{"location":"post_execution.artifact/#property-type","text":"Returns the artifact type.","title":"property type"},{"location":"post_execution.artifact/#property-uri","text":"Returns the URI where the artifact data is stored.","title":"property uri"},{"location":"post_execution.artifact/#method-read","text":"read( output_data_type: Optional[Type[Any]] = None, materializer_class: Optional[Type[ForwardRef('BaseMaterializer')]] = None ) \u2192 Any Materializes the data stored in this artifact. Args: output_data_type : The datatype to which the materializer should read, will be passed to the materializers handle_input method. materializer_class : The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. This file was automatically generated via lazydocs .","title":"method read"},{"location":"post_execution/","text":"module post_execution After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object. This file was automatically generated via lazydocs .","title":"Post execution"},{"location":"post_execution/#module-post_execution","text":"After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object. This file was automatically generated via lazydocs .","title":"module post_execution"},{"location":"post_execution.pipeline/","text":"module post_execution.pipeline Global Variables TYPE_CHECKING class PipelineView Post-execution pipeline class which can be used to query pipeline-related information from the metadata store. method __init__ __init__(id_: int, name: str, metadata_store: 'BaseMetadataStore') Initializes a post-execution pipeline object. In most cases PipelineView objects should not be created manually but retrieved using the get_pipelines() method of a zenml.core.repo.Repository instead. Args: id_ : The context id of this pipeline. name : The name of this pipeline. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline. property name Returns the name of the pipeline. property runs Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list. method get_run get_run(name: str) \u2192 PipelineRunView Returns a run for the given name. Args: name : The name of the run to return. Raises: KeyError : If there is no run with the given name. method get_run_names get_run_names() \u2192 List[str] Returns a list of all run names. This file was automatically generated via lazydocs .","title":"Post execution.pipeline"},{"location":"post_execution.pipeline/#module-post_executionpipeline","text":"","title":"module post_execution.pipeline"},{"location":"post_execution.pipeline/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"post_execution.pipeline/#class-pipelineview","text":"Post-execution pipeline class which can be used to query pipeline-related information from the metadata store.","title":"class PipelineView"},{"location":"post_execution.pipeline/#method-__init__","text":"__init__(id_: int, name: str, metadata_store: 'BaseMetadataStore') Initializes a post-execution pipeline object. In most cases PipelineView objects should not be created manually but retrieved using the get_pipelines() method of a zenml.core.repo.Repository instead. Args: id_ : The context id of this pipeline. name : The name of this pipeline. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline.","title":"method __init__"},{"location":"post_execution.pipeline/#property-name","text":"Returns the name of the pipeline.","title":"property name"},{"location":"post_execution.pipeline/#property-runs","text":"Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list.","title":"property runs"},{"location":"post_execution.pipeline/#method-get_run","text":"get_run(name: str) \u2192 PipelineRunView Returns a run for the given name. Args: name : The name of the run to return. Raises: KeyError : If there is no run with the given name.","title":"method get_run"},{"location":"post_execution.pipeline/#method-get_run_names","text":"get_run_names() \u2192 List[str] Returns a list of all run names. This file was automatically generated via lazydocs .","title":"method get_run_names"},{"location":"post_execution.pipeline_run/","text":"module post_execution.pipeline_run Global Variables TYPE_CHECKING class PipelineRunView Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution. method __init__ __init__( id_: int, name: str, executions: List[Execution], metadata_store: 'BaseMetadataStore' ) Initializes a post-execution pipeline run object. In most cases PipelineRunView objects should not be created manually but retrieved from a PipelineView object instead. Args: id_ : The context id of this pipeline run. name : The name of this pipeline run. executions : All executions associated with this pipeline run. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline run. property name Returns the name of the pipeline run. property status Returns the current status of the pipeline run. property steps Returns all steps that were executed as part of this pipeline run. method get_step get_step(name: str) \u2192 StepView Returns a step for the given name. Args: name : The name of the step to return. Raises: KeyError : If there is no step with the given name. method get_step_names get_step_names() \u2192 List[str] Returns a list of all step names. This file was automatically generated via lazydocs .","title":"Post execution.pipeline run"},{"location":"post_execution.pipeline_run/#module-post_executionpipeline_run","text":"","title":"module post_execution.pipeline_run"},{"location":"post_execution.pipeline_run/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"post_execution.pipeline_run/#class-pipelinerunview","text":"Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution.","title":"class PipelineRunView"},{"location":"post_execution.pipeline_run/#method-__init__","text":"__init__( id_: int, name: str, executions: List[Execution], metadata_store: 'BaseMetadataStore' ) Initializes a post-execution pipeline run object. In most cases PipelineRunView objects should not be created manually but retrieved from a PipelineView object instead. Args: id_ : The context id of this pipeline run. name : The name of this pipeline run. executions : All executions associated with this pipeline run. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline run.","title":"method __init__"},{"location":"post_execution.pipeline_run/#property-name","text":"Returns the name of the pipeline run.","title":"property name"},{"location":"post_execution.pipeline_run/#property-status","text":"Returns the current status of the pipeline run.","title":"property status"},{"location":"post_execution.pipeline_run/#property-steps","text":"Returns all steps that were executed as part of this pipeline run.","title":"property steps"},{"location":"post_execution.pipeline_run/#method-get_step","text":"get_step(name: str) \u2192 StepView Returns a step for the given name. Args: name : The name of the step to return. Raises: KeyError : If there is no step with the given name.","title":"method get_step"},{"location":"post_execution.pipeline_run/#method-get_step_names","text":"get_step_names() \u2192 List[str] Returns a list of all step names. This file was automatically generated via lazydocs .","title":"method get_step_names"},{"location":"post_execution.step/","text":"module post_execution.step Global Variables TYPE_CHECKING class StepView Post-execution step class which can be used to query artifact information associated with a pipeline step. method __init__ __init__( id_: int, parents_step_ids: List[int], name: str, pipeline_step_name: str, parameters: Dict[str, Any], metadata_store: 'BaseMetadataStore' ) Initializes a post-execution step object. In most cases StepView objects should not be created manually but retrieved from a PipelineRunView object instead. Args: id_ : The execution id of this step. parents_step_ids : The execution ids of the parents of this step. name : The name of this step. pipeline_step_name : The name of this step within the pipeline parameters : Parameters that were used to run this step. metadata_store : The metadata store which should be used to fetch additional information related to this step. property id Returns the step id. property input Returns the input artifact that was used to run this step. Raises: ValueError : If there were zero or multiple inputs to this step. property inputs Returns all input artifacts that were used to run this step. property is_cached Returns whether the step is cached or not. property name Returns the step name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples: # the step name will be \"my_step\" @step(name=\"my_step\") def my_step_function(...) # the step name will be \"my_step_function\" @step def my_step_function(...) property output Returns the output artifact that was written by this step. Raises: ValueError : If there were zero or multiple step outputs. property outputs Returns all output artifacts that were written by this step. property parameters The parameters used to run this step. property parent_steps Returns a list of all parent steps of this step. property parents_step_ids Returns a list of ID's of all parents of this step. property pipeline_step_name Returns the pipeline step name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The pipeline step name will be step_a property status Returns the current status of the step. This file was automatically generated via lazydocs .","title":"Post execution.step"},{"location":"post_execution.step/#module-post_executionstep","text":"","title":"module post_execution.step"},{"location":"post_execution.step/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"post_execution.step/#class-stepview","text":"Post-execution step class which can be used to query artifact information associated with a pipeline step.","title":"class StepView"},{"location":"post_execution.step/#method-__init__","text":"__init__( id_: int, parents_step_ids: List[int], name: str, pipeline_step_name: str, parameters: Dict[str, Any], metadata_store: 'BaseMetadataStore' ) Initializes a post-execution step object. In most cases StepView objects should not be created manually but retrieved from a PipelineRunView object instead. Args: id_ : The execution id of this step. parents_step_ids : The execution ids of the parents of this step. name : The name of this step. pipeline_step_name : The name of this step within the pipeline parameters : Parameters that were used to run this step. metadata_store : The metadata store which should be used to fetch additional information related to this step.","title":"method __init__"},{"location":"post_execution.step/#property-id","text":"Returns the step id.","title":"property id"},{"location":"post_execution.step/#property-input","text":"Returns the input artifact that was used to run this step. Raises: ValueError : If there were zero or multiple inputs to this step.","title":"property input"},{"location":"post_execution.step/#property-inputs","text":"Returns all input artifacts that were used to run this step.","title":"property inputs"},{"location":"post_execution.step/#property-is_cached","text":"Returns whether the step is cached or not.","title":"property is_cached"},{"location":"post_execution.step/#property-name","text":"Returns the step name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples: # the step name will be \"my_step\" @step(name=\"my_step\") def my_step_function(...) # the step name will be \"my_step_function\" @step def my_step_function(...)","title":"property name"},{"location":"post_execution.step/#property-output","text":"Returns the output artifact that was written by this step. Raises: ValueError : If there were zero or multiple step outputs.","title":"property output"},{"location":"post_execution.step/#property-outputs","text":"Returns all output artifacts that were written by this step.","title":"property outputs"},{"location":"post_execution.step/#property-parameters","text":"The parameters used to run this step.","title":"property parameters"},{"location":"post_execution.step/#property-parent_steps","text":"Returns a list of all parent steps of this step.","title":"property parent_steps"},{"location":"post_execution.step/#property-parents_step_ids","text":"Returns a list of ID's of all parents of this step.","title":"property parents_step_ids"},{"location":"post_execution.step/#property-pipeline_step_name","text":"Returns the pipeline step name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The pipeline step name will be step_a","title":"property pipeline_step_name"},{"location":"post_execution.step/#property-status","text":"Returns the current status of the step. This file was automatically generated via lazydocs .","title":"property status"},{"location":"stacks.base_stack/","text":"module stacks.base_stack Global Variables TYPE_CHECKING class BaseStack Base stack for ZenML. A ZenML stack brings together a Metadata Store, an Artifact Store, and an Orchestrator, the trifecta of the environment required to run a ZenML pipeline. A ZenML stack also happens to be a pydantic BaseSettings class, which means that there are multiple ways to use it. You can set it via env variables. * You can set it through the config yaml file. * You can set it in code by initializing an object of this class, and passing it to pipelines as a configuration. In the case where a value is specified for the same Settings field in multiple ways, the selected value is determined as follows (in descending order of priority): Arguments passed to the Settings class initializer. * Environment variables, e.g. zenml_var as described above. * Variables loaded from a config yaml file. * The default field values. property artifact_store Returns the artifact store of this stack. property container_registry Returns the optional container registry of this stack. property metadata_store Returns the metadata store of this stack. property orchestrator Returns the orchestrator of this stack. method dict dict(**kwargs: Any) \u2192 Dict[str, Any] Removes private attributes from pydantic dict so they don't get stored in our config files. This file was automatically generated via lazydocs .","title":"Stacks.base stack"},{"location":"stacks.base_stack/#module-stacksbase_stack","text":"","title":"module stacks.base_stack"},{"location":"stacks.base_stack/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"stacks.base_stack/#class-basestack","text":"Base stack for ZenML. A ZenML stack brings together a Metadata Store, an Artifact Store, and an Orchestrator, the trifecta of the environment required to run a ZenML pipeline. A ZenML stack also happens to be a pydantic BaseSettings class, which means that there are multiple ways to use it. You can set it via env variables. * You can set it through the config yaml file. * You can set it in code by initializing an object of this class, and passing it to pipelines as a configuration. In the case where a value is specified for the same Settings field in multiple ways, the selected value is determined as follows (in descending order of priority): Arguments passed to the Settings class initializer. * Environment variables, e.g. zenml_var as described above. * Variables loaded from a config yaml file. * The default field values.","title":"class BaseStack"},{"location":"stacks.base_stack/#property-artifact_store","text":"Returns the artifact store of this stack.","title":"property artifact_store"},{"location":"stacks.base_stack/#property-container_registry","text":"Returns the optional container registry of this stack.","title":"property container_registry"},{"location":"stacks.base_stack/#property-metadata_store","text":"Returns the metadata store of this stack.","title":"property metadata_store"},{"location":"stacks.base_stack/#property-orchestrator","text":"Returns the orchestrator of this stack.","title":"property orchestrator"},{"location":"stacks.base_stack/#method-dict","text":"dict(**kwargs: Any) \u2192 Dict[str, Any] Removes private attributes from pydantic dict so they don't get stored in our config files. This file was automatically generated via lazydocs .","title":"method dict"},{"location":"stacks.constants/","text":"module stacks.constants Global Variables DEFAULT_STACK_KEY This file was automatically generated via lazydocs .","title":"Stacks.constants"},{"location":"stacks.constants/#module-stacksconstants","text":"","title":"module stacks.constants"},{"location":"stacks.constants/#global-variables","text":"DEFAULT_STACK_KEY This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"stacks/","text":"module stacks A stack is made up of the following three core components: an Artifact Store, a Metadata Store, and an Orchestrator (backend). A ZenML stack also happens to be a Pydantic BaseSettings class, which means that there are multiple ways to use it. This file was automatically generated via lazydocs .","title":"Stacks"},{"location":"stacks/#module-stacks","text":"A stack is made up of the following three core components: an Artifact Store, a Metadata Store, and an Orchestrator (backend). A ZenML stack also happens to be a Pydantic BaseSettings class, which means that there are multiple ways to use it. This file was automatically generated via lazydocs .","title":"module stacks"},{"location":"steps.base_step/","text":"module steps.base_step Global Variables INSTANCE_CONFIGURATION INTERNAL_EXECUTION_PARAMETER_PREFIX PARAM_CREATED_BY_FUNCTIONAL_API PARAM_ENABLE_CACHE PARAM_PIPELINE_PARAMETER_NAME SINGLE_RETURN_OUT_NAME STEP_INNER_FUNC_NAME class BaseStepMeta Metaclass for BaseStep . Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class class BaseStep Abstract base class for all ZenML steps. Attributes: name : The name of this step. pipeline_parameter_name : The name of the pipeline parameter for which this step was passed as an argument. enable_cache : A boolean indicating if caching is enabled for this step. requires_context : A boolean indicating if this step requires a StepContext object during execution. method __init__ __init__(*args: Any, **kwargs: Any) \u2192 None property component Returns a TFX component. method entrypoint entrypoint(*args: Any, **kwargs: Any) \u2192 Any Abstract method for core step logic. method get_materializers get_materializers( ensure_complete: bool = False ) \u2192 Dict[str, Type[BaseMaterializer]] Returns available materializers for the outputs of this step. Args: ensure_complete : If set to True , this method will raise a StepInterfaceError if no materializer can be found for an output. Returns: A dictionary mapping output names to BaseMaterializer subclasses. If no explicit materializer was set using step.with_return_materializers(...) , this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError : (Only if ensure_complete is set to True ) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. method with_return_materializers with_return_materializers( self: ~T, materializers: Union[Type[BaseMaterializer], Dict[str, Type[BaseMaterializer]]] ) \u2192 ~T Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers : The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError : If a materializer is not a BaseMaterializer subclass or a materializer for a non-existent output is given. This file was automatically generated via lazydocs .","title":"Steps.base step"},{"location":"steps.base_step/#module-stepsbase_step","text":"","title":"module steps.base_step"},{"location":"steps.base_step/#global-variables","text":"INSTANCE_CONFIGURATION INTERNAL_EXECUTION_PARAMETER_PREFIX PARAM_CREATED_BY_FUNCTIONAL_API PARAM_ENABLE_CACHE PARAM_PIPELINE_PARAMETER_NAME SINGLE_RETURN_OUT_NAME STEP_INNER_FUNC_NAME","title":"Global Variables"},{"location":"steps.base_step/#class-basestepmeta","text":"Metaclass for BaseStep . Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class","title":"class BaseStepMeta"},{"location":"steps.base_step/#class-basestep","text":"Abstract base class for all ZenML steps. Attributes: name : The name of this step. pipeline_parameter_name : The name of the pipeline parameter for which this step was passed as an argument. enable_cache : A boolean indicating if caching is enabled for this step. requires_context : A boolean indicating if this step requires a StepContext object during execution.","title":"class BaseStep"},{"location":"steps.base_step/#method-__init__","text":"__init__(*args: Any, **kwargs: Any) \u2192 None","title":"method __init__"},{"location":"steps.base_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"steps.base_step/#method-entrypoint","text":"entrypoint(*args: Any, **kwargs: Any) \u2192 Any Abstract method for core step logic.","title":"method entrypoint"},{"location":"steps.base_step/#method-get_materializers","text":"get_materializers( ensure_complete: bool = False ) \u2192 Dict[str, Type[BaseMaterializer]] Returns available materializers for the outputs of this step. Args: ensure_complete : If set to True , this method will raise a StepInterfaceError if no materializer can be found for an output. Returns: A dictionary mapping output names to BaseMaterializer subclasses. If no explicit materializer was set using step.with_return_materializers(...) , this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError : (Only if ensure_complete is set to True ) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type.","title":"method get_materializers"},{"location":"steps.base_step/#method-with_return_materializers","text":"with_return_materializers( self: ~T, materializers: Union[Type[BaseMaterializer], Dict[str, Type[BaseMaterializer]]] ) \u2192 ~T Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers : The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError : If a materializer is not a BaseMaterializer subclass or a materializer for a non-existent output is given. This file was automatically generated via lazydocs .","title":"method with_return_materializers"},{"location":"steps.base_step_config/","text":"module steps.base_step_config class BaseStepConfig Base configuration class to pass execution params into a step. This file was automatically generated via lazydocs .","title":"Steps.base step config"},{"location":"steps.base_step_config/#module-stepsbase_step_config","text":"","title":"module steps.base_step_config"},{"location":"steps.base_step_config/#class-basestepconfig","text":"Base configuration class to pass execution params into a step. This file was automatically generated via lazydocs .","title":"class BaseStepConfig"},{"location":"steps.builtin_steps/","text":"module steps.builtin_steps This file was automatically generated via lazydocs .","title":"Steps.builtin steps"},{"location":"steps.builtin_steps/#module-stepsbuiltin_steps","text":"This file was automatically generated via lazydocs .","title":"module steps.builtin_steps"},{"location":"steps.builtin_steps.pandas_analyzer/","text":"module steps.builtin_steps.pandas_analyzer class PandasAnalyzerConfig Config class for the PandasAnalyzer Config class PandasAnalyzer Simple step implementation which analyzes a given pd.DataFrame property component Returns a TFX component. method entrypoint entrypoint( dataset: DataFrame, config: PandasAnalyzerConfig ) \u2192 <Output object at 0x7fe5c7e79a30> Main entrypoint function for the pandas analyzer Args: dataset : pd.DataFrame, the given dataset config : the configuration of the step Returns: the statistics and the schema of the given dataframe This file was automatically generated via lazydocs .","title":"Steps.builtin steps.pandas analyzer"},{"location":"steps.builtin_steps.pandas_analyzer/#module-stepsbuiltin_stepspandas_analyzer","text":"","title":"module steps.builtin_steps.pandas_analyzer"},{"location":"steps.builtin_steps.pandas_analyzer/#class-pandasanalyzerconfig","text":"Config class for the PandasAnalyzer Config","title":"class PandasAnalyzerConfig"},{"location":"steps.builtin_steps.pandas_analyzer/#class-pandasanalyzer","text":"Simple step implementation which analyzes a given pd.DataFrame","title":"class PandasAnalyzer"},{"location":"steps.builtin_steps.pandas_analyzer/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"steps.builtin_steps.pandas_analyzer/#method-entrypoint","text":"entrypoint( dataset: DataFrame, config: PandasAnalyzerConfig ) \u2192 <Output object at 0x7fe5c7e79a30> Main entrypoint function for the pandas analyzer Args: dataset : pd.DataFrame, the given dataset config : the configuration of the step Returns: the statistics and the schema of the given dataframe This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"steps.builtin_steps.pandas_datasource/","text":"module steps.builtin_steps.pandas_datasource class PandasDatasourceConfig Config class for the pandas csv datasource class PandasDatasource Simple step implementation to ingest from a csv file using pandas property component Returns a TFX component. method entrypoint entrypoint(config: PandasDatasourceConfig) \u2192 DataFrame Main entrypoint method for the PandasDatasource Args: config : the configuration of the step Returns: the resulting dataframe This file was automatically generated via lazydocs .","title":"Steps.builtin steps.pandas datasource"},{"location":"steps.builtin_steps.pandas_datasource/#module-stepsbuiltin_stepspandas_datasource","text":"","title":"module steps.builtin_steps.pandas_datasource"},{"location":"steps.builtin_steps.pandas_datasource/#class-pandasdatasourceconfig","text":"Config class for the pandas csv datasource","title":"class PandasDatasourceConfig"},{"location":"steps.builtin_steps.pandas_datasource/#class-pandasdatasource","text":"Simple step implementation to ingest from a csv file using pandas","title":"class PandasDatasource"},{"location":"steps.builtin_steps.pandas_datasource/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"steps.builtin_steps.pandas_datasource/#method-entrypoint","text":"entrypoint(config: PandasDatasourceConfig) \u2192 DataFrame Main entrypoint method for the PandasDatasource Args: config : the configuration of the step Returns: the resulting dataframe This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"steps/","text":"module steps A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator. This file was automatically generated via lazydocs .","title":"Steps"},{"location":"steps/#module-steps","text":"A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator. This file was automatically generated via lazydocs .","title":"module steps"},{"location":"steps.step_context/","text":"module steps.step_context Global Variables TYPE_CHECKING class StepContextOutput Tuple containing materializer class and artifact for a step output. class StepContext Provides additional context inside a step function. This class is used to access materializers and artifact URIs inside a step function. To use it, add a StepContext object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a StepContext object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the StepContext and automatically pass it when executing your step. Note : When using a StepContext inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the @step decorator or when initializing your custom step class. method __init__ __init__( step_name: str, output_materializers: Dict[str, Type[ForwardRef('BaseMaterializer')]], output_artifacts: Dict[str, ForwardRef('BaseArtifact')] ) Initializes a StepContext instance. Args: step_name : The name of the step that this context is used in. output_materializers : The output materializers of the step that this context is used in. output_artifacts : The output artifacts of the step that this context is used in. Raises: StepInterfaceError : If the keys of the output materializers and output artifacts do not match. method get_output_artifact_uri get_output_artifact_uri(output_name: Optional[str] = None) \u2192 str Returns the artifact URI for a given step output. Args: output_name : Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepInterfaceError : If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. method get_output_materializer get_output_materializer( output_name: Optional[str] = None, custom_materializer_class: Optional[Type[ForwardRef('BaseMaterializer')]] = None ) \u2192 BaseMaterializer Returns a materializer for a given step output. Args: output_name : Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class : If given, this BaseMaterializer subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepInterfaceError : If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. This file was automatically generated via lazydocs .","title":"Steps.step context"},{"location":"steps.step_context/#module-stepsstep_context","text":"","title":"module steps.step_context"},{"location":"steps.step_context/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"steps.step_context/#class-stepcontextoutput","text":"Tuple containing materializer class and artifact for a step output.","title":"class StepContextOutput"},{"location":"steps.step_context/#class-stepcontext","text":"Provides additional context inside a step function. This class is used to access materializers and artifact URIs inside a step function. To use it, add a StepContext object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a StepContext object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the StepContext and automatically pass it when executing your step. Note : When using a StepContext inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the @step decorator or when initializing your custom step class.","title":"class StepContext"},{"location":"steps.step_context/#method-__init__","text":"__init__( step_name: str, output_materializers: Dict[str, Type[ForwardRef('BaseMaterializer')]], output_artifacts: Dict[str, ForwardRef('BaseArtifact')] ) Initializes a StepContext instance. Args: step_name : The name of the step that this context is used in. output_materializers : The output materializers of the step that this context is used in. output_artifacts : The output artifacts of the step that this context is used in. Raises: StepInterfaceError : If the keys of the output materializers and output artifacts do not match.","title":"method __init__"},{"location":"steps.step_context/#method-get_output_artifact_uri","text":"get_output_artifact_uri(output_name: Optional[str] = None) \u2192 str Returns the artifact URI for a given step output. Args: output_name : Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepInterfaceError : If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs.","title":"method get_output_artifact_uri"},{"location":"steps.step_context/#method-get_output_materializer","text":"get_output_materializer( output_name: Optional[str] = None, custom_materializer_class: Optional[Type[ForwardRef('BaseMaterializer')]] = None ) \u2192 BaseMaterializer Returns a materializer for a given step output. Args: output_name : Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class : If given, this BaseMaterializer subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepInterfaceError : If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. This file was automatically generated via lazydocs .","title":"method get_output_materializer"},{"location":"steps.step_decorator/","text":"module steps.step_decorator Global Variables TYPE_CHECKING INSTANCE_CONFIGURATION OUTPUT_SPEC PARAM_CREATED_BY_FUNCTIONAL_API PARAM_ENABLE_CACHE STEP_INNER_FUNC_NAME function step step( _func: Optional[~F] = None, name: Optional[str] = None, enable_cache: Optional[bool] = None, output_types: Optional[Dict[str, Type[ForwardRef('BaseArtifact')]]] = None ) \u2192 Union[Type[BaseStep], Callable[[~F], Type[BaseStep]]] Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as name , it features a nested decorator structure. Args: _func : The decorated function. name : The name of the step. If left empty, the name of the decorated function will be used as a fallback. enable_cache : Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a StepContext (see :class : zenml.steps.step_context.StepContext for more information). output_types : A dictionary which sets different outputs to non-default artifact types Returns: the inner decorator which creates the step class based on the ZenML BaseStep This file was automatically generated via lazydocs .","title":"Steps.step decorator"},{"location":"steps.step_decorator/#module-stepsstep_decorator","text":"","title":"module steps.step_decorator"},{"location":"steps.step_decorator/#global-variables","text":"TYPE_CHECKING INSTANCE_CONFIGURATION OUTPUT_SPEC PARAM_CREATED_BY_FUNCTIONAL_API PARAM_ENABLE_CACHE STEP_INNER_FUNC_NAME","title":"Global Variables"},{"location":"steps.step_decorator/#function-step","text":"step( _func: Optional[~F] = None, name: Optional[str] = None, enable_cache: Optional[bool] = None, output_types: Optional[Dict[str, Type[ForwardRef('BaseArtifact')]]] = None ) \u2192 Union[Type[BaseStep], Callable[[~F], Type[BaseStep]]] Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as name , it features a nested decorator structure. Args: _func : The decorated function. name : The name of the step. If left empty, the name of the decorated function will be used as a fallback. enable_cache : Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a StepContext (see :class : zenml.steps.step_context.StepContext for more information). output_types : A dictionary which sets different outputs to non-default artifact types Returns: the inner decorator which creates the step class based on the ZenML BaseStep This file was automatically generated via lazydocs .","title":"function step"},{"location":"steps.step_interfaces.base_analyzer_step/","text":"module steps.step_interfaces.base_analyzer_step class BaseAnalyzerConfig Base class for analyzer step configurations class BaseAnalyzerStep Base step implementation for any analyzer step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( dataset: DataArtifact, config: BaseAnalyzerConfig, context: StepContext ) \u2192 <Output object at 0x7fe5c7e31dc0> Base entrypoint for any analyzer implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base analyzer step"},{"location":"steps.step_interfaces.base_analyzer_step/#module-stepsstep_interfacesbase_analyzer_step","text":"","title":"module steps.step_interfaces.base_analyzer_step"},{"location":"steps.step_interfaces.base_analyzer_step/#class-baseanalyzerconfig","text":"Base class for analyzer step configurations","title":"class BaseAnalyzerConfig"},{"location":"steps.step_interfaces.base_analyzer_step/#class-baseanalyzerstep","text":"Base step implementation for any analyzer step implementation on ZenML","title":"class BaseAnalyzerStep"},{"location":"steps.step_interfaces.base_analyzer_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"steps.step_interfaces.base_analyzer_step/#method-entrypoint","text":"entrypoint( dataset: DataArtifact, config: BaseAnalyzerConfig, context: StepContext ) \u2192 <Output object at 0x7fe5c7e31dc0> Base entrypoint for any analyzer implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"steps.step_interfaces.base_datasource_step/","text":"module steps.step_interfaces.base_datasource_step class BaseDatasourceConfig Base class for datasource configs to inherit from class BaseDatasourceStep Base step implementation for any datasource step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint(config: BaseDatasourceConfig, context: StepContext) \u2192 DataArtifact Base entrypoint for any datasource implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base datasource step"},{"location":"steps.step_interfaces.base_datasource_step/#module-stepsstep_interfacesbase_datasource_step","text":"","title":"module steps.step_interfaces.base_datasource_step"},{"location":"steps.step_interfaces.base_datasource_step/#class-basedatasourceconfig","text":"Base class for datasource configs to inherit from","title":"class BaseDatasourceConfig"},{"location":"steps.step_interfaces.base_datasource_step/#class-basedatasourcestep","text":"Base step implementation for any datasource step implementation on ZenML","title":"class BaseDatasourceStep"},{"location":"steps.step_interfaces.base_datasource_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"steps.step_interfaces.base_datasource_step/#method-entrypoint","text":"entrypoint(config: BaseDatasourceConfig, context: StepContext) \u2192 DataArtifact Base entrypoint for any datasource implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"steps.step_interfaces.base_drift_detection_step/","text":"module steps.step_interfaces.base_drift_detection_step class BaseDriftDetectionConfig Base class for drift detection step configurations class BaseDriftDetectionStep Base step implementation for any drift detection step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( reference_dataset: DataArtifact, comparison_dataset: DataArtifact, config: BaseDriftDetectionConfig, context: StepContext ) \u2192 Any Base entrypoint for any drift detection implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base drift detection step"},{"location":"steps.step_interfaces.base_drift_detection_step/#module-stepsstep_interfacesbase_drift_detection_step","text":"","title":"module steps.step_interfaces.base_drift_detection_step"},{"location":"steps.step_interfaces.base_drift_detection_step/#class-basedriftdetectionconfig","text":"Base class for drift detection step configurations","title":"class BaseDriftDetectionConfig"},{"location":"steps.step_interfaces.base_drift_detection_step/#class-basedriftdetectionstep","text":"Base step implementation for any drift detection step implementation on ZenML","title":"class BaseDriftDetectionStep"},{"location":"steps.step_interfaces.base_drift_detection_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"steps.step_interfaces.base_drift_detection_step/#method-entrypoint","text":"entrypoint( reference_dataset: DataArtifact, comparison_dataset: DataArtifact, config: BaseDriftDetectionConfig, context: StepContext ) \u2192 Any Base entrypoint for any drift detection implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"steps.step_interfaces.base_evaluator_step/","text":"module steps.step_interfaces.base_evaluator_step class BaseEvaluatorConfig Base class for evaluator step configurations class BaseEvaluatorStep Base step implementation for any evaluator step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( dataset: DataArtifact, model: ModelArtifact, config: BaseEvaluatorConfig, context: StepContext ) \u2192 DataArtifact Base entrypoint for any evaluator implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base evaluator step"},{"location":"steps.step_interfaces.base_evaluator_step/#module-stepsstep_interfacesbase_evaluator_step","text":"","title":"module steps.step_interfaces.base_evaluator_step"},{"location":"steps.step_interfaces.base_evaluator_step/#class-baseevaluatorconfig","text":"Base class for evaluator step configurations","title":"class BaseEvaluatorConfig"},{"location":"steps.step_interfaces.base_evaluator_step/#class-baseevaluatorstep","text":"Base step implementation for any evaluator step implementation on ZenML","title":"class BaseEvaluatorStep"},{"location":"steps.step_interfaces.base_evaluator_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"steps.step_interfaces.base_evaluator_step/#method-entrypoint","text":"entrypoint( dataset: DataArtifact, model: ModelArtifact, config: BaseEvaluatorConfig, context: StepContext ) \u2192 DataArtifact Base entrypoint for any evaluator implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"steps.step_interfaces.base_preprocessor_step/","text":"module steps.step_interfaces.base_preprocessor_step class BasePreprocessorConfig Base class for Preprocessor step configurations class BasePreprocessorStep Base step implementation for any Preprocessor step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( train_dataset: DataArtifact, test_dataset: DataArtifact, validation_dataset: DataArtifact, statistics: StatisticsArtifact, schema: SchemaArtifact, config: BasePreprocessorConfig, context: StepContext ) \u2192 <Output object at 0x7fe5c7e49ca0> Base entrypoint for any Preprocessor implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base preprocessor step"},{"location":"steps.step_interfaces.base_preprocessor_step/#module-stepsstep_interfacesbase_preprocessor_step","text":"","title":"module steps.step_interfaces.base_preprocessor_step"},{"location":"steps.step_interfaces.base_preprocessor_step/#class-basepreprocessorconfig","text":"Base class for Preprocessor step configurations","title":"class BasePreprocessorConfig"},{"location":"steps.step_interfaces.base_preprocessor_step/#class-basepreprocessorstep","text":"Base step implementation for any Preprocessor step implementation on ZenML","title":"class BasePreprocessorStep"},{"location":"steps.step_interfaces.base_preprocessor_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"steps.step_interfaces.base_preprocessor_step/#method-entrypoint","text":"entrypoint( train_dataset: DataArtifact, test_dataset: DataArtifact, validation_dataset: DataArtifact, statistics: StatisticsArtifact, schema: SchemaArtifact, config: BasePreprocessorConfig, context: StepContext ) \u2192 <Output object at 0x7fe5c7e49ca0> Base entrypoint for any Preprocessor implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"steps.step_interfaces.base_split_step/","text":"module steps.step_interfaces.base_split_step class BaseSplitStepConfig Base class for split configs to inherit from class BaseSplitStep Base step implementation for any split step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( dataset: DataArtifact, config: BaseSplitStepConfig, context: StepContext ) \u2192 <Output object at 0x7fe5c7e5d250> Entrypoint for a function for the split steps to run This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base split step"},{"location":"steps.step_interfaces.base_split_step/#module-stepsstep_interfacesbase_split_step","text":"","title":"module steps.step_interfaces.base_split_step"},{"location":"steps.step_interfaces.base_split_step/#class-basesplitstepconfig","text":"Base class for split configs to inherit from","title":"class BaseSplitStepConfig"},{"location":"steps.step_interfaces.base_split_step/#class-basesplitstep","text":"Base step implementation for any split step implementation on ZenML","title":"class BaseSplitStep"},{"location":"steps.step_interfaces.base_split_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"steps.step_interfaces.base_split_step/#method-entrypoint","text":"entrypoint( dataset: DataArtifact, config: BaseSplitStepConfig, context: StepContext ) \u2192 <Output object at 0x7fe5c7e5d250> Entrypoint for a function for the split steps to run This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"steps.step_interfaces.base_trainer_step/","text":"module steps.step_interfaces.base_trainer_step class BaseTrainerConfig Base class for Trainer step configurations class BaseTrainerStep Base step implementation for any Trainer step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( train_dataset: DataArtifact, validation_dataset: DataArtifact, config: BaseTrainerConfig, context: StepContext ) \u2192 ModelArtifact Base entrypoint for any Trainer implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base trainer step"},{"location":"steps.step_interfaces.base_trainer_step/#module-stepsstep_interfacesbase_trainer_step","text":"","title":"module steps.step_interfaces.base_trainer_step"},{"location":"steps.step_interfaces.base_trainer_step/#class-basetrainerconfig","text":"Base class for Trainer step configurations","title":"class BaseTrainerConfig"},{"location":"steps.step_interfaces.base_trainer_step/#class-basetrainerstep","text":"Base step implementation for any Trainer step implementation on ZenML","title":"class BaseTrainerStep"},{"location":"steps.step_interfaces.base_trainer_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"steps.step_interfaces.base_trainer_step/#method-entrypoint","text":"entrypoint( train_dataset: DataArtifact, validation_dataset: DataArtifact, config: BaseTrainerConfig, context: StepContext ) \u2192 ModelArtifact Base entrypoint for any Trainer implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"steps.step_interfaces/","text":"module steps.step_interfaces This file was automatically generated via lazydocs .","title":"Steps.step interfaces"},{"location":"steps.step_interfaces/#module-stepsstep_interfaces","text":"This file was automatically generated via lazydocs .","title":"module steps.step_interfaces"},{"location":"steps.step_output/","text":"module steps.step_output class Output A named tuple with a default name that cannot be overridden. method __init__ __init__(**kwargs: Type[Any]) method items items() \u2192 Iterator[Tuple[str, Type[Any]]] Yields a tuple of type (output_name, output_type). This file was automatically generated via lazydocs .","title":"Steps.step output"},{"location":"steps.step_output/#module-stepsstep_output","text":"","title":"module steps.step_output"},{"location":"steps.step_output/#class-output","text":"A named tuple with a default name that cannot be overridden.","title":"class Output"},{"location":"steps.step_output/#method-__init__","text":"__init__(**kwargs: Type[Any])","title":"method __init__"},{"location":"steps.step_output/#method-items","text":"items() \u2192 Iterator[Tuple[str, Type[Any]]] Yields a tuple of type (output_name, output_type). This file was automatically generated via lazydocs .","title":"method items"},{"location":"steps.utils/","text":"module steps.utils The collection of utility functions/classes are inspired by their original implementation of the Tensorflow Extended team, which can be found here: https://github.com/tensorflow/tfx/blob/master/tfx/dsl/component/experimental /decorators.py This version is heavily adjusted to work with the Pipeline-Step paradigm which is proposed by ZenML. Global Variables STEP_INNER_FUNC_NAME SINGLE_RETURN_OUT_NAME PARAM_STEP_NAME PARAM_ENABLE_CACHE PARAM_PIPELINE_PARAMETER_NAME PARAM_CREATED_BY_FUNCTIONAL_API INTERNAL_EXECUTION_PARAMETER_PREFIX INSTANCE_CONFIGURATION OUTPUT_SPEC function do_types_match do_types_match(type_a: Type[Any], type_b: Type[Any]) \u2192 bool Check whether type_a and type_b match. Args: type_a : First Type to check. type_b : Second Type to check. Returns: True if types match, otherwise False. function generate_component_spec_class generate_component_spec_class( step_name: str, input_spec: Dict[str, Type[BaseArtifact]], output_spec: Dict[str, Type[BaseArtifact]], execution_parameter_names: Set[str] ) \u2192 Type[ComponentSpec] Generates a TFX component spec class for a ZenML step. Args: step_name : Name of the step for which the component will be created. input_spec : Input artifacts of the step. output_spec : Output artifacts of the step execution_parameter_names : Execution parameter names of the step. Returns: A TFX component spec class. function generate_component_class generate_component_class( step_name: str, step_module: str, input_spec: Dict[str, Type[BaseArtifact]], output_spec: Dict[str, Type[BaseArtifact]], execution_parameter_names: Set[str], step_function: Callable[, Any], materializers: Dict[str, Type[BaseMaterializer]] ) \u2192 Type[ForwardRef('_ZenMLSimpleComponent')] Generates a TFX component class for a ZenML step. Args: step_name : Name of the step for which the component will be created. step_module : Module in which the step class is defined. input_spec : Input artifacts of the step. output_spec : Output artifacts of the step execution_parameter_names : Execution parameter names of the step. step_function : The actual function to execute when running the step. materializers : Materializer classes for all outputs of the step. Returns: A TFX component class. This file was automatically generated via lazydocs .","title":"Steps.utils"},{"location":"steps.utils/#module-stepsutils","text":"The collection of utility functions/classes are inspired by their original implementation of the Tensorflow Extended team, which can be found here: https://github.com/tensorflow/tfx/blob/master/tfx/dsl/component/experimental /decorators.py This version is heavily adjusted to work with the Pipeline-Step paradigm which is proposed by ZenML.","title":"module steps.utils"},{"location":"steps.utils/#global-variables","text":"STEP_INNER_FUNC_NAME SINGLE_RETURN_OUT_NAME PARAM_STEP_NAME PARAM_ENABLE_CACHE PARAM_PIPELINE_PARAMETER_NAME PARAM_CREATED_BY_FUNCTIONAL_API INTERNAL_EXECUTION_PARAMETER_PREFIX INSTANCE_CONFIGURATION OUTPUT_SPEC","title":"Global Variables"},{"location":"steps.utils/#function-do_types_match","text":"do_types_match(type_a: Type[Any], type_b: Type[Any]) \u2192 bool Check whether type_a and type_b match. Args: type_a : First Type to check. type_b : Second Type to check. Returns: True if types match, otherwise False.","title":"function do_types_match"},{"location":"steps.utils/#function-generate_component_spec_class","text":"generate_component_spec_class( step_name: str, input_spec: Dict[str, Type[BaseArtifact]], output_spec: Dict[str, Type[BaseArtifact]], execution_parameter_names: Set[str] ) \u2192 Type[ComponentSpec] Generates a TFX component spec class for a ZenML step. Args: step_name : Name of the step for which the component will be created. input_spec : Input artifacts of the step. output_spec : Output artifacts of the step execution_parameter_names : Execution parameter names of the step. Returns: A TFX component spec class.","title":"function generate_component_spec_class"},{"location":"steps.utils/#function-generate_component_class","text":"generate_component_class( step_name: str, step_module: str, input_spec: Dict[str, Type[BaseArtifact]], output_spec: Dict[str, Type[BaseArtifact]], execution_parameter_names: Set[str], step_function: Callable[, Any], materializers: Dict[str, Type[BaseMaterializer]] ) \u2192 Type[ForwardRef('_ZenMLSimpleComponent')] Generates a TFX component class for a ZenML step. Args: step_name : Name of the step for which the component will be created. step_module : Module in which the step class is defined. input_spec : Input artifacts of the step. output_spec : Output artifacts of the step execution_parameter_names : Execution parameter names of the step. step_function : The actual function to execute when running the step. materializers : Materializer classes for all outputs of the step. Returns: A TFX component class. This file was automatically generated via lazydocs .","title":"function generate_component_class"},{"location":"utils.analytics_utils/","text":"module utils.analytics_utils Analytics code for ZenML Global Variables IS_DEBUG_ENV SEGMENT_KEY_DEV SEGMENT_KEY_PROD RUN_PIPELINE GET_PIPELINES GET_PIPELINE INITIALIZE_REPO REGISTERED_METADATA_STORE REGISTERED_ARTIFACT_STORE REGISTERED_ORCHESTRATOR REGISTERED_CONTAINER_REGISTRY REGISTERED_STACK SET_STACK OPT_IN_ANALYTICS OPT_OUT_ANALYTICS RUN_EXAMPLE EVENT_TEST function get_segment_key get_segment_key() \u2192 str Get key for authorizing to Segment backend. Returns: Segment key as a string. function in_docker in_docker() \u2192 bool Returns: True if running in a Docker container, else False function in_google_colab in_google_colab() \u2192 bool Returns: True if running in a Google Colab env, else False function in_paperspace_gradient in_paperspace_gradient() \u2192 bool Returns: True if running in a Paperspace Gradient env, else False function get_system_info get_system_info() \u2192 Dict[str, Any] Returns system info as a dict. Returns: A dict of system information. function get_environment get_environment() \u2192 str Returns a string representing the execution environment of the pipeline. Currently, one of docker , paperspace , 'colab', or native function track_event track_event(event: str, metadata: Optional[Dict[str, Any]] = None) \u2192 bool Track segment event if user opted-in. Args: event : Name of event to track in segment. metadata : Dict of metadata to track. Returns: True if event is sent successfully, False is not. function parametrized parametrized( dec: Callable[, Callable[, Any]] ) \u2192 Callable[, Callable[[Callable[, Any]], Callable[, Any]]] This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments: function layer layer(*args: Any, **kwargs: Any) \u2192 Callable[[Callable[, Any]], Callable[, Any]] Internal layer This file was automatically generated via lazydocs .","title":"Utils.analytics utils"},{"location":"utils.analytics_utils/#module-utilsanalytics_utils","text":"Analytics code for ZenML","title":"module utils.analytics_utils"},{"location":"utils.analytics_utils/#global-variables","text":"IS_DEBUG_ENV SEGMENT_KEY_DEV SEGMENT_KEY_PROD RUN_PIPELINE GET_PIPELINES GET_PIPELINE INITIALIZE_REPO REGISTERED_METADATA_STORE REGISTERED_ARTIFACT_STORE REGISTERED_ORCHESTRATOR REGISTERED_CONTAINER_REGISTRY REGISTERED_STACK SET_STACK OPT_IN_ANALYTICS OPT_OUT_ANALYTICS RUN_EXAMPLE EVENT_TEST","title":"Global Variables"},{"location":"utils.analytics_utils/#function-get_segment_key","text":"get_segment_key() \u2192 str Get key for authorizing to Segment backend. Returns: Segment key as a string.","title":"function get_segment_key"},{"location":"utils.analytics_utils/#function-in_docker","text":"in_docker() \u2192 bool Returns: True if running in a Docker container, else False","title":"function in_docker"},{"location":"utils.analytics_utils/#function-in_google_colab","text":"in_google_colab() \u2192 bool Returns: True if running in a Google Colab env, else False","title":"function in_google_colab"},{"location":"utils.analytics_utils/#function-in_paperspace_gradient","text":"in_paperspace_gradient() \u2192 bool Returns: True if running in a Paperspace Gradient env, else False","title":"function in_paperspace_gradient"},{"location":"utils.analytics_utils/#function-get_system_info","text":"get_system_info() \u2192 Dict[str, Any] Returns system info as a dict. Returns: A dict of system information.","title":"function get_system_info"},{"location":"utils.analytics_utils/#function-get_environment","text":"get_environment() \u2192 str Returns a string representing the execution environment of the pipeline. Currently, one of docker , paperspace , 'colab', or native","title":"function get_environment"},{"location":"utils.analytics_utils/#function-track_event","text":"track_event(event: str, metadata: Optional[Dict[str, Any]] = None) \u2192 bool Track segment event if user opted-in. Args: event : Name of event to track in segment. metadata : Dict of metadata to track. Returns: True if event is sent successfully, False is not.","title":"function track_event"},{"location":"utils.analytics_utils/#function-parametrized","text":"parametrized( dec: Callable[, Callable[, Any]] ) \u2192 Callable[, Callable[[Callable[, Any]], Callable[, Any]]] This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments:","title":"function parametrized"},{"location":"utils.analytics_utils/#function-layer","text":"layer(*args: Any, **kwargs: Any) \u2192 Callable[[Callable[, Any]], Callable[, Any]] Internal layer This file was automatically generated via lazydocs .","title":"function layer"},{"location":"utils.daemon/","text":"module utils.daemon Utility functions to start/stop daemon processes. This is only implemented for UNIX systems and therefore doesn't work on Windows. Based on https://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/ function run_as_daemon run_as_daemon( daemon_function: Callable[, Any], pid_file: str, log_file: Optional[str] = None, working_directory: str = '/' ) \u2192 None Runs a function as a daemon process. Args: daemon_function : The function to run as a daemon. pid_file : Path to file in which to store the PID of the daemon process. log_file : Optional file to which the daemons stdout/stderr will be redirected to. working_directory : Working directory for the daemon process, defaults to the root directory. Raises: FileExistsError : If the PID file already exists. function stop_daemon stop_daemon(pid_file: str, kill_children: bool = True) \u2192 None Stops a daemon process. Args: pid_file : Path to file containing the PID of the daemon process to kill. kill_children : If True , all child processes of the daemon process will be killed as well. function check_if_daemon_is_running check_if_daemon_is_running(pid_file: str) \u2192 bool Checks whether a daemon process indicated by the PID file is running. Args: pid_file : Path to file containing the PID of the daemon process to check. This file was automatically generated via lazydocs .","title":"Utils.daemon"},{"location":"utils.daemon/#module-utilsdaemon","text":"Utility functions to start/stop daemon processes. This is only implemented for UNIX systems and therefore doesn't work on Windows. Based on https://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/","title":"module utils.daemon"},{"location":"utils.daemon/#function-run_as_daemon","text":"run_as_daemon( daemon_function: Callable[, Any], pid_file: str, log_file: Optional[str] = None, working_directory: str = '/' ) \u2192 None Runs a function as a daemon process. Args: daemon_function : The function to run as a daemon. pid_file : Path to file in which to store the PID of the daemon process. log_file : Optional file to which the daemons stdout/stderr will be redirected to. working_directory : Working directory for the daemon process, defaults to the root directory. Raises: FileExistsError : If the PID file already exists.","title":"function run_as_daemon"},{"location":"utils.daemon/#function-stop_daemon","text":"stop_daemon(pid_file: str, kill_children: bool = True) \u2192 None Stops a daemon process. Args: pid_file : Path to file containing the PID of the daemon process to kill. kill_children : If True , all child processes of the daemon process will be killed as well.","title":"function stop_daemon"},{"location":"utils.daemon/#function-check_if_daemon_is_running","text":"check_if_daemon_is_running(pid_file: str) \u2192 bool Checks whether a daemon process indicated by the PID file is running. Args: pid_file : Path to file containing the PID of the daemon process to check. This file was automatically generated via lazydocs .","title":"function check_if_daemon_is_running"},{"location":"utils/","text":"module utils The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions. This file was automatically generated via lazydocs .","title":"Utils"},{"location":"utils/#module-utils","text":"The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions. This file was automatically generated via lazydocs .","title":"module utils"},{"location":"utils.networking_utils/","text":"module utils.networking_utils function port_available port_available(port: int) \u2192 bool Checks if a local port is available. function find_available_port find_available_port() \u2192 int Finds a local unoccupied port. This file was automatically generated via lazydocs .","title":"Utils.networking utils"},{"location":"utils.networking_utils/#module-utilsnetworking_utils","text":"","title":"module utils.networking_utils"},{"location":"utils.networking_utils/#function-port_available","text":"port_available(port: int) \u2192 bool Checks if a local port is available.","title":"function port_available"},{"location":"utils.networking_utils/#function-find_available_port","text":"find_available_port() \u2192 int Finds a local unoccupied port. This file was automatically generated via lazydocs .","title":"function find_available_port"},{"location":"utils.source_utils/","text":"module utils.source_utils These utils are predicated on the following definitions: class_source: This is a python-import type path to a class, e.g. some.mod.class * module_source: This is a python-import type path to a module, e.g. some.mod * file_path, relative_path, absolute_path: These are file system paths. * source: This is a class_source or module_source. If it is a class_source, it can also be optionally pinned. * pin: Whatever comes after the @ symbol from a source, usually the git sha or the version of zenml as a string. Global Variables APP_NAME function is_standard_pin is_standard_pin(pin: str) \u2192 bool Returns True if pin is valid ZenML pin, else False. Args: pin : potential ZenML pin like 'zenml_0.1.1' function is_inside_repository is_inside_repository(file_path: str) \u2192 bool Returns whether a file is inside a zenml repository. function is_third_party_module is_third_party_module(file_path: str) \u2192 bool Returns whether a file belongs to a third party package. function create_zenml_pin create_zenml_pin() \u2192 str Creates a ZenML pin for source pinning from release version. function resolve_standard_source resolve_standard_source(source: str) \u2192 str Creates a ZenML pin for source pinning from release version. Args: source : class_source e.g. this.module.Class. function is_standard_source is_standard_source(source: str) \u2192 bool Returns True if source is a standard ZenML source. Args: source : class_source e.g. this.module.Class[@pin]. function get_class_source_from_source get_class_source_from_source(source: str) \u2192 str Gets class source from source, i.e. module.path@version, returns version. Args: source : source pointing to potentially pinned sha. function get_module_source_from_source get_module_source_from_source(source: str) \u2192 str Gets module source from source. E.g. some.module.file.class@version , returns some.module . Args: source : source pointing to potentially pinned sha. function get_module_source_from_file_path get_module_source_from_file_path(file_path: str) \u2192 str Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py returns myrepo.step.trainer if myrepo is the root of the repo. Args: file_path : Absolute file path to a file within the module. function get_relative_path_from_module_source get_relative_path_from_module_source(module_source: str) \u2192 str Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Args: module_source : A module e.g. zenml.core.step function get_absolute_path_from_module_source get_absolute_path_from_module_source(module: str) \u2192 str Get a directory path from module source. E.g. zenml.core.step will return full/path/to/zenml/core/step . Args: module : A module e.g. zenml.core.step . function get_module_source_from_class get_module_source_from_class( class_: Union[Type[Any], str] ) \u2192 Union[str, NoneType] Takes class input and returns module_source. If class is already string then returns the same. Args: class_ : object of type class. function resolve_class resolve_class(class_: Type[Any]) \u2192 str Resolves a class into a serializable source string. Args: class_ : A Python Class reference. Returns: source_path e.g. this.module.Class. function import_class_by_path import_class_by_path(class_path: str) \u2192 Type[Any] Imports a class based on a given path Args: class_path : str, class_source e.g. this.module.Class Returns: the given class function load_source_path_class load_source_path_class(source: str) \u2192 Type[Any] Loads a Python class from the source. Args: source : class_source e.g. this.module.Class[@sha] function import_python_file import_python_file(file_path: str) \u2192 module Imports a python file. Args: file_path : Path to python file that should be imported. Returns: The imported module. This file was automatically generated via lazydocs .","title":"Utils.source utils"},{"location":"utils.source_utils/#module-utilssource_utils","text":"These utils are predicated on the following definitions: class_source: This is a python-import type path to a class, e.g. some.mod.class * module_source: This is a python-import type path to a module, e.g. some.mod * file_path, relative_path, absolute_path: These are file system paths. * source: This is a class_source or module_source. If it is a class_source, it can also be optionally pinned. * pin: Whatever comes after the @ symbol from a source, usually the git sha or the version of zenml as a string.","title":"module utils.source_utils"},{"location":"utils.source_utils/#global-variables","text":"APP_NAME","title":"Global Variables"},{"location":"utils.source_utils/#function-is_standard_pin","text":"is_standard_pin(pin: str) \u2192 bool Returns True if pin is valid ZenML pin, else False. Args: pin : potential ZenML pin like 'zenml_0.1.1'","title":"function is_standard_pin"},{"location":"utils.source_utils/#function-is_inside_repository","text":"is_inside_repository(file_path: str) \u2192 bool Returns whether a file is inside a zenml repository.","title":"function is_inside_repository"},{"location":"utils.source_utils/#function-is_third_party_module","text":"is_third_party_module(file_path: str) \u2192 bool Returns whether a file belongs to a third party package.","title":"function is_third_party_module"},{"location":"utils.source_utils/#function-create_zenml_pin","text":"create_zenml_pin() \u2192 str Creates a ZenML pin for source pinning from release version.","title":"function create_zenml_pin"},{"location":"utils.source_utils/#function-resolve_standard_source","text":"resolve_standard_source(source: str) \u2192 str Creates a ZenML pin for source pinning from release version. Args: source : class_source e.g. this.module.Class.","title":"function resolve_standard_source"},{"location":"utils.source_utils/#function-is_standard_source","text":"is_standard_source(source: str) \u2192 bool Returns True if source is a standard ZenML source. Args: source : class_source e.g. this.module.Class[@pin].","title":"function is_standard_source"},{"location":"utils.source_utils/#function-get_class_source_from_source","text":"get_class_source_from_source(source: str) \u2192 str Gets class source from source, i.e. module.path@version, returns version. Args: source : source pointing to potentially pinned sha.","title":"function get_class_source_from_source"},{"location":"utils.source_utils/#function-get_module_source_from_source","text":"get_module_source_from_source(source: str) \u2192 str Gets module source from source. E.g. some.module.file.class@version , returns some.module . Args: source : source pointing to potentially pinned sha.","title":"function get_module_source_from_source"},{"location":"utils.source_utils/#function-get_module_source_from_file_path","text":"get_module_source_from_file_path(file_path: str) \u2192 str Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py returns myrepo.step.trainer if myrepo is the root of the repo. Args: file_path : Absolute file path to a file within the module.","title":"function get_module_source_from_file_path"},{"location":"utils.source_utils/#function-get_relative_path_from_module_source","text":"get_relative_path_from_module_source(module_source: str) \u2192 str Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Args: module_source : A module e.g. zenml.core.step","title":"function get_relative_path_from_module_source"},{"location":"utils.source_utils/#function-get_absolute_path_from_module_source","text":"get_absolute_path_from_module_source(module: str) \u2192 str Get a directory path from module source. E.g. zenml.core.step will return full/path/to/zenml/core/step . Args: module : A module e.g. zenml.core.step .","title":"function get_absolute_path_from_module_source"},{"location":"utils.source_utils/#function-get_module_source_from_class","text":"get_module_source_from_class( class_: Union[Type[Any], str] ) \u2192 Union[str, NoneType] Takes class input and returns module_source. If class is already string then returns the same. Args: class_ : object of type class.","title":"function get_module_source_from_class"},{"location":"utils.source_utils/#function-resolve_class","text":"resolve_class(class_: Type[Any]) \u2192 str Resolves a class into a serializable source string. Args: class_ : A Python Class reference. Returns: source_path e.g. this.module.Class.","title":"function resolve_class"},{"location":"utils.source_utils/#function-import_class_by_path","text":"import_class_by_path(class_path: str) \u2192 Type[Any] Imports a class based on a given path Args: class_path : str, class_source e.g. this.module.Class Returns: the given class","title":"function import_class_by_path"},{"location":"utils.source_utils/#function-load_source_path_class","text":"load_source_path_class(source: str) \u2192 Type[Any] Loads a Python class from the source. Args: source : class_source e.g. this.module.Class[@sha]","title":"function load_source_path_class"},{"location":"utils.source_utils/#function-import_python_file","text":"import_python_file(file_path: str) \u2192 module Imports a python file. Args: file_path : Path to python file that should be imported. Returns: The imported module. This file was automatically generated via lazydocs .","title":"function import_python_file"},{"location":"utils.string_utils/","text":"module utils.string_utils function get_human_readable_time get_human_readable_time(seconds: float) \u2192 str Convert seconds into a human-readable string. function get_human_readable_filesize get_human_readable_filesize(bytes_: int) \u2192 str Convert a file size in bytes into a human-readable string. This file was automatically generated via lazydocs .","title":"Utils.string utils"},{"location":"utils.string_utils/#module-utilsstring_utils","text":"","title":"module utils.string_utils"},{"location":"utils.string_utils/#function-get_human_readable_time","text":"get_human_readable_time(seconds: float) \u2192 str Convert seconds into a human-readable string.","title":"function get_human_readable_time"},{"location":"utils.string_utils/#function-get_human_readable_filesize","text":"get_human_readable_filesize(bytes_: int) \u2192 str Convert a file size in bytes into a human-readable string. This file was automatically generated via lazydocs .","title":"function get_human_readable_filesize"},{"location":"utils.yaml_utils/","text":"module utils.yaml_utils function write_yaml write_yaml(file_path: str, contents: Dict[Any, Any]) \u2192 None Write contents as YAML format to file_path. Args: file_path : Path to YAML file. contents : Contents of YAML file as dict. Raises: FileNotFoundError if directory does not exist. function read_yaml read_yaml(file_path: str) \u2192 Any Read YAML on file path and returns contents as dict. Args: file_path : Path to YAML file. Returns: Contents of the file in a dict. Raises: FileNotFoundError if file does not exist. function is_yaml is_yaml(file_path: str) \u2192 bool Returns True if file_path is YAML, else False Args: file_path : Path to YAML file. Returns: True if is yaml, else False. function write_json write_json(file_path: str, contents: Dict[str, Any]) \u2192 None Write contents as JSON format to file_path. Args: file_path : Path to JSON file. contents : Contents of JSON file as dict. Returns: Contents of the file in a dict. Raises: FileNotFoundError if directory does not exist. function read_json read_json(file_path: str) \u2192 Any Read JSON on file path and returns contents as dict. Args: file_path : Path to JSON file. This file was automatically generated via lazydocs .","title":"Utils.yaml utils"},{"location":"utils.yaml_utils/#module-utilsyaml_utils","text":"","title":"module utils.yaml_utils"},{"location":"utils.yaml_utils/#function-write_yaml","text":"write_yaml(file_path: str, contents: Dict[Any, Any]) \u2192 None Write contents as YAML format to file_path. Args: file_path : Path to YAML file. contents : Contents of YAML file as dict. Raises: FileNotFoundError if directory does not exist.","title":"function write_yaml"},{"location":"utils.yaml_utils/#function-read_yaml","text":"read_yaml(file_path: str) \u2192 Any Read YAML on file path and returns contents as dict. Args: file_path : Path to YAML file. Returns: Contents of the file in a dict. Raises: FileNotFoundError if file does not exist.","title":"function read_yaml"},{"location":"utils.yaml_utils/#function-is_yaml","text":"is_yaml(file_path: str) \u2192 bool Returns True if file_path is YAML, else False Args: file_path : Path to YAML file. Returns: True if is yaml, else False.","title":"function is_yaml"},{"location":"utils.yaml_utils/#function-write_json","text":"write_json(file_path: str, contents: Dict[str, Any]) \u2192 None Write contents as JSON format to file_path. Args: file_path : Path to JSON file. contents : Contents of JSON file as dict. Returns: Contents of the file in a dict. Raises: FileNotFoundError if directory does not exist.","title":"function write_json"},{"location":"utils.yaml_utils/#function-read_json","text":"read_json(file_path: str) \u2192 Any Read JSON on file path and returns contents as dict. Args: file_path : Path to JSON file. This file was automatically generated via lazydocs .","title":"function read_json"},{"location":"visualizers.base_pipeline_run_visualizer/","text":"module visualizers.base_pipeline_run_visualizer class BasePipelineRunVisualizer The base implementation of a ZenML Pipeline Run Visualizer. method visualize visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 None Method to visualize pipeline runs. This file was automatically generated via lazydocs .","title":"Visualizers.base pipeline run visualizer"},{"location":"visualizers.base_pipeline_run_visualizer/#module-visualizersbase_pipeline_run_visualizer","text":"","title":"module visualizers.base_pipeline_run_visualizer"},{"location":"visualizers.base_pipeline_run_visualizer/#class-basepipelinerunvisualizer","text":"The base implementation of a ZenML Pipeline Run Visualizer.","title":"class BasePipelineRunVisualizer"},{"location":"visualizers.base_pipeline_run_visualizer/#method-visualize","text":"visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 None Method to visualize pipeline runs. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"visualizers.base_pipeline_visualizer/","text":"module visualizers.base_pipeline_visualizer class BasePipelineVisualizer The base implementation of a ZenML Pipeline Visualizer. method visualize visualize(object: PipelineView, *args: Any, **kwargs: Any) \u2192 Any Method to visualize pipelines. This file was automatically generated via lazydocs .","title":"Visualizers.base pipeline visualizer"},{"location":"visualizers.base_pipeline_visualizer/#module-visualizersbase_pipeline_visualizer","text":"","title":"module visualizers.base_pipeline_visualizer"},{"location":"visualizers.base_pipeline_visualizer/#class-basepipelinevisualizer","text":"The base implementation of a ZenML Pipeline Visualizer.","title":"class BasePipelineVisualizer"},{"location":"visualizers.base_pipeline_visualizer/#method-visualize","text":"visualize(object: PipelineView, *args: Any, **kwargs: Any) \u2192 Any Method to visualize pipelines. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"visualizers.base_step_visualizer/","text":"module visualizers.base_step_visualizer class BaseStepVisualizer The base implementation of a ZenML Step Visualizer. method running_in_notebook running_in_notebook() \u2192 bool Detect whether we're running in a Jupyter notebook or not method visualize visualize(object: StepView, *args: Any, **kwargs: Any) \u2192 Any Method to visualize steps. This file was automatically generated via lazydocs .","title":"Visualizers.base step visualizer"},{"location":"visualizers.base_step_visualizer/#module-visualizersbase_step_visualizer","text":"","title":"module visualizers.base_step_visualizer"},{"location":"visualizers.base_step_visualizer/#class-basestepvisualizer","text":"The base implementation of a ZenML Step Visualizer.","title":"class BaseStepVisualizer"},{"location":"visualizers.base_step_visualizer/#method-running_in_notebook","text":"running_in_notebook() \u2192 bool Detect whether we're running in a Jupyter notebook or not","title":"method running_in_notebook"},{"location":"visualizers.base_step_visualizer/#method-visualize","text":"visualize(object: StepView, *args: Any, **kwargs: Any) \u2192 Any Method to visualize steps. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"visualizers.base_visualizer/","text":"module visualizers.base_visualizer class BaseVisualizer Base class for all ZenML Visualizers. method visualize visualize(object: Any, *args: Any, **kwargs: Any) \u2192 None Method to visualize objects. This file was automatically generated via lazydocs .","title":"Visualizers.base visualizer"},{"location":"visualizers.base_visualizer/#module-visualizersbase_visualizer","text":"","title":"module visualizers.base_visualizer"},{"location":"visualizers.base_visualizer/#class-basevisualizer","text":"Base class for all ZenML Visualizers.","title":"class BaseVisualizer"},{"location":"visualizers.base_visualizer/#method-visualize","text":"visualize(object: Any, *args: Any, **kwargs: Any) \u2192 None Method to visualize objects. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"visualizers/","text":"module visualizers The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves. This file was automatically generated via lazydocs .","title":"Visualizers"},{"location":"visualizers/#module-visualizers","text":"The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves. This file was automatically generated via lazydocs .","title":"module visualizers"},{"location":"api_docs/OVERVIEW/","text":"API Overview Modules artifact_stores : An artifact store is a place where artifacts are stored. These artifacts may artifact_stores.base_artifact_store : Definition of an Artifact Store artifact_stores.local_artifact_store artifacts : Artifacts are the data that power your experimentation and model training. It is artifacts.base_artifact : The below code is copied from the TFX source repo with minor changes. artifacts.constants artifacts.data_analysis_artifact artifacts.data_artifact artifacts.model_artifact artifacts.schema_artifact artifacts.statistics_artifact artifacts.type_registry cli : ZenML CLI cli.artifact_store cli.base cli.cli : .. currentmodule:: ce_cli.cli cli.config : CLI for manipulating ZenML local and global config file. cli.container_registry cli.example cli.integration cli.metadata_store cli.orchestrator cli.pipeline : CLI to interact with pipelines. cli.stack : CLI for manipulating ZenML local and global config file. cli.utils cli.version config : The config module contains classes and functions that manage user-specific config.config_keys config.constants config.global_config : Global config for the ZenML installation. constants container_registries container_registries.base_container_registry : Base class for all container registries. core : The core module is where all the base ZenML functionality is defined, core.base_component core.component_factory : Factory to register all components. core.constants core.git_wrapper : Wrapper class to handle Git integration core.local_service core.mapping_utils core.repo : Base ZenML repository core.utils enums exceptions : ZenML specific exception definitions integrations : The ZenML integrations module contains sub-modules for each integration that we integrations.airflow : The Airflow integration sub-module powers an alternative to the local integrations.airflow.orchestrators integrations.airflow.orchestrators.airflow_component : Definition for Airflow component for TFX. integrations.airflow.orchestrators.airflow_dag_runner : Definition of Airflow TFX runner. This is an unmodified copy from the TFX integrations.airflow.orchestrators.airflow_orchestrator integrations.constants integrations.dash integrations.dash.visualizers integrations.dash.visualizers.pipeline_run_lineage_visualizer integrations.evidently : The Evidently integration provides a way to monitor your models in production. integrations.evidently.steps integrations.evidently.steps.evidently_profile integrations.evidently.visualizers integrations.evidently.visualizers.evidently_visualizer integrations.facets : The Facets integration provides a simple way to visualize post-execution objects integrations.facets.visualizers integrations.facets.visualizers.facet_statistics_visualizer integrations.gcp : The GCP integration submodule provides a way to run ZenML pipelines in a cloud integrations.gcp.artifact_stores integrations.gcp.artifact_stores.gcp_artifact_store integrations.gcp.io integrations.gcp.io.gcs_plugin : Plugin which is created to add Google Cloud Store support to ZenML. integrations.graphviz integrations.graphviz.visualizers integrations.graphviz.visualizers.pipeline_run_dag_visualizer integrations.integration integrations.kubeflow : The Kubeflow integration sub-module powers an alternative to the local integrations.kubeflow.container_entrypoint : Main entrypoint for containers with Kubeflow TFX component executors. integrations.kubeflow.docker_utils integrations.kubeflow.metadata integrations.kubeflow.metadata.kubeflow_metadata_store integrations.kubeflow.orchestrators integrations.kubeflow.orchestrators.kubeflow_component : Kubeflow Pipelines based implementation of TFX components. integrations.kubeflow.orchestrators.kubeflow_dag_runner : The below code is copied from the TFX source repo with minor changes. integrations.kubeflow.orchestrators.kubeflow_orchestrator integrations.kubeflow.orchestrators.kubeflow_utils : Common utility for Kubeflow-based orchestrator. integrations.kubeflow.orchestrators.local_deployment_utils integrations.mlflow : The mlflow integrations currently enables you to use mlflow tracking as a integrations.mlflow.mlflow_utils integrations.plotly integrations.plotly.visualizers integrations.plotly.visualizers.pipeline_lineage_visualizer integrations.pytorch integrations.pytorch.materializers integrations.pytorch.materializers.pytorch_materializer integrations.pytorch.materializers.pytorch_types integrations.pytorch_lightning integrations.pytorch_lightning.materializers integrations.pytorch_lightning.materializers.pytorch_lightning_materializer integrations.registry integrations.sklearn integrations.sklearn.helpers integrations.sklearn.helpers.digits integrations.sklearn.materializers integrations.sklearn.materializers.sklearn_materializer integrations.sklearn.steps integrations.sklearn.steps.sklearn_evaluator integrations.sklearn.steps.sklearn_splitter integrations.sklearn.steps.sklearn_standard_scaler integrations.tensorflow integrations.tensorflow.materializers integrations.tensorflow.materializers.keras_materializer integrations.tensorflow.materializers.tf_dataset_materializer integrations.tensorflow.steps integrations.tensorflow.steps.tensorflow_trainer integrations.utils io : The io module handles file operations for the ZenML package. It offers a io.fileio io.fileio_registry : Filesystem registry managing filesystem plugins. io.filesystem io.utils logger materializers : Materializers are used to convert a ZenML artifact into a specific format. They materializers.base_materializer materializers.beam_materializer materializers.built_in_materializer materializers.default_materializer_registry materializers.numpy_materializer materializers.pandas_materializer metadata_stores : The configuration of each pipeline, step, backend, and produced artifacts are metadata_stores.base_metadata_store metadata_stores.mysql_metadata_store metadata_stores.sqlite_metadata_store orchestrators : An orchestrator is a special kind of backend that manages the running of each orchestrators.base_orchestrator orchestrators.local orchestrators.local.local_dag_runner : Inspired by local dag runner implementation orchestrators.local.local_orchestrator orchestrators.utils pipelines : A ZenML pipeline is a sequence of tasks that execute in a specific order and pipelines.base_pipeline pipelines.builtin_pipelines pipelines.builtin_pipelines.training_pipeline pipelines.pipeline_decorator post_execution : After executing a pipeline, the user needs to be able to fetch it from history post_execution.artifact post_execution.pipeline post_execution.pipeline_run post_execution.step stacks : A stack is made up of the following three core components: an Artifact Store, a stacks.base_stack stacks.constants steps : A step is a single piece or stage of a ZenML pipeline. Think of each step as steps.base_step steps.base_step_config steps.builtin_steps steps.builtin_steps.pandas_analyzer steps.builtin_steps.pandas_datasource steps.step_context steps.step_decorator steps.step_interfaces steps.step_interfaces.base_analyzer_step steps.step_interfaces.base_datasource_step steps.step_interfaces.base_drift_detection_step steps.step_interfaces.base_evaluator_step steps.step_interfaces.base_preprocessor_step steps.step_interfaces.base_split_step steps.step_interfaces.base_trainer_step steps.step_output steps.utils : The collection of utility functions/classes are inspired by their original utils : The utils module contains utility functions handling analytics, reading and utils.analytics_utils : Analytics code for ZenML utils.daemon : Utility functions to start/stop daemon processes. utils.networking_utils utils.source_utils : These utils are predicated on the following definitions: utils.string_utils utils.yaml_utils visualizers : The visualizers module offers a way of constructing and displaying visualizers.base_pipeline_run_visualizer visualizers.base_pipeline_visualizer visualizers.base_step_visualizer visualizers.base_visualizer Classes base_artifact_store.BaseArtifactStore : Base class for all ZenML Artifact Store. local_artifact_store.LocalArtifactStore : Artifact Store for local artifacts. base_artifact.BaseArtifact : Base class for all ZenML artifacts. data_analysis_artifact.DataAnalysisArtifact : Class for all ZenML data analysis artifacts. data_artifact.DataArtifact : Class for all ZenML data artifacts. model_artifact.ModelArtifact : Class for all ZenML model artifacts. schema_artifact.SchemaArtifact : Class for all ZenML schema artifacts. statistics_artifact.StatisticsArtifact : Class for all ZenML statistics artifacts. type_registry.ArtifactTypeRegistry : A registry to keep track of which datatypes map to which artifact example.Example : Class for all example objects. example.ExamplesRepo : Class for the examples repository object. example.GitExamplesHandler : Class for the GitExamplesHandler that interfaces with the CLI tool. example.LocalExample : Class to encapsulate all properties and methods of the local example config_keys.ConfigKeys : Class to validate dictionary configurations. config_keys.PipelineConfigurationKeys : Keys for a pipeline configuration dict. config_keys.StepConfigurationKeys : Keys for a step configuration dict. global_config.GlobalConfig : Class definition for the global config. base_container_registry.BaseContainerRegistry : Base class for all ZenML container registries. base_component.BaseComponent : Class definition for the base config. component_factory.ComponentFactory : Definition of ComponentFactory to track all BaseComponent subclasses. git_wrapper.GitWrapper : Wrapper class for Git. local_service.LocalService : Definition of a local service that keeps track of all ZenML mapping_utils.UUIDSourceTuple : Container used to store UUID and source information repo.Repository : ZenML repository definition. enums.ArtifactStoreTypes : All supported Artifact Store types. enums.ExecutionStatus : Enum that represents the current status of a step or pipeline run. enums.LoggingLevels : Enum for logging levels. enums.MLMetadataTypes : All supported ML Metadata types. enums.OrchestratorTypes : All supported Orchestrator types enums.StackTypes : All supported Stack types. exceptions.AlreadyExistsException : Raises exception when the name already exist in the system but an exceptions.ArtifactInterfaceError : Raises exception when interacting with the Artifact interface exceptions.DoesNotExistException : Raises exception when the entity does not exist in the system but an exceptions.DuplicateRunNameError : Raises exception when a run with the same name already exists. exceptions.EmptyDatasourceException : Raises exception when a datasource data is accessed without running exceptions.GitException : Raises exception when a problem occurs in git resolution. exceptions.InitializationException : Raises exception when a function is run before zenml initialization. exceptions.IntegrationError : Raises exceptions when a requested integration can not be activated. exceptions.MissingStepParameterError : Raises exceptions when a step parameter is missing when running a exceptions.PipelineConfigurationError : Raises exceptions when a pipeline configuration contains exceptions.PipelineInterfaceError : Raises exception when interacting with the Pipeline interface exceptions.PipelineNotSucceededException : Raises exception when trying to fetch artifacts from a not succeeded exceptions.StepContextError : Raises exception when interacting with a StepContext exceptions.StepInterfaceError : Raises exception when interacting with the Step interface airflow.AirflowIntegration : Definition of Airflow Integration for ZenML. airflow_component.AirflowComponent : Airflow-specific TFX Component. airflow_dag_runner.AirflowDagRunner : Tfx runner on Airflow. airflow_dag_runner.AirflowPipelineConfig : Pipeline config for AirflowDagRunner. airflow_orchestrator.AirflowOrchestrator : Orchestrator responsible for running pipelines using Airflow. dash.DashIntegration : Definition of Dash integration for ZenML. pipeline_run_lineage_visualizer.PipelineRunLineageVisualizer : Implementation of a lineage diagram via the [dash]( evidently.EvidentlyIntegration : Definition of Evidently integration evidently_profile.EvidentlyProfileConfig : Config class for Evidently profile steps. evidently_profile.EvidentlyProfileStep : Simple step implementation which implements Evidently's functionality for evidently_visualizer.EvidentlyVisualizer : The implementation of an Evidently Visualizer. facets.FacetsIntegration : Definition of Facet integration facet_statistics_visualizer.FacetStatisticsVisualizer : The base implementation of a ZenML Visualizer. gcp.GcpIntegration : Definition of Google Cloud Platform integration for ZenML. gcp_artifact_store.GCPArtifactStore : Artifact Store for Google Cloud Storage based artifacts. gcs_plugin.ZenGCS : Filesystem that delegates to Google Cloud Store using gcsfs. graphviz.GraphvizIntegration : Definition of Graphviz integration for ZenML. pipeline_run_dag_visualizer.PipelineRunDagVisualizer : Visualize the lineage of runs in a pipeline. integration.Integration : Base class for integration in ZenML integration.IntegrationMeta : Metaclass responsible for registering different Integration kubeflow.KubeflowIntegration : Definition of Kubeflow Integration for ZenML. kubeflow_metadata_store.KubeflowMetadataStore : Kubeflow MySQL backend for ZenML metadata store. kubeflow_component.KubeflowComponent : Base component for all Kubeflow pipelines TFX components. kubeflow_dag_runner.KubeflowDagRunner : Kubeflow Pipelines runner. kubeflow_dag_runner.KubeflowDagRunnerConfig : Runtime configuration parameters specific to execution on Kubeflow. kubeflow_orchestrator.KubeflowOrchestrator : Orchestrator responsible for running pipelines using Kubeflow. mlflow.MlflowIntegration : Definition of Plotly integration for ZenML. plotly.PlotlyIntegration : Definition of Plotly integration for ZenML. pipeline_lineage_visualizer.PipelineLineageVisualizer : Visualize the lineage of runs in a pipeline using plotly. pytorch.PytorchIntegration : Definition of PyTorch integration for ZenML. pytorch_materializer.PyTorchMaterializer : Materializer to read/write Pytorch models. pytorch_types.TorchDict : A type of dict that represents saving a model. pytorch_lightning.PytorchLightningIntegration : Definition of PyTorch Lightning integration for ZenML. pytorch_lightning_materializer.PyTorchLightningMaterializer : Materializer to read/write Pytorch models. registry.IntegrationRegistry : Registry to keep track of ZenML Integrations sklearn.SklearnIntegration : Definition of sklearn integration for ZenML. sklearn_materializer.SklearnMaterializer : Materializer to read data to and from sklearn. sklearn_evaluator.SklearnEvaluator : A simple step implementation which utilizes sklearn to evaluate the sklearn_evaluator.SklearnEvaluatorConfig : Config class for the sklearn evaluator sklearn_splitter.SklearnSplitter : A simple step implementation which utilizes sklearn to split a given sklearn_splitter.SklearnSplitterConfig : Config class for the sklearn splitter sklearn_standard_scaler.SklearnStandardScaler : Simple step implementation which utilizes the StandardScaler from sklearn sklearn_standard_scaler.SklearnStandardScalerConfig : Config class for the sklearn standard scaler tensorflow.TensorflowIntegration : Definition of Tensorflow integration for ZenML. keras_materializer.KerasMaterializer : Materializer to read/write Keras models. tf_dataset_materializer.TensorflowDatasetMaterializer : Materializer to read data to and from tf.data.Dataset. tensorflow_trainer.TensorflowBinaryClassifier : Simple step implementation which creates a simple tensorflow feedforward tensorflow_trainer.TensorflowBinaryClassifierConfig : Config class for the tensorflow trainer io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation fileio_registry.FileIORegistry : Registry of pluggable filesystem implementations used in TFX components. filesystem.FileSystemMeta : Metaclass which is responsible for registering the defined filesystem filesystem.Filesystem : Abstract Filesystem class. filesystem.NotFoundError : Auxiliary not found error logger.CustomFormatter : Formats logs according to custom specifications. base_materializer.BaseMaterializer : Base Materializer to realize artifact data. base_materializer.BaseMaterializerMeta : Metaclass responsible for registering different BaseMaterializer beam_materializer.BeamMaterializer : Materializer to read data to and from beam. built_in_materializer.BuiltInMaterializer : Read/Write JSON files. default_materializer_registry.MaterializerRegistry : Matches a python type to a default materializer. numpy_materializer.NumpyMaterializer : Materializer to read data to and from pandas. pandas_materializer.PandasMaterializer : Materializer to read data to and from pandas. base_metadata_store.BaseMetadataStore : Metadata store base class to track metadata of zenml first class mysql_metadata_store.MySQLMetadataStore : MySQL backend for ZenML metadata store. sqlite_metadata_store.SQLiteMetadataStore : SQLite backend for ZenML metadata store. base_orchestrator.BaseOrchestrator : Base Orchestrator class to orchestrate ZenML pipelines. local_dag_runner.LocalDagRunner : Local TFX DAG runner. local_orchestrator.LocalOrchestrator : Orchestrator responsible for running pipelines locally. base_pipeline.BasePipeline : Abstract base class for all ZenML pipelines. base_pipeline.BasePipelineMeta : Pipeline Metaclass responsible for validating the pipeline definition. training_pipeline.TrainingPipeline : Class for the classic training pipeline implementation artifact.ArtifactView : Post-execution artifact class which can be used to read pipeline.PipelineView : Post-execution pipeline class which can be used to query pipeline_run.PipelineRunView : Post-execution pipeline run class which can be used to query step.StepView : Post-execution step class which can be used to query base_stack.BaseStack : Base stack for ZenML. base_step.BaseStep : Abstract base class for all ZenML steps. base_step.BaseStepMeta : Metaclass for BaseStep . base_step_config.BaseStepConfig : Base configuration class to pass execution params into a step. pandas_analyzer.PandasAnalyzer : Simple step implementation which analyzes a given pd.DataFrame pandas_analyzer.PandasAnalyzerConfig : Config class for the PandasAnalyzer Config pandas_datasource.PandasDatasource : Simple step implementation to ingest from a csv file using pandas pandas_datasource.PandasDatasourceConfig : Config class for the pandas csv datasource step_context.StepContext : Provides additional context inside a step function. step_context.StepContextOutput : Tuple containing materializer class and artifact for a step output. base_analyzer_step.BaseAnalyzerConfig : Base class for analyzer step configurations base_analyzer_step.BaseAnalyzerStep : Base step implementation for any analyzer step implementation on ZenML base_datasource_step.BaseDatasourceConfig : Base class for datasource configs to inherit from base_datasource_step.BaseDatasourceStep : Base step implementation for any datasource step implementation on ZenML base_drift_detection_step.BaseDriftDetectionConfig : Base class for drift detection step configurations base_drift_detection_step.BaseDriftDetectionStep : Base step implementation for any drift detection step implementation base_evaluator_step.BaseEvaluatorConfig : Base class for evaluator step configurations base_evaluator_step.BaseEvaluatorStep : Base step implementation for any evaluator step implementation on ZenML base_preprocessor_step.BasePreprocessorConfig : Base class for Preprocessor step configurations base_preprocessor_step.BasePreprocessorStep : Base step implementation for any Preprocessor step implementation on base_split_step.BaseSplitStep : Base step implementation for any split step implementation on ZenML base_split_step.BaseSplitStepConfig : Base class for split configs to inherit from base_trainer_step.BaseTrainerConfig : Base class for Trainer step configurations base_trainer_step.BaseTrainerStep : Base step implementation for any Trainer step implementation on step_output.Output : A named tuple with a default name that cannot be overridden. base_pipeline_run_visualizer.BasePipelineRunVisualizer : The base implementation of a ZenML Pipeline Run Visualizer. base_pipeline_visualizer.BasePipelineVisualizer : The base implementation of a ZenML Pipeline Visualizer. base_step_visualizer.BaseStepVisualizer : The base implementation of a ZenML Step Visualizer. base_visualizer.BaseVisualizer : Base class for all ZenML Visualizers. Functions example.check_for_version_mismatch utils.activate_integrations : Decorator that activates all ZenML integrations. utils.confirmation : Echo a confirmation string on the CLI. utils.declare : Echo a declaration on the CLI. utils.error : Echo an error string on the CLI. utils.format_component_list : Formats a list of components into a List of Dicts. This list of dicts utils.format_date : Format a date into a string. utils.install_package : Installs pypi package into the current environment with pip utils.parse_unknown_options : Parse unknown options from the CLI. utils.pretty_print : Pretty print an object on the CLI. utils.print_component_properties : Prints the properties of a component. utils.print_table : Echoes the list of dicts in a table format. The input object should be a utils.title : Echo a title formatted string on the CLI. utils.uninstall_package : Uninstalls pypi package from the current environment with pip utils.warning : Echo a warning string on the CLI. constants.handle_bool_env_var : Converts normal env var to boolean constants.handle_int_env_var : Converts normal env var to int mapping_utils.get_component_from_key : Given a key and a mapping, return an initialized component. mapping_utils.get_components_from_store : Returns a list of components from a store. mapping_utils.get_key_from_uuid : Return the key that points to a certain uuid in a mapping. utils.define_json_config_settings_source : Define a function to essentially deserialize a model from a serialized utils.generate_customise_sources : Generate a customise_sources function as defined here: container_entrypoint.main : Runs a single step defined by the command line arguments. docker_utils.build_docker_image : Builds a docker image. docker_utils.create_custom_build_context : Creates a docker build context. docker_utils.generate_dockerfile_contents : Generates a Dockerfile. docker_utils.get_current_environment_requirements : Returns a dict of package requirements for the environment that docker_utils.get_image_digest : Gets the digest of a docker image. docker_utils.push_docker_image : Pushes a docker image to a container registry. kubeflow_dag_runner.get_default_pipeline_operator_funcs : Returns a default list of pipeline operator functions. kubeflow_dag_runner.get_default_pod_labels : Returns the default pod label dict for Kubeflow. kubeflow_utils.replace_placeholder : Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. local_deployment_utils.check_prerequisites : Checks whether all prerequisites for a local kubeflow pipelines local_deployment_utils.create_k3d_cluster : Creates a K3D cluster. local_deployment_utils.delete_k3d_cluster : Deletes a K3D cluster with the given name. local_deployment_utils.deploy_kubeflow_pipelines : Deploys Kubeflow Pipelines. local_deployment_utils.k3d_cluster_exists : Checks whether there exists a K3D cluster with the given name. local_deployment_utils.kubeflow_pipelines_ready : Returns whether all Kubeflow Pipelines pods are ready. local_deployment_utils.start_kfp_ui_daemon : Starts a daemon process that forwards ports so the Kubeflow Pipelines local_deployment_utils.write_local_registry_yaml : Writes a K3D registry config file. mlflow_utils.enable_mlflow : Outer decorator function for the creation of a ZenML pipeline with mlflow mlflow_utils.enable_mlflow_init : Outer decorator function for extending the init method for pipelines mlflow_utils.enable_mlflow_run : Outer decorator function for extending the run method for pipelines mlflow_utils.local_mlflow_backend : Returns the local mlflow backend inside the global zenml directory mlflow_utils.setup_mlflow : Setup all mlflow related configurations. This includes specifying which digits.get_digits : Returns the digits dataset in the form of a tuple of numpy digits.get_digits_model : Creates a support vector classifier for digits dataset. utils.get_integration_for_module : Gets the integration class for a module inside an integration. utils.get_requirements_for_module : Gets requirements for a module inside an integration. fileio.append_file : Appends file_contents to file. fileio.convert_to_str : Converts a PathType to a str using UTF-8. fileio.copy : Copy a file from the source to the destination. fileio.copy_dir : Copies dir from source to destination. fileio.create_dir_if_not_exists : Creates directory if it does not exist. fileio.create_dir_recursive_if_not_exists : Creates directory recursively if it does not exist. fileio.create_file_if_not_exists : Creates file if it does not exist. fileio.file_exists : Returns True if the given path exists. fileio.find_files : Find files in a directory that match pattern. fileio.get_grandparent : Get grandparent of dir. fileio.get_parent : Get parent of dir. fileio.glob : Return the paths that match a glob pattern. fileio.is_dir : Returns whether the given path points to a directory. fileio.is_remote : Returns True if path exists remotely. fileio.is_root : Returns true if path has no parent in local filesystem. fileio.list_dir : Returns a list of files under dir. fileio.make_dirs : Make a directory at the given path, recursively creating parents. fileio.mkdir : Make a directory at the given path; parent directory must exist. fileio.move : Moves dir or file from source to destination. Can be used to rename. fileio.open : Open a file at the given path. fileio.remove : Remove the file at the given path. Dangerous operation. fileio.rename : Rename source file to destination file. fileio.resolve_relative_path : Takes relative path and resolves it absolutely. fileio.rm_dir : Deletes dir recursively. Dangerous operation. fileio.stat : Return the stat descriptor for a given file path. fileio.walk : Return an iterator that walks the contents of the given directory. utils.create_tarfile : Create a compressed representation of source_dir. utils.extract_tarfile : Extracts all files in a compressed tar file to output_dir. utils.get_global_config_directory : Returns the global config directory for ZenML. utils.get_zenml_config_dir : Recursive function to find the zenml config starting from path. utils.get_zenml_dir : Returns path to a ZenML repository directory. utils.is_gcs_path : Returns True if path is on Google Cloud Storage. utils.is_zenml_dir : Check if dir is a zenml dir or not. utils.read_file_contents_as_string : Reads contents of file. utils.write_file_contents_as_string : Writes contents of file. logger.get_console_handler : Get console handler for logging. logger.get_file_handler : Return a file handler for logging. logger.get_logger : Main function to get logger name,. logger.get_logging_level : Get logging level from the env variable. logger.init_logging : Initialize logging with default levels. logger.set_root_verbosity : Set the root verbosity. utils.create_tfx_pipeline : Creates a tfx pipeline from a ZenML pipeline. utils.execute_step : Executes a tfx component. pipeline_decorator.pipeline : Outer decorator function for the creation of a ZenML pipeline step_decorator.step : Outer decorator function for the creation of a ZenML step utils.do_types_match : Check whether type_a and type_b match. utils.generate_component_class : Generates a TFX component class for a ZenML step. utils.generate_component_spec_class : Generates a TFX component spec class for a ZenML step. analytics_utils.get_environment : Returns a string representing the execution environment of the pipeline. analytics_utils.get_segment_key : Get key for authorizing to Segment backend. analytics_utils.get_system_info : Returns system info as a dict. analytics_utils.in_docker : Returns: True if running in a Docker container, else False analytics_utils.in_google_colab : Returns: True if running in a Google Colab env, else False analytics_utils.in_paperspace_gradient : Returns: True if running in a Paperspace Gradient env, else False analytics_utils.parametrized : This is a meta-decorator, that is, a decorator for decorators. analytics_utils.layer : Internal layer analytics_utils.track_event : Track segment event if user opted-in. daemon.check_if_daemon_is_running : Checks whether a daemon process indicated by the PID file is running. daemon.run_as_daemon : Runs a function as a daemon process. daemon.stop_daemon : Stops a daemon process. networking_utils.find_available_port : Finds a local unoccupied port. networking_utils.port_available : Checks if a local port is available. source_utils.create_zenml_pin : Creates a ZenML pin for source pinning from release version. source_utils.get_absolute_path_from_module_source : Get a directory path from module source. source_utils.get_class_source_from_source : Gets class source from source, i.e. module.path@version, returns version. source_utils.get_module_source_from_class : Takes class input and returns module_source. If class is already string source_utils.get_module_source_from_file_path : Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py source_utils.get_module_source_from_source : Gets module source from source. E.g. some.module.file.class@version , source_utils.get_relative_path_from_module_source : Get a directory path from module, relative to root of repository. source_utils.import_class_by_path : Imports a class based on a given path source_utils.import_python_file : Imports a python file. source_utils.is_inside_repository : Returns whether a file is inside a zenml repository. source_utils.is_standard_pin : Returns True if pin is valid ZenML pin, else False. source_utils.is_standard_source : Returns True if source is a standard ZenML source. source_utils.is_third_party_module : Returns whether a file belongs to a third party package. source_utils.load_source_path_class : Loads a Python class from the source. source_utils.resolve_class : Resolves a class into a serializable source string. source_utils.resolve_standard_source : Creates a ZenML pin for source pinning from release version. string_utils.get_human_readable_filesize : Convert a file size in bytes into a human-readable string. string_utils.get_human_readable_time : Convert seconds into a human-readable string. yaml_utils.is_yaml : Returns True if file_path is YAML, else False yaml_utils.read_json : Read JSON on file path and returns contents as dict. yaml_utils.read_yaml : Read YAML on file path and returns contents as dict. yaml_utils.write_json : Write contents as JSON format to file_path. yaml_utils.write_yaml : Write contents as YAML format to file_path. This file was automatically generated via lazydocs .","title":"Overview"},{"location":"api_docs/OVERVIEW/#api-overview","text":"","title":"API Overview"},{"location":"api_docs/OVERVIEW/#modules","text":"artifact_stores : An artifact store is a place where artifacts are stored. These artifacts may artifact_stores.base_artifact_store : Definition of an Artifact Store artifact_stores.local_artifact_store artifacts : Artifacts are the data that power your experimentation and model training. It is artifacts.base_artifact : The below code is copied from the TFX source repo with minor changes. artifacts.constants artifacts.data_analysis_artifact artifacts.data_artifact artifacts.model_artifact artifacts.schema_artifact artifacts.statistics_artifact artifacts.type_registry cli : ZenML CLI cli.artifact_store cli.base cli.cli : .. currentmodule:: ce_cli.cli cli.config : CLI for manipulating ZenML local and global config file. cli.container_registry cli.example cli.integration cli.metadata_store cli.orchestrator cli.pipeline : CLI to interact with pipelines. cli.stack : CLI for manipulating ZenML local and global config file. cli.utils cli.version config : The config module contains classes and functions that manage user-specific config.config_keys config.constants config.global_config : Global config for the ZenML installation. constants container_registries container_registries.base_container_registry : Base class for all container registries. core : The core module is where all the base ZenML functionality is defined, core.base_component core.component_factory : Factory to register all components. core.constants core.git_wrapper : Wrapper class to handle Git integration core.local_service core.mapping_utils core.repo : Base ZenML repository core.utils enums exceptions : ZenML specific exception definitions integrations : The ZenML integrations module contains sub-modules for each integration that we integrations.airflow : The Airflow integration sub-module powers an alternative to the local integrations.airflow.orchestrators integrations.airflow.orchestrators.airflow_component : Definition for Airflow component for TFX. integrations.airflow.orchestrators.airflow_dag_runner : Definition of Airflow TFX runner. This is an unmodified copy from the TFX integrations.airflow.orchestrators.airflow_orchestrator integrations.constants integrations.dash integrations.dash.visualizers integrations.dash.visualizers.pipeline_run_lineage_visualizer integrations.evidently : The Evidently integration provides a way to monitor your models in production. integrations.evidently.steps integrations.evidently.steps.evidently_profile integrations.evidently.visualizers integrations.evidently.visualizers.evidently_visualizer integrations.facets : The Facets integration provides a simple way to visualize post-execution objects integrations.facets.visualizers integrations.facets.visualizers.facet_statistics_visualizer integrations.gcp : The GCP integration submodule provides a way to run ZenML pipelines in a cloud integrations.gcp.artifact_stores integrations.gcp.artifact_stores.gcp_artifact_store integrations.gcp.io integrations.gcp.io.gcs_plugin : Plugin which is created to add Google Cloud Store support to ZenML. integrations.graphviz integrations.graphviz.visualizers integrations.graphviz.visualizers.pipeline_run_dag_visualizer integrations.integration integrations.kubeflow : The Kubeflow integration sub-module powers an alternative to the local integrations.kubeflow.container_entrypoint : Main entrypoint for containers with Kubeflow TFX component executors. integrations.kubeflow.docker_utils integrations.kubeflow.metadata integrations.kubeflow.metadata.kubeflow_metadata_store integrations.kubeflow.orchestrators integrations.kubeflow.orchestrators.kubeflow_component : Kubeflow Pipelines based implementation of TFX components. integrations.kubeflow.orchestrators.kubeflow_dag_runner : The below code is copied from the TFX source repo with minor changes. integrations.kubeflow.orchestrators.kubeflow_orchestrator integrations.kubeflow.orchestrators.kubeflow_utils : Common utility for Kubeflow-based orchestrator. integrations.kubeflow.orchestrators.local_deployment_utils integrations.mlflow : The mlflow integrations currently enables you to use mlflow tracking as a integrations.mlflow.mlflow_utils integrations.plotly integrations.plotly.visualizers integrations.plotly.visualizers.pipeline_lineage_visualizer integrations.pytorch integrations.pytorch.materializers integrations.pytorch.materializers.pytorch_materializer integrations.pytorch.materializers.pytorch_types integrations.pytorch_lightning integrations.pytorch_lightning.materializers integrations.pytorch_lightning.materializers.pytorch_lightning_materializer integrations.registry integrations.sklearn integrations.sklearn.helpers integrations.sklearn.helpers.digits integrations.sklearn.materializers integrations.sklearn.materializers.sklearn_materializer integrations.sklearn.steps integrations.sklearn.steps.sklearn_evaluator integrations.sklearn.steps.sklearn_splitter integrations.sklearn.steps.sklearn_standard_scaler integrations.tensorflow integrations.tensorflow.materializers integrations.tensorflow.materializers.keras_materializer integrations.tensorflow.materializers.tf_dataset_materializer integrations.tensorflow.steps integrations.tensorflow.steps.tensorflow_trainer integrations.utils io : The io module handles file operations for the ZenML package. It offers a io.fileio io.fileio_registry : Filesystem registry managing filesystem plugins. io.filesystem io.utils logger materializers : Materializers are used to convert a ZenML artifact into a specific format. They materializers.base_materializer materializers.beam_materializer materializers.built_in_materializer materializers.default_materializer_registry materializers.numpy_materializer materializers.pandas_materializer metadata_stores : The configuration of each pipeline, step, backend, and produced artifacts are metadata_stores.base_metadata_store metadata_stores.mysql_metadata_store metadata_stores.sqlite_metadata_store orchestrators : An orchestrator is a special kind of backend that manages the running of each orchestrators.base_orchestrator orchestrators.local orchestrators.local.local_dag_runner : Inspired by local dag runner implementation orchestrators.local.local_orchestrator orchestrators.utils pipelines : A ZenML pipeline is a sequence of tasks that execute in a specific order and pipelines.base_pipeline pipelines.builtin_pipelines pipelines.builtin_pipelines.training_pipeline pipelines.pipeline_decorator post_execution : After executing a pipeline, the user needs to be able to fetch it from history post_execution.artifact post_execution.pipeline post_execution.pipeline_run post_execution.step stacks : A stack is made up of the following three core components: an Artifact Store, a stacks.base_stack stacks.constants steps : A step is a single piece or stage of a ZenML pipeline. Think of each step as steps.base_step steps.base_step_config steps.builtin_steps steps.builtin_steps.pandas_analyzer steps.builtin_steps.pandas_datasource steps.step_context steps.step_decorator steps.step_interfaces steps.step_interfaces.base_analyzer_step steps.step_interfaces.base_datasource_step steps.step_interfaces.base_drift_detection_step steps.step_interfaces.base_evaluator_step steps.step_interfaces.base_preprocessor_step steps.step_interfaces.base_split_step steps.step_interfaces.base_trainer_step steps.step_output steps.utils : The collection of utility functions/classes are inspired by their original utils : The utils module contains utility functions handling analytics, reading and utils.analytics_utils : Analytics code for ZenML utils.daemon : Utility functions to start/stop daemon processes. utils.networking_utils utils.source_utils : These utils are predicated on the following definitions: utils.string_utils utils.yaml_utils visualizers : The visualizers module offers a way of constructing and displaying visualizers.base_pipeline_run_visualizer visualizers.base_pipeline_visualizer visualizers.base_step_visualizer visualizers.base_visualizer","title":"Modules"},{"location":"api_docs/OVERVIEW/#classes","text":"base_artifact_store.BaseArtifactStore : Base class for all ZenML Artifact Store. local_artifact_store.LocalArtifactStore : Artifact Store for local artifacts. base_artifact.BaseArtifact : Base class for all ZenML artifacts. data_analysis_artifact.DataAnalysisArtifact : Class for all ZenML data analysis artifacts. data_artifact.DataArtifact : Class for all ZenML data artifacts. model_artifact.ModelArtifact : Class for all ZenML model artifacts. schema_artifact.SchemaArtifact : Class for all ZenML schema artifacts. statistics_artifact.StatisticsArtifact : Class for all ZenML statistics artifacts. type_registry.ArtifactTypeRegistry : A registry to keep track of which datatypes map to which artifact example.Example : Class for all example objects. example.ExamplesRepo : Class for the examples repository object. example.GitExamplesHandler : Class for the GitExamplesHandler that interfaces with the CLI tool. example.LocalExample : Class to encapsulate all properties and methods of the local example config_keys.ConfigKeys : Class to validate dictionary configurations. config_keys.PipelineConfigurationKeys : Keys for a pipeline configuration dict. config_keys.StepConfigurationKeys : Keys for a step configuration dict. global_config.GlobalConfig : Class definition for the global config. base_container_registry.BaseContainerRegistry : Base class for all ZenML container registries. base_component.BaseComponent : Class definition for the base config. component_factory.ComponentFactory : Definition of ComponentFactory to track all BaseComponent subclasses. git_wrapper.GitWrapper : Wrapper class for Git. local_service.LocalService : Definition of a local service that keeps track of all ZenML mapping_utils.UUIDSourceTuple : Container used to store UUID and source information repo.Repository : ZenML repository definition. enums.ArtifactStoreTypes : All supported Artifact Store types. enums.ExecutionStatus : Enum that represents the current status of a step or pipeline run. enums.LoggingLevels : Enum for logging levels. enums.MLMetadataTypes : All supported ML Metadata types. enums.OrchestratorTypes : All supported Orchestrator types enums.StackTypes : All supported Stack types. exceptions.AlreadyExistsException : Raises exception when the name already exist in the system but an exceptions.ArtifactInterfaceError : Raises exception when interacting with the Artifact interface exceptions.DoesNotExistException : Raises exception when the entity does not exist in the system but an exceptions.DuplicateRunNameError : Raises exception when a run with the same name already exists. exceptions.EmptyDatasourceException : Raises exception when a datasource data is accessed without running exceptions.GitException : Raises exception when a problem occurs in git resolution. exceptions.InitializationException : Raises exception when a function is run before zenml initialization. exceptions.IntegrationError : Raises exceptions when a requested integration can not be activated. exceptions.MissingStepParameterError : Raises exceptions when a step parameter is missing when running a exceptions.PipelineConfigurationError : Raises exceptions when a pipeline configuration contains exceptions.PipelineInterfaceError : Raises exception when interacting with the Pipeline interface exceptions.PipelineNotSucceededException : Raises exception when trying to fetch artifacts from a not succeeded exceptions.StepContextError : Raises exception when interacting with a StepContext exceptions.StepInterfaceError : Raises exception when interacting with the Step interface airflow.AirflowIntegration : Definition of Airflow Integration for ZenML. airflow_component.AirflowComponent : Airflow-specific TFX Component. airflow_dag_runner.AirflowDagRunner : Tfx runner on Airflow. airflow_dag_runner.AirflowPipelineConfig : Pipeline config for AirflowDagRunner. airflow_orchestrator.AirflowOrchestrator : Orchestrator responsible for running pipelines using Airflow. dash.DashIntegration : Definition of Dash integration for ZenML. pipeline_run_lineage_visualizer.PipelineRunLineageVisualizer : Implementation of a lineage diagram via the [dash]( evidently.EvidentlyIntegration : Definition of Evidently integration evidently_profile.EvidentlyProfileConfig : Config class for Evidently profile steps. evidently_profile.EvidentlyProfileStep : Simple step implementation which implements Evidently's functionality for evidently_visualizer.EvidentlyVisualizer : The implementation of an Evidently Visualizer. facets.FacetsIntegration : Definition of Facet integration facet_statistics_visualizer.FacetStatisticsVisualizer : The base implementation of a ZenML Visualizer. gcp.GcpIntegration : Definition of Google Cloud Platform integration for ZenML. gcp_artifact_store.GCPArtifactStore : Artifact Store for Google Cloud Storage based artifacts. gcs_plugin.ZenGCS : Filesystem that delegates to Google Cloud Store using gcsfs. graphviz.GraphvizIntegration : Definition of Graphviz integration for ZenML. pipeline_run_dag_visualizer.PipelineRunDagVisualizer : Visualize the lineage of runs in a pipeline. integration.Integration : Base class for integration in ZenML integration.IntegrationMeta : Metaclass responsible for registering different Integration kubeflow.KubeflowIntegration : Definition of Kubeflow Integration for ZenML. kubeflow_metadata_store.KubeflowMetadataStore : Kubeflow MySQL backend for ZenML metadata store. kubeflow_component.KubeflowComponent : Base component for all Kubeflow pipelines TFX components. kubeflow_dag_runner.KubeflowDagRunner : Kubeflow Pipelines runner. kubeflow_dag_runner.KubeflowDagRunnerConfig : Runtime configuration parameters specific to execution on Kubeflow. kubeflow_orchestrator.KubeflowOrchestrator : Orchestrator responsible for running pipelines using Kubeflow. mlflow.MlflowIntegration : Definition of Plotly integration for ZenML. plotly.PlotlyIntegration : Definition of Plotly integration for ZenML. pipeline_lineage_visualizer.PipelineLineageVisualizer : Visualize the lineage of runs in a pipeline using plotly. pytorch.PytorchIntegration : Definition of PyTorch integration for ZenML. pytorch_materializer.PyTorchMaterializer : Materializer to read/write Pytorch models. pytorch_types.TorchDict : A type of dict that represents saving a model. pytorch_lightning.PytorchLightningIntegration : Definition of PyTorch Lightning integration for ZenML. pytorch_lightning_materializer.PyTorchLightningMaterializer : Materializer to read/write Pytorch models. registry.IntegrationRegistry : Registry to keep track of ZenML Integrations sklearn.SklearnIntegration : Definition of sklearn integration for ZenML. sklearn_materializer.SklearnMaterializer : Materializer to read data to and from sklearn. sklearn_evaluator.SklearnEvaluator : A simple step implementation which utilizes sklearn to evaluate the sklearn_evaluator.SklearnEvaluatorConfig : Config class for the sklearn evaluator sklearn_splitter.SklearnSplitter : A simple step implementation which utilizes sklearn to split a given sklearn_splitter.SklearnSplitterConfig : Config class for the sklearn splitter sklearn_standard_scaler.SklearnStandardScaler : Simple step implementation which utilizes the StandardScaler from sklearn sklearn_standard_scaler.SklearnStandardScalerConfig : Config class for the sklearn standard scaler tensorflow.TensorflowIntegration : Definition of Tensorflow integration for ZenML. keras_materializer.KerasMaterializer : Materializer to read/write Keras models. tf_dataset_materializer.TensorflowDatasetMaterializer : Materializer to read data to and from tf.data.Dataset. tensorflow_trainer.TensorflowBinaryClassifier : Simple step implementation which creates a simple tensorflow feedforward tensorflow_trainer.TensorflowBinaryClassifierConfig : Config class for the tensorflow trainer io.BufferedIOBase : Base class for buffered IO objects. io.IOBase : The abstract base class for all I/O classes, acting on streams of io.RawIOBase : Base class for raw binary I/O. io.TextIOBase : Base class for text I/O. io.UnsupportedOperation fileio_registry.FileIORegistry : Registry of pluggable filesystem implementations used in TFX components. filesystem.FileSystemMeta : Metaclass which is responsible for registering the defined filesystem filesystem.Filesystem : Abstract Filesystem class. filesystem.NotFoundError : Auxiliary not found error logger.CustomFormatter : Formats logs according to custom specifications. base_materializer.BaseMaterializer : Base Materializer to realize artifact data. base_materializer.BaseMaterializerMeta : Metaclass responsible for registering different BaseMaterializer beam_materializer.BeamMaterializer : Materializer to read data to and from beam. built_in_materializer.BuiltInMaterializer : Read/Write JSON files. default_materializer_registry.MaterializerRegistry : Matches a python type to a default materializer. numpy_materializer.NumpyMaterializer : Materializer to read data to and from pandas. pandas_materializer.PandasMaterializer : Materializer to read data to and from pandas. base_metadata_store.BaseMetadataStore : Metadata store base class to track metadata of zenml first class mysql_metadata_store.MySQLMetadataStore : MySQL backend for ZenML metadata store. sqlite_metadata_store.SQLiteMetadataStore : SQLite backend for ZenML metadata store. base_orchestrator.BaseOrchestrator : Base Orchestrator class to orchestrate ZenML pipelines. local_dag_runner.LocalDagRunner : Local TFX DAG runner. local_orchestrator.LocalOrchestrator : Orchestrator responsible for running pipelines locally. base_pipeline.BasePipeline : Abstract base class for all ZenML pipelines. base_pipeline.BasePipelineMeta : Pipeline Metaclass responsible for validating the pipeline definition. training_pipeline.TrainingPipeline : Class for the classic training pipeline implementation artifact.ArtifactView : Post-execution artifact class which can be used to read pipeline.PipelineView : Post-execution pipeline class which can be used to query pipeline_run.PipelineRunView : Post-execution pipeline run class which can be used to query step.StepView : Post-execution step class which can be used to query base_stack.BaseStack : Base stack for ZenML. base_step.BaseStep : Abstract base class for all ZenML steps. base_step.BaseStepMeta : Metaclass for BaseStep . base_step_config.BaseStepConfig : Base configuration class to pass execution params into a step. pandas_analyzer.PandasAnalyzer : Simple step implementation which analyzes a given pd.DataFrame pandas_analyzer.PandasAnalyzerConfig : Config class for the PandasAnalyzer Config pandas_datasource.PandasDatasource : Simple step implementation to ingest from a csv file using pandas pandas_datasource.PandasDatasourceConfig : Config class for the pandas csv datasource step_context.StepContext : Provides additional context inside a step function. step_context.StepContextOutput : Tuple containing materializer class and artifact for a step output. base_analyzer_step.BaseAnalyzerConfig : Base class for analyzer step configurations base_analyzer_step.BaseAnalyzerStep : Base step implementation for any analyzer step implementation on ZenML base_datasource_step.BaseDatasourceConfig : Base class for datasource configs to inherit from base_datasource_step.BaseDatasourceStep : Base step implementation for any datasource step implementation on ZenML base_drift_detection_step.BaseDriftDetectionConfig : Base class for drift detection step configurations base_drift_detection_step.BaseDriftDetectionStep : Base step implementation for any drift detection step implementation base_evaluator_step.BaseEvaluatorConfig : Base class for evaluator step configurations base_evaluator_step.BaseEvaluatorStep : Base step implementation for any evaluator step implementation on ZenML base_preprocessor_step.BasePreprocessorConfig : Base class for Preprocessor step configurations base_preprocessor_step.BasePreprocessorStep : Base step implementation for any Preprocessor step implementation on base_split_step.BaseSplitStep : Base step implementation for any split step implementation on ZenML base_split_step.BaseSplitStepConfig : Base class for split configs to inherit from base_trainer_step.BaseTrainerConfig : Base class for Trainer step configurations base_trainer_step.BaseTrainerStep : Base step implementation for any Trainer step implementation on step_output.Output : A named tuple with a default name that cannot be overridden. base_pipeline_run_visualizer.BasePipelineRunVisualizer : The base implementation of a ZenML Pipeline Run Visualizer. base_pipeline_visualizer.BasePipelineVisualizer : The base implementation of a ZenML Pipeline Visualizer. base_step_visualizer.BaseStepVisualizer : The base implementation of a ZenML Step Visualizer. base_visualizer.BaseVisualizer : Base class for all ZenML Visualizers.","title":"Classes"},{"location":"api_docs/OVERVIEW/#functions","text":"example.check_for_version_mismatch utils.activate_integrations : Decorator that activates all ZenML integrations. utils.confirmation : Echo a confirmation string on the CLI. utils.declare : Echo a declaration on the CLI. utils.error : Echo an error string on the CLI. utils.format_component_list : Formats a list of components into a List of Dicts. This list of dicts utils.format_date : Format a date into a string. utils.install_package : Installs pypi package into the current environment with pip utils.parse_unknown_options : Parse unknown options from the CLI. utils.pretty_print : Pretty print an object on the CLI. utils.print_component_properties : Prints the properties of a component. utils.print_table : Echoes the list of dicts in a table format. The input object should be a utils.title : Echo a title formatted string on the CLI. utils.uninstall_package : Uninstalls pypi package from the current environment with pip utils.warning : Echo a warning string on the CLI. constants.handle_bool_env_var : Converts normal env var to boolean constants.handle_int_env_var : Converts normal env var to int mapping_utils.get_component_from_key : Given a key and a mapping, return an initialized component. mapping_utils.get_components_from_store : Returns a list of components from a store. mapping_utils.get_key_from_uuid : Return the key that points to a certain uuid in a mapping. utils.define_json_config_settings_source : Define a function to essentially deserialize a model from a serialized utils.generate_customise_sources : Generate a customise_sources function as defined here: container_entrypoint.main : Runs a single step defined by the command line arguments. docker_utils.build_docker_image : Builds a docker image. docker_utils.create_custom_build_context : Creates a docker build context. docker_utils.generate_dockerfile_contents : Generates a Dockerfile. docker_utils.get_current_environment_requirements : Returns a dict of package requirements for the environment that docker_utils.get_image_digest : Gets the digest of a docker image. docker_utils.push_docker_image : Pushes a docker image to a container registry. kubeflow_dag_runner.get_default_pipeline_operator_funcs : Returns a default list of pipeline operator functions. kubeflow_dag_runner.get_default_pod_labels : Returns the default pod label dict for Kubeflow. kubeflow_utils.replace_placeholder : Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. local_deployment_utils.check_prerequisites : Checks whether all prerequisites for a local kubeflow pipelines local_deployment_utils.create_k3d_cluster : Creates a K3D cluster. local_deployment_utils.delete_k3d_cluster : Deletes a K3D cluster with the given name. local_deployment_utils.deploy_kubeflow_pipelines : Deploys Kubeflow Pipelines. local_deployment_utils.k3d_cluster_exists : Checks whether there exists a K3D cluster with the given name. local_deployment_utils.kubeflow_pipelines_ready : Returns whether all Kubeflow Pipelines pods are ready. local_deployment_utils.start_kfp_ui_daemon : Starts a daemon process that forwards ports so the Kubeflow Pipelines local_deployment_utils.write_local_registry_yaml : Writes a K3D registry config file. mlflow_utils.enable_mlflow : Outer decorator function for the creation of a ZenML pipeline with mlflow mlflow_utils.enable_mlflow_init : Outer decorator function for extending the init method for pipelines mlflow_utils.enable_mlflow_run : Outer decorator function for extending the run method for pipelines mlflow_utils.local_mlflow_backend : Returns the local mlflow backend inside the global zenml directory mlflow_utils.setup_mlflow : Setup all mlflow related configurations. This includes specifying which digits.get_digits : Returns the digits dataset in the form of a tuple of numpy digits.get_digits_model : Creates a support vector classifier for digits dataset. utils.get_integration_for_module : Gets the integration class for a module inside an integration. utils.get_requirements_for_module : Gets requirements for a module inside an integration. fileio.append_file : Appends file_contents to file. fileio.convert_to_str : Converts a PathType to a str using UTF-8. fileio.copy : Copy a file from the source to the destination. fileio.copy_dir : Copies dir from source to destination. fileio.create_dir_if_not_exists : Creates directory if it does not exist. fileio.create_dir_recursive_if_not_exists : Creates directory recursively if it does not exist. fileio.create_file_if_not_exists : Creates file if it does not exist. fileio.file_exists : Returns True if the given path exists. fileio.find_files : Find files in a directory that match pattern. fileio.get_grandparent : Get grandparent of dir. fileio.get_parent : Get parent of dir. fileio.glob : Return the paths that match a glob pattern. fileio.is_dir : Returns whether the given path points to a directory. fileio.is_remote : Returns True if path exists remotely. fileio.is_root : Returns true if path has no parent in local filesystem. fileio.list_dir : Returns a list of files under dir. fileio.make_dirs : Make a directory at the given path, recursively creating parents. fileio.mkdir : Make a directory at the given path; parent directory must exist. fileio.move : Moves dir or file from source to destination. Can be used to rename. fileio.open : Open a file at the given path. fileio.remove : Remove the file at the given path. Dangerous operation. fileio.rename : Rename source file to destination file. fileio.resolve_relative_path : Takes relative path and resolves it absolutely. fileio.rm_dir : Deletes dir recursively. Dangerous operation. fileio.stat : Return the stat descriptor for a given file path. fileio.walk : Return an iterator that walks the contents of the given directory. utils.create_tarfile : Create a compressed representation of source_dir. utils.extract_tarfile : Extracts all files in a compressed tar file to output_dir. utils.get_global_config_directory : Returns the global config directory for ZenML. utils.get_zenml_config_dir : Recursive function to find the zenml config starting from path. utils.get_zenml_dir : Returns path to a ZenML repository directory. utils.is_gcs_path : Returns True if path is on Google Cloud Storage. utils.is_zenml_dir : Check if dir is a zenml dir or not. utils.read_file_contents_as_string : Reads contents of file. utils.write_file_contents_as_string : Writes contents of file. logger.get_console_handler : Get console handler for logging. logger.get_file_handler : Return a file handler for logging. logger.get_logger : Main function to get logger name,. logger.get_logging_level : Get logging level from the env variable. logger.init_logging : Initialize logging with default levels. logger.set_root_verbosity : Set the root verbosity. utils.create_tfx_pipeline : Creates a tfx pipeline from a ZenML pipeline. utils.execute_step : Executes a tfx component. pipeline_decorator.pipeline : Outer decorator function for the creation of a ZenML pipeline step_decorator.step : Outer decorator function for the creation of a ZenML step utils.do_types_match : Check whether type_a and type_b match. utils.generate_component_class : Generates a TFX component class for a ZenML step. utils.generate_component_spec_class : Generates a TFX component spec class for a ZenML step. analytics_utils.get_environment : Returns a string representing the execution environment of the pipeline. analytics_utils.get_segment_key : Get key for authorizing to Segment backend. analytics_utils.get_system_info : Returns system info as a dict. analytics_utils.in_docker : Returns: True if running in a Docker container, else False analytics_utils.in_google_colab : Returns: True if running in a Google Colab env, else False analytics_utils.in_paperspace_gradient : Returns: True if running in a Paperspace Gradient env, else False analytics_utils.parametrized : This is a meta-decorator, that is, a decorator for decorators. analytics_utils.layer : Internal layer analytics_utils.track_event : Track segment event if user opted-in. daemon.check_if_daemon_is_running : Checks whether a daemon process indicated by the PID file is running. daemon.run_as_daemon : Runs a function as a daemon process. daemon.stop_daemon : Stops a daemon process. networking_utils.find_available_port : Finds a local unoccupied port. networking_utils.port_available : Checks if a local port is available. source_utils.create_zenml_pin : Creates a ZenML pin for source pinning from release version. source_utils.get_absolute_path_from_module_source : Get a directory path from module source. source_utils.get_class_source_from_source : Gets class source from source, i.e. module.path@version, returns version. source_utils.get_module_source_from_class : Takes class input and returns module_source. If class is already string source_utils.get_module_source_from_file_path : Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py source_utils.get_module_source_from_source : Gets module source from source. E.g. some.module.file.class@version , source_utils.get_relative_path_from_module_source : Get a directory path from module, relative to root of repository. source_utils.import_class_by_path : Imports a class based on a given path source_utils.import_python_file : Imports a python file. source_utils.is_inside_repository : Returns whether a file is inside a zenml repository. source_utils.is_standard_pin : Returns True if pin is valid ZenML pin, else False. source_utils.is_standard_source : Returns True if source is a standard ZenML source. source_utils.is_third_party_module : Returns whether a file belongs to a third party package. source_utils.load_source_path_class : Loads a Python class from the source. source_utils.resolve_class : Resolves a class into a serializable source string. source_utils.resolve_standard_source : Creates a ZenML pin for source pinning from release version. string_utils.get_human_readable_filesize : Convert a file size in bytes into a human-readable string. string_utils.get_human_readable_time : Convert seconds into a human-readable string. yaml_utils.is_yaml : Returns True if file_path is YAML, else False yaml_utils.read_json : Read JSON on file path and returns contents as dict. yaml_utils.read_yaml : Read YAML on file path and returns contents as dict. yaml_utils.write_json : Write contents as JSON format to file_path. yaml_utils.write_yaml : Write contents as YAML format to file_path. This file was automatically generated via lazydocs .","title":"Functions"},{"location":"api_docs/artifact_stores.base_artifact_store/","text":"module artifact_stores.base_artifact_store Definition of an Artifact Store class BaseArtifactStore Base class for all ZenML Artifact Store. Every ZenML Artifact Store should override this class. method __init__ __init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseArtifactStore instance. Args: repo_path : Path to the repository of this artifact store. method get_component_name_from_uri get_component_name_from_uri(artifact_uri: str) \u2192 str Gets component name from artifact URI. Args: artifact_uri : URI to artifact. Returns: Name of the component. method resolve_uri_locally resolve_uri_locally(artifact_uri: str, path: Optional[str] = None) \u2192 str Takes a URI that points within the artifact store, downloads the URI locally, then returns local URI. Args: artifact_uri : uri to artifact. path : optional path to download to. If None, is inferred. Returns: Locally resolved uri. This file was automatically generated via lazydocs .","title":"Artifact stores.base artifact store"},{"location":"api_docs/artifact_stores.base_artifact_store/#module-artifact_storesbase_artifact_store","text":"Definition of an Artifact Store","title":"module artifact_stores.base_artifact_store"},{"location":"api_docs/artifact_stores.base_artifact_store/#class-baseartifactstore","text":"Base class for all ZenML Artifact Store. Every ZenML Artifact Store should override this class.","title":"class BaseArtifactStore"},{"location":"api_docs/artifact_stores.base_artifact_store/#method-__init__","text":"__init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseArtifactStore instance. Args: repo_path : Path to the repository of this artifact store.","title":"method __init__"},{"location":"api_docs/artifact_stores.base_artifact_store/#method-get_component_name_from_uri","text":"get_component_name_from_uri(artifact_uri: str) \u2192 str Gets component name from artifact URI. Args: artifact_uri : URI to artifact. Returns: Name of the component.","title":"method get_component_name_from_uri"},{"location":"api_docs/artifact_stores.base_artifact_store/#method-resolve_uri_locally","text":"resolve_uri_locally(artifact_uri: str, path: Optional[str] = None) \u2192 str Takes a URI that points within the artifact store, downloads the URI locally, then returns local URI. Args: artifact_uri : uri to artifact. path : optional path to download to. If None, is inferred. Returns: Locally resolved uri. This file was automatically generated via lazydocs .","title":"method resolve_uri_locally"},{"location":"api_docs/artifact_stores.local_artifact_store/","text":"module artifact_stores.local_artifact_store Global Variables REMOTE_FS_PREFIX class LocalArtifactStore Artifact Store for local artifacts. classmethod must_be_local_path must_be_local_path(v: str) \u2192 str Validates that the path is a local path. This file was automatically generated via lazydocs .","title":"Artifact stores.local artifact store"},{"location":"api_docs/artifact_stores.local_artifact_store/#module-artifact_storeslocal_artifact_store","text":"","title":"module artifact_stores.local_artifact_store"},{"location":"api_docs/artifact_stores.local_artifact_store/#global-variables","text":"REMOTE_FS_PREFIX","title":"Global Variables"},{"location":"api_docs/artifact_stores.local_artifact_store/#class-localartifactstore","text":"Artifact Store for local artifacts.","title":"class LocalArtifactStore"},{"location":"api_docs/artifact_stores.local_artifact_store/#classmethod-must_be_local_path","text":"must_be_local_path(v: str) \u2192 str Validates that the path is a local path. This file was automatically generated via lazydocs .","title":"classmethod must_be_local_path"},{"location":"api_docs/artifact_stores/","text":"module artifact_stores An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores . This file was automatically generated via lazydocs .","title":"Artifact stores"},{"location":"api_docs/artifact_stores/#module-artifact_stores","text":"An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores . This file was automatically generated via lazydocs .","title":"module artifact_stores"},{"location":"api_docs/artifacts.base_artifact/","text":"module artifacts.base_artifact The below code is copied from the TFX source repo with minor changes. All credits go to the TFX team for the core implementation Global Variables DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY class BaseArtifact Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: Upon creation, each artifact class needs to be given a unique TYPE_NAME. Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs. method __init__ __init__(*args: Any, **kwargs: Any) \u2192 None Init method for BaseArtifact property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. classmethod set_zenml_artifact_type set_zenml_artifact_type() \u2192 None Set the type of the artifact. This file was automatically generated via lazydocs .","title":"Artifacts.base artifact"},{"location":"api_docs/artifacts.base_artifact/#module-artifactsbase_artifact","text":"The below code is copied from the TFX source repo with minor changes. All credits go to the TFX team for the core implementation","title":"module artifacts.base_artifact"},{"location":"api_docs/artifacts.base_artifact/#global-variables","text":"DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY","title":"Global Variables"},{"location":"api_docs/artifacts.base_artifact/#class-baseartifact","text":"Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: Upon creation, each artifact class needs to be given a unique TYPE_NAME. Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs.","title":"class BaseArtifact"},{"location":"api_docs/artifacts.base_artifact/#method-__init__","text":"__init__(*args: Any, **kwargs: Any) \u2192 None Init method for BaseArtifact","title":"method __init__"},{"location":"api_docs/artifacts.base_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"api_docs/artifacts.base_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"api_docs/artifacts.base_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"api_docs/artifacts.base_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"api_docs/artifacts.base_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"api_docs/artifacts.base_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"api_docs/artifacts.base_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"api_docs/artifacts.base_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"api_docs/artifacts.base_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"api_docs/artifacts.base_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"api_docs/artifacts.base_artifact/#property-uri","text":"Artifact URI.","title":"property uri"},{"location":"api_docs/artifacts.base_artifact/#classmethod-set_zenml_artifact_type","text":"set_zenml_artifact_type() \u2192 None Set the type of the artifact. This file was automatically generated via lazydocs .","title":"classmethod set_zenml_artifact_type"},{"location":"api_docs/artifacts.constants/","text":"module artifacts.constants Global Variables DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY This file was automatically generated via lazydocs .","title":"Artifacts.constants"},{"location":"api_docs/artifacts.constants/#module-artifactsconstants","text":"","title":"module artifacts.constants"},{"location":"api_docs/artifacts.constants/#global-variables","text":"DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/artifacts.data_analysis_artifact/","text":"module artifacts.data_analysis_artifact class DataAnalysisArtifact Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc. property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. This file was automatically generated via lazydocs .","title":"Artifacts.data analysis artifact"},{"location":"api_docs/artifacts.data_analysis_artifact/#module-artifactsdata_analysis_artifact","text":"","title":"module artifacts.data_analysis_artifact"},{"location":"api_docs/artifacts.data_analysis_artifact/#class-dataanalysisartifact","text":"Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc.","title":"class DataAnalysisArtifact"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"api_docs/artifacts.data_analysis_artifact/#property-uri","text":"Artifact URI. This file was automatically generated via lazydocs .","title":"property uri"},{"location":"api_docs/artifacts.data_artifact/","text":"module artifacts.data_artifact class DataArtifact Class for all ZenML data artifacts. property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. This file was automatically generated via lazydocs .","title":"Artifacts.data artifact"},{"location":"api_docs/artifacts.data_artifact/#module-artifactsdata_artifact","text":"","title":"module artifacts.data_artifact"},{"location":"api_docs/artifacts.data_artifact/#class-dataartifact","text":"Class for all ZenML data artifacts.","title":"class DataArtifact"},{"location":"api_docs/artifacts.data_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"api_docs/artifacts.data_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"api_docs/artifacts.data_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"api_docs/artifacts.data_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"api_docs/artifacts.data_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"api_docs/artifacts.data_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"api_docs/artifacts.data_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"api_docs/artifacts.data_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"api_docs/artifacts.data_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"api_docs/artifacts.data_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"api_docs/artifacts.data_artifact/#property-uri","text":"Artifact URI. This file was automatically generated via lazydocs .","title":"property uri"},{"location":"api_docs/artifacts/","text":"module artifacts Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: .. code:: python // Some code def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer. This file was automatically generated via lazydocs .","title":"Artifacts"},{"location":"api_docs/artifacts/#module-artifacts","text":"Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: .. code:: python // Some code def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer. This file was automatically generated via lazydocs .","title":"module artifacts"},{"location":"api_docs/artifacts.model_artifact/","text":"module artifacts.model_artifact class ModelArtifact Class for all ZenML model artifacts. property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. This file was automatically generated via lazydocs .","title":"Artifacts.model artifact"},{"location":"api_docs/artifacts.model_artifact/#module-artifactsmodel_artifact","text":"","title":"module artifacts.model_artifact"},{"location":"api_docs/artifacts.model_artifact/#class-modelartifact","text":"Class for all ZenML model artifacts.","title":"class ModelArtifact"},{"location":"api_docs/artifacts.model_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"api_docs/artifacts.model_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"api_docs/artifacts.model_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"api_docs/artifacts.model_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"api_docs/artifacts.model_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"api_docs/artifacts.model_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"api_docs/artifacts.model_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"api_docs/artifacts.model_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"api_docs/artifacts.model_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"api_docs/artifacts.model_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"api_docs/artifacts.model_artifact/#property-uri","text":"Artifact URI. This file was automatically generated via lazydocs .","title":"property uri"},{"location":"api_docs/artifacts.schema_artifact/","text":"module artifacts.schema_artifact class SchemaArtifact Class for all ZenML schema artifacts. property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. This file was automatically generated via lazydocs .","title":"Artifacts.schema artifact"},{"location":"api_docs/artifacts.schema_artifact/#module-artifactsschema_artifact","text":"","title":"module artifacts.schema_artifact"},{"location":"api_docs/artifacts.schema_artifact/#class-schemaartifact","text":"Class for all ZenML schema artifacts.","title":"class SchemaArtifact"},{"location":"api_docs/artifacts.schema_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"api_docs/artifacts.schema_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"api_docs/artifacts.schema_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"api_docs/artifacts.schema_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"api_docs/artifacts.schema_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"api_docs/artifacts.schema_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"api_docs/artifacts.schema_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"api_docs/artifacts.schema_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"api_docs/artifacts.schema_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"api_docs/artifacts.schema_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"api_docs/artifacts.schema_artifact/#property-uri","text":"Artifact URI. This file was automatically generated via lazydocs .","title":"property uri"},{"location":"api_docs/artifacts.statistics_artifact/","text":"module artifacts.statistics_artifact class StatisticsArtifact Class for all ZenML statistics artifacts. property artifact_type Type of the underlying mlmd artifact. property id Id of the underlying mlmd artifact. property mlmd_artifact Underlying mlmd artifact. property name Name of the underlying mlmd artifact. property pipeline_name Name of the pipeline that produce the artifact. property producer_component Producer component of the artifact. property state State of the underlying mlmd artifact. property type Type of the artifact. property type_id Type id of the underlying mlmd artifact. property type_name Type name of the underlying mlmd artifact. property uri Artifact URI. This file was automatically generated via lazydocs .","title":"Artifacts.statistics artifact"},{"location":"api_docs/artifacts.statistics_artifact/#module-artifactsstatistics_artifact","text":"","title":"module artifacts.statistics_artifact"},{"location":"api_docs/artifacts.statistics_artifact/#class-statisticsartifact","text":"Class for all ZenML statistics artifacts.","title":"class StatisticsArtifact"},{"location":"api_docs/artifacts.statistics_artifact/#property-artifact_type","text":"Type of the underlying mlmd artifact.","title":"property artifact_type"},{"location":"api_docs/artifacts.statistics_artifact/#property-id","text":"Id of the underlying mlmd artifact.","title":"property id"},{"location":"api_docs/artifacts.statistics_artifact/#property-mlmd_artifact","text":"Underlying mlmd artifact.","title":"property mlmd_artifact"},{"location":"api_docs/artifacts.statistics_artifact/#property-name","text":"Name of the underlying mlmd artifact.","title":"property name"},{"location":"api_docs/artifacts.statistics_artifact/#property-pipeline_name","text":"Name of the pipeline that produce the artifact.","title":"property pipeline_name"},{"location":"api_docs/artifacts.statistics_artifact/#property-producer_component","text":"Producer component of the artifact.","title":"property producer_component"},{"location":"api_docs/artifacts.statistics_artifact/#property-state","text":"State of the underlying mlmd artifact.","title":"property state"},{"location":"api_docs/artifacts.statistics_artifact/#property-type","text":"Type of the artifact.","title":"property type"},{"location":"api_docs/artifacts.statistics_artifact/#property-type_id","text":"Type id of the underlying mlmd artifact.","title":"property type_id"},{"location":"api_docs/artifacts.statistics_artifact/#property-type_name","text":"Type name of the underlying mlmd artifact.","title":"property type_name"},{"location":"api_docs/artifacts.statistics_artifact/#property-uri","text":"Artifact URI. This file was automatically generated via lazydocs .","title":"property uri"},{"location":"api_docs/artifacts.type_registry/","text":"module artifacts.type_registry Global Variables TYPE_CHECKING type_registry class ArtifactTypeRegistry A registry to keep track of which datatypes map to which artifact types method __init__ __init__() \u2192 None Initialization with an empty registry method get_artifact_type get_artifact_type(key: Type[Any]) \u2192 List[Type[ForwardRef('BaseArtifact')]] Method to extract the list of artifact types given the data type method register_integration register_integration( key: Type[Any], type_: List[Type[ForwardRef('BaseArtifact')]] ) \u2192 None Method to register an integration within the registry Args: key : any datatype type_ : the list of artifact type that the given datatypes is associated with This file was automatically generated via lazydocs .","title":"Artifacts.type registry"},{"location":"api_docs/artifacts.type_registry/#module-artifactstype_registry","text":"","title":"module artifacts.type_registry"},{"location":"api_docs/artifacts.type_registry/#global-variables","text":"TYPE_CHECKING type_registry","title":"Global Variables"},{"location":"api_docs/artifacts.type_registry/#class-artifacttyperegistry","text":"A registry to keep track of which datatypes map to which artifact types","title":"class ArtifactTypeRegistry"},{"location":"api_docs/artifacts.type_registry/#method-__init__","text":"__init__() \u2192 None Initialization with an empty registry","title":"method __init__"},{"location":"api_docs/artifacts.type_registry/#method-get_artifact_type","text":"get_artifact_type(key: Type[Any]) \u2192 List[Type[ForwardRef('BaseArtifact')]] Method to extract the list of artifact types given the data type","title":"method get_artifact_type"},{"location":"api_docs/artifacts.type_registry/#method-register_integration","text":"register_integration( key: Type[Any], type_: List[Type[ForwardRef('BaseArtifact')]] ) \u2192 None Method to register an integration within the registry Args: key : any datatype type_ : the list of artifact type that the given datatypes is associated with This file was automatically generated via lazydocs .","title":"method register_integration"},{"location":"api_docs/cli.artifact_store/","text":"module cli.artifact_store Global Variables TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Cli.artifact store"},{"location":"api_docs/cli.artifact_store/#module-cliartifact_store","text":"","title":"module cli.artifact_store"},{"location":"api_docs/cli.artifact_store/#global-variables","text":"TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/cli.base/","text":"module cli.base Global Variables INITIALIZE_REPO This file was automatically generated via lazydocs .","title":"Cli.base"},{"location":"api_docs/cli.base/#module-clibase","text":"","title":"module cli.base"},{"location":"api_docs/cli.base/#global-variables","text":"INITIALIZE_REPO This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/cli.cli/","text":"module cli.cli .. currentmodule:: ce_cli.cli .. moduleauthor:: ZenML GmbH support@zenml.io This file was automatically generated via lazydocs .","title":"Cli.cli"},{"location":"api_docs/cli.cli/#module-clicli","text":".. currentmodule:: ce_cli.cli .. moduleauthor:: ZenML GmbH support@zenml.io This file was automatically generated via lazydocs .","title":"module cli.cli"},{"location":"api_docs/cli.config/","text":"module cli.config CLI for manipulating ZenML local and global config file. Global Variables TYPE_CHECKING OPT_IN_ANALYTICS OPT_OUT_ANALYTICS This file was automatically generated via lazydocs .","title":"Cli.config"},{"location":"api_docs/cli.config/#module-cliconfig","text":"CLI for manipulating ZenML local and global config file.","title":"module cli.config"},{"location":"api_docs/cli.config/#global-variables","text":"TYPE_CHECKING OPT_IN_ANALYTICS OPT_OUT_ANALYTICS This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/cli.container_registry/","text":"module cli.container_registry This file was automatically generated via lazydocs .","title":"Cli.container registry"},{"location":"api_docs/cli.container_registry/#module-clicontainer_registry","text":"This file was automatically generated via lazydocs .","title":"module cli.container_registry"},{"location":"api_docs/cli.example/","text":"module cli.example Global Variables zenml_version_installed GIT_REPO_URL RUN_EXAMPLE EXAMPLES_GITHUB_REPO EXAMPLES_RUN_SCRIPT SHELL_EXECUTABLE function check_for_version_mismatch check_for_version_mismatch(git_examples_handler: GitExamplesHandler) \u2192 None class LocalExample Class to encapsulate all properties and methods of the local example that can be run from the CLI method __init__ __init__(path: Path, name: str) \u2192 None Create a new LocalExample instance. Args: name : The name of the example, specifically the name of the folder on git path : Path at which the example is installed property executable_python_example Return the python file for the example property has_any_python_file Boolean that states if any python file is present property has_single_python_file Boolean that states if only one python file is present property python_files_in_dir List of all python files in the drectl in local example directory the init .py file is excluded from this list method is_present is_present() \u2192 bool Checks if the example is installed at the given path. method run_example run_example(example_runner: List[str], force: bool) \u2192 None Run the local example using the bash script at the supplied location Args: example_runner : Sequence of locations of executable file(s) to run the example force : Whether to force the install class Example Class for all example objects. method __init__ __init__(name: str, path_in_repo: Path) \u2192 None Create a new Example instance. Args: name : The name of the example, specifically the name of the folder on git path_in_repo : Path to the local example within the global zenml folder. property readme_content Returns the readme content associated with a particular example. class ExamplesRepo Class for the examples repository object. method __init__ __init__(cloning_path: Path) \u2192 None Create a new ExamplesRepo instance. property active_version In case a tagged version is checked out, this property returns that version, else None is returned property examples_dir Returns the path for the examples directory. property examples_run_bash_script property is_cloned Returns whether we have already cloned the examples repository. property latest_release Returns the latest release for the examples repository. method checkout checkout(branch: str) \u2192 None Checks out a specific branch or tag of the examples repository Raises: GitCommandError : if branch doesn't exist. method checkout_latest_release checkout_latest_release() \u2192 None Checks out the latest release of the examples repository. method clone clone() \u2192 None Clones repo to cloning_path. If you break off the operation with a KeyBoardInterrupt before the cloning is completed, this method will delete whatever was partially downloaded from your system. method delete delete() \u2192 None Delete cloning_path if it exists. class GitExamplesHandler Class for the GitExamplesHandler that interfaces with the CLI tool. method __init__ __init__() \u2192 None Create a new GitExamplesHandler instance. property examples Property that contains a list of examples property is_matching_versions Returns a boolean whether the checked out examples are on the same code version as zenml method clean_current_examples clean_current_examples() \u2192 None Deletes the ZenML examples directory from your current working directory. method copy_example copy_example(example: Example, destination_dir: str) \u2192 None Copies an example to the destination_dir. method get_examples get_examples(example_name: Optional[str] = None) \u2192 List[Example] Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name : Name of an example. method is_example is_example(example_name: Optional[str] = None) \u2192 bool Checks if the supplied example_name corresponds to an example method pull pull(version: str = '', force: bool = False, branch: str = 'main') \u2192 None Pulls the examples from the main git examples repository. method pull_latest_examples pull_latest_examples() \u2192 None Pulls the latest examples from the examples repository. This file was automatically generated via lazydocs .","title":"Cli.example"},{"location":"api_docs/cli.example/#module-cliexample","text":"","title":"module cli.example"},{"location":"api_docs/cli.example/#global-variables","text":"zenml_version_installed GIT_REPO_URL RUN_EXAMPLE EXAMPLES_GITHUB_REPO EXAMPLES_RUN_SCRIPT SHELL_EXECUTABLE","title":"Global Variables"},{"location":"api_docs/cli.example/#function-check_for_version_mismatch","text":"check_for_version_mismatch(git_examples_handler: GitExamplesHandler) \u2192 None","title":"function check_for_version_mismatch"},{"location":"api_docs/cli.example/#class-localexample","text":"Class to encapsulate all properties and methods of the local example that can be run from the CLI","title":"class LocalExample"},{"location":"api_docs/cli.example/#method-__init__","text":"__init__(path: Path, name: str) \u2192 None Create a new LocalExample instance. Args: name : The name of the example, specifically the name of the folder on git path : Path at which the example is installed","title":"method __init__"},{"location":"api_docs/cli.example/#property-executable_python_example","text":"Return the python file for the example","title":"property executable_python_example"},{"location":"api_docs/cli.example/#property-has_any_python_file","text":"Boolean that states if any python file is present","title":"property has_any_python_file"},{"location":"api_docs/cli.example/#property-has_single_python_file","text":"Boolean that states if only one python file is present","title":"property has_single_python_file"},{"location":"api_docs/cli.example/#property-python_files_in_dir","text":"List of all python files in the drectl in local example directory the init .py file is excluded from this list","title":"property python_files_in_dir"},{"location":"api_docs/cli.example/#method-is_present","text":"is_present() \u2192 bool Checks if the example is installed at the given path.","title":"method is_present"},{"location":"api_docs/cli.example/#method-run_example","text":"run_example(example_runner: List[str], force: bool) \u2192 None Run the local example using the bash script at the supplied location Args: example_runner : Sequence of locations of executable file(s) to run the example force : Whether to force the install","title":"method run_example"},{"location":"api_docs/cli.example/#class-example","text":"Class for all example objects.","title":"class Example"},{"location":"api_docs/cli.example/#method-__init___1","text":"__init__(name: str, path_in_repo: Path) \u2192 None Create a new Example instance. Args: name : The name of the example, specifically the name of the folder on git path_in_repo : Path to the local example within the global zenml folder.","title":"method __init__"},{"location":"api_docs/cli.example/#property-readme_content","text":"Returns the readme content associated with a particular example.","title":"property readme_content"},{"location":"api_docs/cli.example/#class-examplesrepo","text":"Class for the examples repository object.","title":"class ExamplesRepo"},{"location":"api_docs/cli.example/#method-__init___2","text":"__init__(cloning_path: Path) \u2192 None Create a new ExamplesRepo instance.","title":"method __init__"},{"location":"api_docs/cli.example/#property-active_version","text":"In case a tagged version is checked out, this property returns that version, else None is returned","title":"property active_version"},{"location":"api_docs/cli.example/#property-examples_dir","text":"Returns the path for the examples directory.","title":"property examples_dir"},{"location":"api_docs/cli.example/#property-examples_run_bash_script","text":"","title":"property examples_run_bash_script"},{"location":"api_docs/cli.example/#property-is_cloned","text":"Returns whether we have already cloned the examples repository.","title":"property is_cloned"},{"location":"api_docs/cli.example/#property-latest_release","text":"Returns the latest release for the examples repository.","title":"property latest_release"},{"location":"api_docs/cli.example/#method-checkout","text":"checkout(branch: str) \u2192 None Checks out a specific branch or tag of the examples repository Raises: GitCommandError : if branch doesn't exist.","title":"method checkout"},{"location":"api_docs/cli.example/#method-checkout_latest_release","text":"checkout_latest_release() \u2192 None Checks out the latest release of the examples repository.","title":"method checkout_latest_release"},{"location":"api_docs/cli.example/#method-clone","text":"clone() \u2192 None Clones repo to cloning_path. If you break off the operation with a KeyBoardInterrupt before the cloning is completed, this method will delete whatever was partially downloaded from your system.","title":"method clone"},{"location":"api_docs/cli.example/#method-delete","text":"delete() \u2192 None Delete cloning_path if it exists.","title":"method delete"},{"location":"api_docs/cli.example/#class-gitexampleshandler","text":"Class for the GitExamplesHandler that interfaces with the CLI tool.","title":"class GitExamplesHandler"},{"location":"api_docs/cli.example/#method-__init___3","text":"__init__() \u2192 None Create a new GitExamplesHandler instance.","title":"method __init__"},{"location":"api_docs/cli.example/#property-examples","text":"Property that contains a list of examples","title":"property examples"},{"location":"api_docs/cli.example/#property-is_matching_versions","text":"Returns a boolean whether the checked out examples are on the same code version as zenml","title":"property is_matching_versions"},{"location":"api_docs/cli.example/#method-clean_current_examples","text":"clean_current_examples() \u2192 None Deletes the ZenML examples directory from your current working directory.","title":"method clean_current_examples"},{"location":"api_docs/cli.example/#method-copy_example","text":"copy_example(example: Example, destination_dir: str) \u2192 None Copies an example to the destination_dir.","title":"method copy_example"},{"location":"api_docs/cli.example/#method-get_examples","text":"get_examples(example_name: Optional[str] = None) \u2192 List[Example] Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name : Name of an example.","title":"method get_examples"},{"location":"api_docs/cli.example/#method-is_example","text":"is_example(example_name: Optional[str] = None) \u2192 bool Checks if the supplied example_name corresponds to an example","title":"method is_example"},{"location":"api_docs/cli.example/#method-pull","text":"pull(version: str = '', force: bool = False, branch: str = 'main') \u2192 None Pulls the examples from the main git examples repository.","title":"method pull"},{"location":"api_docs/cli.example/#method-pull_latest_examples","text":"pull_latest_examples() \u2192 None Pulls the latest examples from the examples repository. This file was automatically generated via lazydocs .","title":"method pull_latest_examples"},{"location":"api_docs/cli.integration/","text":"module cli.integration This file was automatically generated via lazydocs .","title":"Cli.integration"},{"location":"api_docs/cli.integration/#module-cliintegration","text":"This file was automatically generated via lazydocs .","title":"module cli.integration"},{"location":"api_docs/cli/","text":"module cli ZenML CLI ================== The ZenML CLI tool is usually downloaded and installed via PyPI and a pip install zenml command. Please see the Installation & Setup section above for more information about that process. How to use the CLI Our CLI behaves similarly to many other CLIs for basic features. In order to find out which version of ZenML you are running, type: .. code:: bash zenml version If you ever need more information on exactly what a certain command will do, use the --help flag attached to the end of your command string. For example, to get a sense of all of the commands available to you while using the zenml command, type: .. code:: bash zenml --help If you were instead looking to know more about a specific command, you can type something like this: .. code:: bash zenml metadata-store register --help This will give you information about how to register a metadata store. (See below for more on that). Beginning a Project In order to start working on your project, initialize a ZenML repository within your current directory with ZenML\u2019s own config and resource management tools: .. code:: bash zenml init This is all you need to begin using all the MLOps goodness that ZenML provides! By default, zenml init will install its own hidden .zen folder inside the current directory from which you are running the command. You can also pass in a directory path manually using the --repo_path option: .. code:: bash zenml init --repo_path /path/to/dir If you wish to specify that you do not want analytics to be transmitted back to ZenML about your usage of the tool, pass in False to the --analytics_opt_in option: .. code:: bash zenml init --analytics_opt_in false If you wish to delete all data relating to your project from the directory, use the zenml clean command. This will: delete all pipelines delete all artifacts delete all metadata Note that the clean command is not implemented for the current version. Loading and using pre-built examples If you don\u2019t have a project of your own that you\u2019re currently working on, or if you just want to play around a bit and see some functional code, we\u2019ve got your back! You can use the ZenML CLI tool to download some pre-built examples. We know that working examples are a great way to get to know a tool, so we\u2019ve made some examples for you to use to get started. (This is something that will grow as we add more). To list all the examples available to you, type: .. code:: bash zenml example list If you want more detailed information about a specific example, use the info subcommand in combination with the name of the example, like this: .. code:: bash zenml example info quickstart If you want to pull all the examples into your current working directory (wherever you are executing the zenml command from in your terminal), the CLI will create a zenml_examples folder for you if it doesn\u2019t already exist whenever you use the pull subcommand. The default is to copy all the examples, like this: .. code:: bash zenml example pull If you\u2019d only like to pull a single example, add the name of that example (for example, quickstart ) as an argument to the same command, as follows: .. code:: bash zenml example pull quickstart If you would like to force-redownload the examples, use the --force or -f flag as in this example: .. code:: bash zenml example pull --force This will redownload all the examples afresh, using the same version of ZenML as you currently have installed. If for some reason you want to download examples corresponding to a previous release of ZenML, use the --version or -v flag to specify, as in the following example: .. code:: bash zenml example pull --force --version 0.3.8 If you wish to run the example, allowing the ZenML CLI to do the work of setting up whatever dependencies are required, use the run subcommand: .. code:: bash zenml example run quickstart Using integrations Integrations are the different pieces of a project stack that enable custom functionality. This ranges from bigger libraries like kubeflow for orchestration down to smaller visualization tools like facets . Our CLI is an easy way to get started with these integrations. To list all the integrations available to you, type: zenml integration list To see the requirements for a specific integration, use the requirements command: zenml integration requirements INTEGRATION_NAME If you wish to install the integration, using the requirements listed in the previous command, install allows you to do this for your local environment: zenml integration install INTEGRATION_NAME Note that if you don't specify a specific integration to be installed, the ZenML CLI will install all available integrations. Uninstalling a specific integration is as simple as typing: zenml integration uninstall INTEGRATION_NAME Customizing your Metadata Store The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. By default ZenML initializes your repository with a metadata store kept on your local machine. If you wish to register a new metadata store, do so with the register command: .. code:: bash zenml metadata-store register METADATA_STORE_NAME --type METADATA_STORE_TYPE [--OPTIONS] If you wish to list the metadata stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml metadata-store list If you wish to delete a particular metadata store, pass the name of the metadata store into the CLI with the following command: .. code:: bash zenml metadata-store delete METADATA_STORE_NAME Customizing your Artifact Store The artifact store is where all the inputs and outputs of your pipeline steps are stored. By default, ZenML initializes your repository with an artifact store with everything kept on your local machine. If you wish to register a new artifact store, do so with the register command: .. code:: bash zenml artifact-store register ARTIFACT_STORE_NAME --type ARTIFACT_STORE_TYPE [--OPTIONS] If you wish to list the artifact stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml artifact-store list If you wish to delete a particular artifact store, pass the name of the artifact store into the CLI with the following command: .. code:: bash zenml artifact-store delete ARTIFACT_STORE_NAME Customizing your Orchestrator An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. By default, ZenML initializes your repository with an orchestrator that runs everything on your local machine. If you wish to register a new orchestrator, do so with the register command: .. code:: bash zenml orchestrator register ORCHESTRATOR_NAME --type ORCHESTRATOR_TYPE [--ORCHESTRATOR_OPTIONS] If you wish to list the orchestrators that have already been registered within your ZenML project / repository, type: .. code:: bash zenml orchestrator list If you wish to delete a particular orchestrator, pass the name of the orchestrator into the CLI with the following command: .. code:: bash zenml orchestrator delete ORCHESTRATOR_NAME Customizing your Container Registry The container registry is where all the images that are used by a container-based orchestrator are stored. By default, a default ZenML local stack will not register a container registry. If you wish to register a new container registry, do so with the register command: zenml container-registry register REGISTRY_NAME --type REGISTRY_TYPE [--REGISTRY_OPTIONS] If you want the name of the current container registry, use the get command: zenml container-registry get To list all container registries available and registered for use, use the list command: zenml container-registry list For details about a particular container registry, use the describe command. By default (without a specific registry name passed in) it will describe the active or currently used container registry: zenml container-registry describe [REGISTRY_NAME] To delete a container registry (and all of its contents), use the delete command: zenml container-registry delete Administering the Stack The stack is a grouping of your artifact store, your metadata store and your orchestrator. With the ZenML tool, switching from a local stack to a distributed cloud environment can be accomplished with just a few CLI commands. To register a new stack, you must already have registered the individual components of the stack using the commands listed above. Use the zenml stack register command to register your stack. It takes four arguments as in the following example: .. code:: bash zenml stack register STACK_NAME -m METADATA_STORE_NAME -a ARTIFACT_STORE_NAME -o ORCHESTRATOR_NAME Each corresponding argument should be the name you passed in as an identifier for the artifact store, metadata store or orchestrator when you originally registered it. To list the stacks that you have registered within your current ZenML project, type: .. code:: bash zenml stack list To delete a stack that you have previously registered, type: .. code:: bash zenml stack delete STACK_NAME By default, ZenML uses a local stack whereby all pipelines run on your local computer. If you wish to set a different stack as the current active stack to be used when running your pipeline, type: .. code:: bash zenml stack set STACK_NAME This changes a configuration property within your local environment. To see which stack is currently set as the default active stack, type: .. code:: bash zenml stack get Global Variables TYPE_CHECKING click INITIALIZE_REPO OPT_IN_ANALYTICS OPT_OUT_ANALYTICS zenml_version_installed GIT_REPO_URL RUN_EXAMPLE EXAMPLES_GITHUB_REPO EXAMPLES_RUN_SCRIPT SHELL_EXECUTABLE ascii_arts This file was automatically generated via lazydocs .","title":"Cli"},{"location":"api_docs/cli/#module-cli","text":"ZenML CLI ================== The ZenML CLI tool is usually downloaded and installed via PyPI and a pip install zenml command. Please see the Installation & Setup section above for more information about that process.","title":"module cli"},{"location":"api_docs/cli/#how-to-use-the-cli","text":"Our CLI behaves similarly to many other CLIs for basic features. In order to find out which version of ZenML you are running, type: .. code:: bash zenml version If you ever need more information on exactly what a certain command will do, use the --help flag attached to the end of your command string. For example, to get a sense of all of the commands available to you while using the zenml command, type: .. code:: bash zenml --help If you were instead looking to know more about a specific command, you can type something like this: .. code:: bash zenml metadata-store register --help This will give you information about how to register a metadata store. (See below for more on that).","title":"How to use the CLI"},{"location":"api_docs/cli/#beginning-a-project","text":"In order to start working on your project, initialize a ZenML repository within your current directory with ZenML\u2019s own config and resource management tools: .. code:: bash zenml init This is all you need to begin using all the MLOps goodness that ZenML provides! By default, zenml init will install its own hidden .zen folder inside the current directory from which you are running the command. You can also pass in a directory path manually using the --repo_path option: .. code:: bash zenml init --repo_path /path/to/dir If you wish to specify that you do not want analytics to be transmitted back to ZenML about your usage of the tool, pass in False to the --analytics_opt_in option: .. code:: bash zenml init --analytics_opt_in false If you wish to delete all data relating to your project from the directory, use the zenml clean command. This will: delete all pipelines delete all artifacts delete all metadata Note that the clean command is not implemented for the current version.","title":"Beginning a Project"},{"location":"api_docs/cli/#loading-and-using-pre-built-examples","text":"If you don\u2019t have a project of your own that you\u2019re currently working on, or if you just want to play around a bit and see some functional code, we\u2019ve got your back! You can use the ZenML CLI tool to download some pre-built examples. We know that working examples are a great way to get to know a tool, so we\u2019ve made some examples for you to use to get started. (This is something that will grow as we add more). To list all the examples available to you, type: .. code:: bash zenml example list If you want more detailed information about a specific example, use the info subcommand in combination with the name of the example, like this: .. code:: bash zenml example info quickstart If you want to pull all the examples into your current working directory (wherever you are executing the zenml command from in your terminal), the CLI will create a zenml_examples folder for you if it doesn\u2019t already exist whenever you use the pull subcommand. The default is to copy all the examples, like this: .. code:: bash zenml example pull If you\u2019d only like to pull a single example, add the name of that example (for example, quickstart ) as an argument to the same command, as follows: .. code:: bash zenml example pull quickstart If you would like to force-redownload the examples, use the --force or -f flag as in this example: .. code:: bash zenml example pull --force This will redownload all the examples afresh, using the same version of ZenML as you currently have installed. If for some reason you want to download examples corresponding to a previous release of ZenML, use the --version or -v flag to specify, as in the following example: .. code:: bash zenml example pull --force --version 0.3.8 If you wish to run the example, allowing the ZenML CLI to do the work of setting up whatever dependencies are required, use the run subcommand: .. code:: bash zenml example run quickstart","title":"Loading and using pre-built examples"},{"location":"api_docs/cli/#using-integrations","text":"Integrations are the different pieces of a project stack that enable custom functionality. This ranges from bigger libraries like kubeflow for orchestration down to smaller visualization tools like facets . Our CLI is an easy way to get started with these integrations. To list all the integrations available to you, type: zenml integration list To see the requirements for a specific integration, use the requirements command: zenml integration requirements INTEGRATION_NAME If you wish to install the integration, using the requirements listed in the previous command, install allows you to do this for your local environment: zenml integration install INTEGRATION_NAME Note that if you don't specify a specific integration to be installed, the ZenML CLI will install all available integrations. Uninstalling a specific integration is as simple as typing: zenml integration uninstall INTEGRATION_NAME","title":"Using integrations"},{"location":"api_docs/cli/#customizing-your-metadata-store","text":"The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. By default ZenML initializes your repository with a metadata store kept on your local machine. If you wish to register a new metadata store, do so with the register command: .. code:: bash zenml metadata-store register METADATA_STORE_NAME --type METADATA_STORE_TYPE [--OPTIONS] If you wish to list the metadata stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml metadata-store list If you wish to delete a particular metadata store, pass the name of the metadata store into the CLI with the following command: .. code:: bash zenml metadata-store delete METADATA_STORE_NAME","title":"Customizing your Metadata Store"},{"location":"api_docs/cli/#customizing-your-artifact-store","text":"The artifact store is where all the inputs and outputs of your pipeline steps are stored. By default, ZenML initializes your repository with an artifact store with everything kept on your local machine. If you wish to register a new artifact store, do so with the register command: .. code:: bash zenml artifact-store register ARTIFACT_STORE_NAME --type ARTIFACT_STORE_TYPE [--OPTIONS] If you wish to list the artifact stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml artifact-store list If you wish to delete a particular artifact store, pass the name of the artifact store into the CLI with the following command: .. code:: bash zenml artifact-store delete ARTIFACT_STORE_NAME","title":"Customizing your Artifact Store"},{"location":"api_docs/cli/#customizing-your-orchestrator","text":"An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. By default, ZenML initializes your repository with an orchestrator that runs everything on your local machine. If you wish to register a new orchestrator, do so with the register command: .. code:: bash zenml orchestrator register ORCHESTRATOR_NAME --type ORCHESTRATOR_TYPE [--ORCHESTRATOR_OPTIONS] If you wish to list the orchestrators that have already been registered within your ZenML project / repository, type: .. code:: bash zenml orchestrator list If you wish to delete a particular orchestrator, pass the name of the orchestrator into the CLI with the following command: .. code:: bash zenml orchestrator delete ORCHESTRATOR_NAME","title":"Customizing your Orchestrator"},{"location":"api_docs/cli/#customizing-your-container-registry","text":"The container registry is where all the images that are used by a container-based orchestrator are stored. By default, a default ZenML local stack will not register a container registry. If you wish to register a new container registry, do so with the register command: zenml container-registry register REGISTRY_NAME --type REGISTRY_TYPE [--REGISTRY_OPTIONS] If you want the name of the current container registry, use the get command: zenml container-registry get To list all container registries available and registered for use, use the list command: zenml container-registry list For details about a particular container registry, use the describe command. By default (without a specific registry name passed in) it will describe the active or currently used container registry: zenml container-registry describe [REGISTRY_NAME] To delete a container registry (and all of its contents), use the delete command: zenml container-registry delete","title":"Customizing your Container Registry"},{"location":"api_docs/cli/#administering-the-stack","text":"The stack is a grouping of your artifact store, your metadata store and your orchestrator. With the ZenML tool, switching from a local stack to a distributed cloud environment can be accomplished with just a few CLI commands. To register a new stack, you must already have registered the individual components of the stack using the commands listed above. Use the zenml stack register command to register your stack. It takes four arguments as in the following example: .. code:: bash zenml stack register STACK_NAME -m METADATA_STORE_NAME -a ARTIFACT_STORE_NAME -o ORCHESTRATOR_NAME Each corresponding argument should be the name you passed in as an identifier for the artifact store, metadata store or orchestrator when you originally registered it. To list the stacks that you have registered within your current ZenML project, type: .. code:: bash zenml stack list To delete a stack that you have previously registered, type: .. code:: bash zenml stack delete STACK_NAME By default, ZenML uses a local stack whereby all pipelines run on your local computer. If you wish to set a different stack as the current active stack to be used when running your pipeline, type: .. code:: bash zenml stack set STACK_NAME This changes a configuration property within your local environment. To see which stack is currently set as the default active stack, type: .. code:: bash zenml stack get","title":"Administering the Stack"},{"location":"api_docs/cli/#global-variables","text":"TYPE_CHECKING click INITIALIZE_REPO OPT_IN_ANALYTICS OPT_OUT_ANALYTICS zenml_version_installed GIT_REPO_URL RUN_EXAMPLE EXAMPLES_GITHUB_REPO EXAMPLES_RUN_SCRIPT SHELL_EXECUTABLE ascii_arts This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/cli.metadata_store/","text":"module cli.metadata_store Global Variables TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Cli.metadata store"},{"location":"api_docs/cli.metadata_store/#module-climetadata_store","text":"","title":"module cli.metadata_store"},{"location":"api_docs/cli.metadata_store/#global-variables","text":"TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/cli.orchestrator/","text":"module cli.orchestrator Global Variables TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Cli.orchestrator"},{"location":"api_docs/cli.orchestrator/#module-cliorchestrator","text":"","title":"module cli.orchestrator"},{"location":"api_docs/cli.orchestrator/#global-variables","text":"TYPE_CHECKING This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/cli.pipeline/","text":"module cli.pipeline CLI to interact with pipelines. This file was automatically generated via lazydocs .","title":"Cli.pipeline"},{"location":"api_docs/cli.pipeline/#module-clipipeline","text":"CLI to interact with pipelines. This file was automatically generated via lazydocs .","title":"module cli.pipeline"},{"location":"api_docs/cli.stack/","text":"module cli.stack CLI for manipulating ZenML local and global config file. This file was automatically generated via lazydocs .","title":"Cli.stack"},{"location":"api_docs/cli.stack/#module-clistack","text":"CLI for manipulating ZenML local and global config file. This file was automatically generated via lazydocs .","title":"module cli.stack"},{"location":"api_docs/cli.utils/","text":"module cli.utils Global Variables TYPE_CHECKING function title title(text: str) \u2192 None Echo a title formatted string on the CLI. Args: text : Input text string. function confirmation confirmation(text: str, *args: Any, **kwargs: Any) \u2192 bool Echo a confirmation string on the CLI. Args: text : Input text string. *args : Args to be passed to click.confirm(). **kwargs : Kwargs to be passed to click.confirm(). Returns: Boolean based on user response. function declare declare(text: str) \u2192 None Echo a declaration on the CLI. Args: text : Input text string. function error error(text: str) \u2192 None Echo an error string on the CLI. Args: text : Input text string. Raises: click.ClickException when called. function warning warning(text: str) \u2192 None Echo a warning string on the CLI. Args: text : Input text string. function pretty_print pretty_print(obj: Any) \u2192 None Pretty print an object on the CLI. Args: obj : Any object with a str method defined. function print_table print_table(obj: List[Dict[str, Any]]) \u2192 None Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Args: obj : A List containing dictionaries. function format_component_list format_component_list( component_list: Mapping[str, ForwardRef('BaseComponent')], active_component: str ) \u2192 List[Dict[str, str]] Formats a list of components into a List of Dicts. This list of dicts can then be printed in a table style using cli_utils.print_table. Args: component_list : The component_list is a mapping of component key to component class with its relevant attributes active_component : The component that is currently active Returns: list_of_dicts : A list of all components with each component as a dict function print_component_properties print_component_properties(properties: Dict[str, str]) \u2192 None Prints the properties of a component. Args: properties : A dictionary of properties. function format_date format_date(dt: datetime, format: str = '%Y-%m-%d %H:%M:%S') \u2192 str Format a date into a string. Args: dt : Datetime object to be formatted. format : The format in string you want the datetime formatted to. Returns: Formatted string according to specification. function parse_unknown_options parse_unknown_options(args: List[str]) \u2192 Dict[str, Any] Parse unknown options from the CLI. Args: args : A list of strings from the CLI. Returns: Dict of parsed args. function activate_integrations activate_integrations(func: ~F) \u2192 ~F Decorator that activates all ZenML integrations. function install_package install_package(package: str) \u2192 None Installs pypi package into the current environment with pip function uninstall_package uninstall_package(package: str) \u2192 None Uninstalls pypi package from the current environment with pip This file was automatically generated via lazydocs .","title":"Cli.utils"},{"location":"api_docs/cli.utils/#module-cliutils","text":"","title":"module cli.utils"},{"location":"api_docs/cli.utils/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/cli.utils/#function-title","text":"title(text: str) \u2192 None Echo a title formatted string on the CLI. Args: text : Input text string.","title":"function title"},{"location":"api_docs/cli.utils/#function-confirmation","text":"confirmation(text: str, *args: Any, **kwargs: Any) \u2192 bool Echo a confirmation string on the CLI. Args: text : Input text string. *args : Args to be passed to click.confirm(). **kwargs : Kwargs to be passed to click.confirm(). Returns: Boolean based on user response.","title":"function confirmation"},{"location":"api_docs/cli.utils/#function-declare","text":"declare(text: str) \u2192 None Echo a declaration on the CLI. Args: text : Input text string.","title":"function declare"},{"location":"api_docs/cli.utils/#function-error","text":"error(text: str) \u2192 None Echo an error string on the CLI. Args: text : Input text string. Raises: click.ClickException when called.","title":"function error"},{"location":"api_docs/cli.utils/#function-warning","text":"warning(text: str) \u2192 None Echo a warning string on the CLI. Args: text : Input text string.","title":"function warning"},{"location":"api_docs/cli.utils/#function-pretty_print","text":"pretty_print(obj: Any) \u2192 None Pretty print an object on the CLI. Args: obj : Any object with a str method defined.","title":"function pretty_print"},{"location":"api_docs/cli.utils/#function-print_table","text":"print_table(obj: List[Dict[str, Any]]) \u2192 None Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Args: obj : A List containing dictionaries.","title":"function print_table"},{"location":"api_docs/cli.utils/#function-format_component_list","text":"format_component_list( component_list: Mapping[str, ForwardRef('BaseComponent')], active_component: str ) \u2192 List[Dict[str, str]] Formats a list of components into a List of Dicts. This list of dicts can then be printed in a table style using cli_utils.print_table. Args: component_list : The component_list is a mapping of component key to component class with its relevant attributes active_component : The component that is currently active Returns: list_of_dicts : A list of all components with each component as a dict","title":"function format_component_list"},{"location":"api_docs/cli.utils/#function-print_component_properties","text":"print_component_properties(properties: Dict[str, str]) \u2192 None Prints the properties of a component. Args: properties : A dictionary of properties.","title":"function print_component_properties"},{"location":"api_docs/cli.utils/#function-format_date","text":"format_date(dt: datetime, format: str = '%Y-%m-%d %H:%M:%S') \u2192 str Format a date into a string. Args: dt : Datetime object to be formatted. format : The format in string you want the datetime formatted to. Returns: Formatted string according to specification.","title":"function format_date"},{"location":"api_docs/cli.utils/#function-parse_unknown_options","text":"parse_unknown_options(args: List[str]) \u2192 Dict[str, Any] Parse unknown options from the CLI. Args: args : A list of strings from the CLI. Returns: Dict of parsed args.","title":"function parse_unknown_options"},{"location":"api_docs/cli.utils/#function-activate_integrations","text":"activate_integrations(func: ~F) \u2192 ~F Decorator that activates all ZenML integrations.","title":"function activate_integrations"},{"location":"api_docs/cli.utils/#function-install_package","text":"install_package(package: str) \u2192 None Installs pypi package into the current environment with pip","title":"function install_package"},{"location":"api_docs/cli.utils/#function-uninstall_package","text":"uninstall_package(package: str) \u2192 None Uninstalls pypi package from the current environment with pip This file was automatically generated via lazydocs .","title":"function uninstall_package"},{"location":"api_docs/cli.version/","text":"module cli.version Global Variables ascii_arts This file was automatically generated via lazydocs .","title":"Cli.version"},{"location":"api_docs/cli.version/#module-cliversion","text":"","title":"module cli.version"},{"location":"api_docs/cli.version/#global-variables","text":"ascii_arts This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/config.config_keys/","text":"module config.config_keys class ConfigKeys Class to validate dictionary configurations. classmethod get_keys get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. classmethod key_check key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key. class PipelineConfigurationKeys Keys for a pipeline configuration dict. classmethod get_keys get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. classmethod key_check key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key. class StepConfigurationKeys Keys for a step configuration dict. classmethod get_keys get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. classmethod key_check key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key. This file was automatically generated via lazydocs .","title":"Config.config keys"},{"location":"api_docs/config.config_keys/#module-configconfig_keys","text":"","title":"module config.config_keys"},{"location":"api_docs/config.config_keys/#class-configkeys","text":"Class to validate dictionary configurations.","title":"class ConfigKeys"},{"location":"api_docs/config.config_keys/#classmethod-get_keys","text":"get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class.","title":"classmethod get_keys"},{"location":"api_docs/config.config_keys/#classmethod-key_check","text":"key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key.","title":"classmethod key_check"},{"location":"api_docs/config.config_keys/#class-pipelineconfigurationkeys","text":"Keys for a pipeline configuration dict.","title":"class PipelineConfigurationKeys"},{"location":"api_docs/config.config_keys/#classmethod-get_keys_1","text":"get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class.","title":"classmethod get_keys"},{"location":"api_docs/config.config_keys/#classmethod-key_check_1","text":"key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key.","title":"classmethod key_check"},{"location":"api_docs/config.config_keys/#class-stepconfigurationkeys","text":"Keys for a step configuration dict.","title":"class StepConfigurationKeys"},{"location":"api_docs/config.config_keys/#classmethod-get_keys_2","text":"get_keys() \u2192 Tuple[List[str], List[str]] Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class.","title":"classmethod get_keys"},{"location":"api_docs/config.config_keys/#classmethod-key_check_2","text":"key_check(config: Dict[str, Any]) \u2192 None Checks whether a configuration dict contains all required keys and no unknown keys. Args: config : The configuration dict to verify. Raises: AssertionError : If the dictionary contains unknown keys or is missing any required key. This file was automatically generated via lazydocs .","title":"classmethod key_check"},{"location":"api_docs/config.constants/","text":"module config.constants Global Variables GLOBAL_CONFIG_NAME This file was automatically generated via lazydocs .","title":"Config.constants"},{"location":"api_docs/config.constants/#module-configconstants","text":"","title":"module config.constants"},{"location":"api_docs/config.constants/#global-variables","text":"GLOBAL_CONFIG_NAME This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/config.global_config/","text":"module config.global_config Global config for the ZenML installation. Global Variables GLOBAL_CONFIG_NAME class GlobalConfig Class definition for the global config. Defines global data such as unique user ID and whether they opted in for analytics. method __init__ __init__(**data: Any) We persist the attributes in the config file. For the global config, we want to persist the data as soon as it is initialized for the first time. method get_serialization_file_name get_serialization_file_name() \u2192 str Gets the global config dir for installed package. This file was automatically generated via lazydocs .","title":"Config.global config"},{"location":"api_docs/config.global_config/#module-configglobal_config","text":"Global config for the ZenML installation.","title":"module config.global_config"},{"location":"api_docs/config.global_config/#global-variables","text":"GLOBAL_CONFIG_NAME","title":"Global Variables"},{"location":"api_docs/config.global_config/#class-globalconfig","text":"Class definition for the global config. Defines global data such as unique user ID and whether they opted in for analytics.","title":"class GlobalConfig"},{"location":"api_docs/config.global_config/#method-__init__","text":"__init__(**data: Any) We persist the attributes in the config file. For the global config, we want to persist the data as soon as it is initialized for the first time.","title":"method __init__"},{"location":"api_docs/config.global_config/#method-get_serialization_file_name","text":"get_serialization_file_name() \u2192 str Gets the global config dir for installed package. This file was automatically generated via lazydocs .","title":"method get_serialization_file_name"},{"location":"api_docs/config/","text":"module config The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions. This file was automatically generated via lazydocs .","title":"Config"},{"location":"api_docs/config/#module-config","text":"The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions. This file was automatically generated via lazydocs .","title":"module config"},{"location":"api_docs/constants/","text":"module constants Global Variables APP_NAME CONFIG_VERSION GIT_REPO_URL ENV_ZENML_DEBUG ENV_ZENML_LOGGING_VERBOSITY ENV_ABSL_LOGGING_VERBOSITY ENV_ZENML_REPOSITORY_PATH ENV_ZENML_PREVENT_PIPELINE_EXECUTION IS_DEBUG_ENV ZENML_LOGGING_VERBOSITY ABSL_LOGGING_VERBOSITY ZENML_REGISTRY ZENML_BASE_IMAGE_NAME ZENML_TRAINER_IMAGE_NAME ZENML_DATAFLOW_IMAGE_NAME COMPARISON_NOTEBOOK EVALUATION_NOTEBOOK PREPROCESSING_FN TRAINER_FN GCP_ENTRYPOINT AWS_ENTRYPOINT K8S_ENTRYPOINT VALID_OPERATING_SYSTEMS REMOTE_FS_PREFIX SEGMENT_KEY_DEV SEGMENT_KEY_PROD SHOULD_PREVENT_PIPELINE_EXECUTION function handle_bool_env_var handle_bool_env_var(var: str, default: bool = False) \u2192 bool Converts normal env var to boolean function handle_int_env_var handle_int_env_var(var: str, default: int = 0) \u2192 int Converts normal env var to int This file was automatically generated via lazydocs .","title":"Constants"},{"location":"api_docs/constants/#module-constants","text":"","title":"module constants"},{"location":"api_docs/constants/#global-variables","text":"APP_NAME CONFIG_VERSION GIT_REPO_URL ENV_ZENML_DEBUG ENV_ZENML_LOGGING_VERBOSITY ENV_ABSL_LOGGING_VERBOSITY ENV_ZENML_REPOSITORY_PATH ENV_ZENML_PREVENT_PIPELINE_EXECUTION IS_DEBUG_ENV ZENML_LOGGING_VERBOSITY ABSL_LOGGING_VERBOSITY ZENML_REGISTRY ZENML_BASE_IMAGE_NAME ZENML_TRAINER_IMAGE_NAME ZENML_DATAFLOW_IMAGE_NAME COMPARISON_NOTEBOOK EVALUATION_NOTEBOOK PREPROCESSING_FN TRAINER_FN GCP_ENTRYPOINT AWS_ENTRYPOINT K8S_ENTRYPOINT VALID_OPERATING_SYSTEMS REMOTE_FS_PREFIX SEGMENT_KEY_DEV SEGMENT_KEY_PROD SHOULD_PREVENT_PIPELINE_EXECUTION","title":"Global Variables"},{"location":"api_docs/constants/#function-handle_bool_env_var","text":"handle_bool_env_var(var: str, default: bool = False) \u2192 bool Converts normal env var to boolean","title":"function handle_bool_env_var"},{"location":"api_docs/constants/#function-handle_int_env_var","text":"handle_int_env_var(var: str, default: int = 0) \u2192 int Converts normal env var to int This file was automatically generated via lazydocs .","title":"function handle_int_env_var"},{"location":"api_docs/container_registries.base_container_registry/","text":"module container_registries.base_container_registry Base class for all container registries. class BaseContainerRegistry Base class for all ZenML container registries. method __init__ __init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseContainerRegistry instance. Args: repo_path : Path to the repository of this container registry. This file was automatically generated via lazydocs .","title":"Container registries.base container registry"},{"location":"api_docs/container_registries.base_container_registry/#module-container_registriesbase_container_registry","text":"Base class for all container registries.","title":"module container_registries.base_container_registry"},{"location":"api_docs/container_registries.base_container_registry/#class-basecontainerregistry","text":"Base class for all ZenML container registries.","title":"class BaseContainerRegistry"},{"location":"api_docs/container_registries.base_container_registry/#method-__init__","text":"__init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseContainerRegistry instance. Args: repo_path : Path to the repository of this container registry. This file was automatically generated via lazydocs .","title":"method __init__"},{"location":"api_docs/container_registries/","text":"module container_registries This file was automatically generated via lazydocs .","title":"Container registries"},{"location":"api_docs/container_registries/#module-container_registries","text":"This file was automatically generated via lazydocs .","title":"module container_registries"},{"location":"api_docs/core.base_component/","text":"module core.base_component Global Variables SUPERFLUOUS_OPTIONS_ATTRIBUTE_NAME class BaseComponent Class definition for the base config. The base component class defines the basic serialization / deserialization of various components used in ZenML. The logic of the serialization / deserialization is as follows: If a uuid is passed in, then the object is read from a file, so theconstructor becomes a query for an object that is assumed to already been serialized. * If a 'uuid` is NOT passed, then a new object is created with the default args (and any other args that are passed), and therefore a fresh serialization takes place. method __init__ __init__(serialization_dir: str, **values: Any) classmethod check_superfluous_options check_superfluous_options(values: Dict[str, Any]) \u2192 Dict[str, Any] Detects superfluous config values (usually read from an existing config file after the schema changed) and saves them in the classes _superfluous_options attribute. method delete delete() \u2192 None Deletes the persisted state of this object. method dict dict(**kwargs: Any) \u2192 Dict[str, Any] Removes private attributes from pydantic dict so they don't get stored in our config files. method get_serialization_dir get_serialization_dir() \u2192 str Return the dir where object is serialized. method get_serialization_file_name get_serialization_file_name() \u2192 str Return the name of the file where object is serialized. This has a sane default in cases where uuid is not passed externally, and therefore reading from a serialize file is not an option for the table. However, we still this function to go through without an exception, therefore the sane default. method get_serialization_full_path get_serialization_full_path() \u2192 str Returns the full path of the serialization file. method update update() \u2192 None Persist the current state of the component. Calling this will result in a persistent, stateful change in the system. This file was automatically generated via lazydocs .","title":"Core.base component"},{"location":"api_docs/core.base_component/#module-corebase_component","text":"","title":"module core.base_component"},{"location":"api_docs/core.base_component/#global-variables","text":"SUPERFLUOUS_OPTIONS_ATTRIBUTE_NAME","title":"Global Variables"},{"location":"api_docs/core.base_component/#class-basecomponent","text":"Class definition for the base config. The base component class defines the basic serialization / deserialization of various components used in ZenML. The logic of the serialization / deserialization is as follows: If a uuid is passed in, then the object is read from a file, so theconstructor becomes a query for an object that is assumed to already been serialized. * If a 'uuid` is NOT passed, then a new object is created with the default args (and any other args that are passed), and therefore a fresh serialization takes place.","title":"class BaseComponent"},{"location":"api_docs/core.base_component/#method-__init__","text":"__init__(serialization_dir: str, **values: Any)","title":"method __init__"},{"location":"api_docs/core.base_component/#classmethod-check_superfluous_options","text":"check_superfluous_options(values: Dict[str, Any]) \u2192 Dict[str, Any] Detects superfluous config values (usually read from an existing config file after the schema changed) and saves them in the classes _superfluous_options attribute.","title":"classmethod check_superfluous_options"},{"location":"api_docs/core.base_component/#method-delete","text":"delete() \u2192 None Deletes the persisted state of this object.","title":"method delete"},{"location":"api_docs/core.base_component/#method-dict","text":"dict(**kwargs: Any) \u2192 Dict[str, Any] Removes private attributes from pydantic dict so they don't get stored in our config files.","title":"method dict"},{"location":"api_docs/core.base_component/#method-get_serialization_dir","text":"get_serialization_dir() \u2192 str Return the dir where object is serialized.","title":"method get_serialization_dir"},{"location":"api_docs/core.base_component/#method-get_serialization_file_name","text":"get_serialization_file_name() \u2192 str Return the name of the file where object is serialized. This has a sane default in cases where uuid is not passed externally, and therefore reading from a serialize file is not an option for the table. However, we still this function to go through without an exception, therefore the sane default.","title":"method get_serialization_file_name"},{"location":"api_docs/core.base_component/#method-get_serialization_full_path","text":"get_serialization_full_path() \u2192 str Returns the full path of the serialization file.","title":"method get_serialization_full_path"},{"location":"api_docs/core.base_component/#method-update","text":"update() \u2192 None Persist the current state of the component. Calling this will result in a persistent, stateful change in the system. This file was automatically generated via lazydocs .","title":"method update"},{"location":"api_docs/core.component_factory/","text":"module core.component_factory Factory to register all components. Global Variables artifact_store_factory metadata_store_factory orchestrator_store_factory class ComponentFactory Definition of ComponentFactory to track all BaseComponent subclasses. All BaseComponents (including custom ones) are to be registered here. method __init__ __init__(name: str) Constructor for the factory. Args: name : Unique name for the factory. method get_component_key get_component_key(component: Type[BaseComponent]) \u2192 str Gets the key of a registered component. method get_components get_components() \u2192 Dict[str, Type[BaseComponent]] Return all components method get_single_component get_single_component(key: str) \u2192 Type[BaseComponent] Get a registered component from a key. method register register(name: str) \u2192 Callable[[Type[BaseComponent]], Type[BaseComponent]] Class decorator to register component classes to the internal registry. Args: name : The name of the component. Returns: A function which registers the class at this ComponentFactory. method register_component register_component(key: str, component: Type[BaseComponent]) \u2192 None Registers a single component class for a given key. This file was automatically generated via lazydocs .","title":"Core.component factory"},{"location":"api_docs/core.component_factory/#module-corecomponent_factory","text":"Factory to register all components.","title":"module core.component_factory"},{"location":"api_docs/core.component_factory/#global-variables","text":"artifact_store_factory metadata_store_factory orchestrator_store_factory","title":"Global Variables"},{"location":"api_docs/core.component_factory/#class-componentfactory","text":"Definition of ComponentFactory to track all BaseComponent subclasses. All BaseComponents (including custom ones) are to be registered here.","title":"class ComponentFactory"},{"location":"api_docs/core.component_factory/#method-__init__","text":"__init__(name: str) Constructor for the factory. Args: name : Unique name for the factory.","title":"method __init__"},{"location":"api_docs/core.component_factory/#method-get_component_key","text":"get_component_key(component: Type[BaseComponent]) \u2192 str Gets the key of a registered component.","title":"method get_component_key"},{"location":"api_docs/core.component_factory/#method-get_components","text":"get_components() \u2192 Dict[str, Type[BaseComponent]] Return all components","title":"method get_components"},{"location":"api_docs/core.component_factory/#method-get_single_component","text":"get_single_component(key: str) \u2192 Type[BaseComponent] Get a registered component from a key.","title":"method get_single_component"},{"location":"api_docs/core.component_factory/#method-register","text":"register(name: str) \u2192 Callable[[Type[BaseComponent]], Type[BaseComponent]] Class decorator to register component classes to the internal registry. Args: name : The name of the component. Returns: A function which registers the class at this ComponentFactory.","title":"method register"},{"location":"api_docs/core.component_factory/#method-register_component","text":"register_component(key: str, component: Type[BaseComponent]) \u2192 None Registers a single component class for a given key. This file was automatically generated via lazydocs .","title":"method register_component"},{"location":"api_docs/core.constants/","text":"module core.constants Global Variables ZENML_DIR_NAME This file was automatically generated via lazydocs .","title":"Core.constants"},{"location":"api_docs/core.constants/#module-coreconstants","text":"","title":"module core.constants"},{"location":"api_docs/core.constants/#global-variables","text":"ZENML_DIR_NAME This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/core.git_wrapper/","text":"module core.git_wrapper Wrapper class to handle Git integration Global Variables APP_NAME GIT_FOLDER_NAME class GitWrapper Wrapper class for Git. This class is responsible for handling git interactions, primarily handling versioning of different steps in pipelines. method __init__ __init__(repo_path: str) Initialize GitWrapper. Should be initialized by ZenML Repository. Args: repo_path: Raises: InvalidGitRepositoryError : If repository is not a git repository. NoSuchPathError : If the repo_path does not exist. method check_file_committed check_file_committed(file_path: str) \u2192 bool Checks file is committed. If yes, return True, else False. Args: file_path (str): Path to any file within the ZenML repo. method check_module_clean check_module_clean(source: str) \u2192 bool Returns True if all files within source's module are committed. Args: source : relative module path pointing to a Class. method checkout checkout( sha_or_branch: Optional[str] = None, directory: Optional[str] = None ) \u2192 None Wrapper for git checkout Args: sha_or_branch : hex string of len 40 representing git sha OR name of branch directory : relative path to directory to scope checkout method get_current_sha get_current_sha() \u2192 str Finds the git sha that each file within the module is currently on. method is_valid_source is_valid_source(source: str) \u2192 bool Checks whether the source_path is valid or not. Args: source (str): class_source e.g. this.module.Class[@pin]. method load_source_path_class load_source_path_class(source: str) \u2192 Type[Any] Loads a Python class from the source. Args: source : class_source e.g. this.module.Class[@sha] method reset reset(directory: Optional[str] = None) \u2192 None Wrapper for git reset HEAD <directory> . Args: directory : Relative path to directory to scope checkout method resolve_class resolve_class(class_: Type[Any]) \u2192 str Resolves a class into a serializable source string. Args: class_ : A Python Class reference. Returns: source_path e.g. this.module.Class[@pin]. method resolve_class_source resolve_class_source(class_source: str) \u2192 str Resolves class_source with an optional pin. Takes source (e.g. this.module.ClassName), and appends relevant sha to it if the files within module are all committed. If even one file is not committed, then returns source unchanged. Args: class_source (str): class_source e.g. this.module.Class method stash stash() \u2192 None Wrapper for git stash method stash_pop stash_pop() \u2192 None Wrapper for git stash pop. Only pops if there's something to pop. This file was automatically generated via lazydocs .","title":"Core.git wrapper"},{"location":"api_docs/core.git_wrapper/#module-coregit_wrapper","text":"Wrapper class to handle Git integration","title":"module core.git_wrapper"},{"location":"api_docs/core.git_wrapper/#global-variables","text":"APP_NAME GIT_FOLDER_NAME","title":"Global Variables"},{"location":"api_docs/core.git_wrapper/#class-gitwrapper","text":"Wrapper class for Git. This class is responsible for handling git interactions, primarily handling versioning of different steps in pipelines.","title":"class GitWrapper"},{"location":"api_docs/core.git_wrapper/#method-__init__","text":"__init__(repo_path: str) Initialize GitWrapper. Should be initialized by ZenML Repository. Args: repo_path: Raises: InvalidGitRepositoryError : If repository is not a git repository. NoSuchPathError : If the repo_path does not exist.","title":"method __init__"},{"location":"api_docs/core.git_wrapper/#method-check_file_committed","text":"check_file_committed(file_path: str) \u2192 bool Checks file is committed. If yes, return True, else False. Args: file_path (str): Path to any file within the ZenML repo.","title":"method check_file_committed"},{"location":"api_docs/core.git_wrapper/#method-check_module_clean","text":"check_module_clean(source: str) \u2192 bool Returns True if all files within source's module are committed. Args: source : relative module path pointing to a Class.","title":"method check_module_clean"},{"location":"api_docs/core.git_wrapper/#method-checkout","text":"checkout( sha_or_branch: Optional[str] = None, directory: Optional[str] = None ) \u2192 None Wrapper for git checkout Args: sha_or_branch : hex string of len 40 representing git sha OR name of branch directory : relative path to directory to scope checkout","title":"method checkout"},{"location":"api_docs/core.git_wrapper/#method-get_current_sha","text":"get_current_sha() \u2192 str Finds the git sha that each file within the module is currently on.","title":"method get_current_sha"},{"location":"api_docs/core.git_wrapper/#method-is_valid_source","text":"is_valid_source(source: str) \u2192 bool Checks whether the source_path is valid or not. Args: source (str): class_source e.g. this.module.Class[@pin].","title":"method is_valid_source"},{"location":"api_docs/core.git_wrapper/#method-load_source_path_class","text":"load_source_path_class(source: str) \u2192 Type[Any] Loads a Python class from the source. Args: source : class_source e.g. this.module.Class[@sha]","title":"method load_source_path_class"},{"location":"api_docs/core.git_wrapper/#method-reset","text":"reset(directory: Optional[str] = None) \u2192 None Wrapper for git reset HEAD <directory> . Args: directory : Relative path to directory to scope checkout","title":"method reset"},{"location":"api_docs/core.git_wrapper/#method-resolve_class","text":"resolve_class(class_: Type[Any]) \u2192 str Resolves a class into a serializable source string. Args: class_ : A Python Class reference. Returns: source_path e.g. this.module.Class[@pin].","title":"method resolve_class"},{"location":"api_docs/core.git_wrapper/#method-resolve_class_source","text":"resolve_class_source(class_source: str) \u2192 str Resolves class_source with an optional pin. Takes source (e.g. this.module.ClassName), and appends relevant sha to it if the files within module are all committed. If even one file is not committed, then returns source unchanged. Args: class_source (str): class_source e.g. this.module.Class","title":"method resolve_class_source"},{"location":"api_docs/core.git_wrapper/#method-stash","text":"stash() \u2192 None Wrapper for git stash","title":"method stash"},{"location":"api_docs/core.git_wrapper/#method-stash_pop","text":"stash_pop() \u2192 None Wrapper for git stash pop. Only pops if there's something to pop. This file was automatically generated via lazydocs .","title":"method stash_pop"},{"location":"api_docs/core.local_service/","text":"module core.local_service Global Variables TYPE_CHECKING REGISTERED_ARTIFACT_STORE REGISTERED_CONTAINER_REGISTRY REGISTERED_METADATA_STORE REGISTERED_ORCHESTRATOR REGISTERED_STACK class LocalService Definition of a local service that keeps track of all ZenML components. method __init__ __init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a LocalService instance. Args: repo_path : Path to the repository of this service. property artifact_stores Returns all registered artifact stores. property container_registries Returns all registered container registries. property metadata_stores Returns all registered metadata stores. property orchestrators Returns all registered orchestrators. method delete delete() \u2192 None Deletes the entire service. Dangerous operation method delete_artifact_store delete_artifact_store(key: str) \u2192 None Delete an artifact_store. Args: key : Unique key of artifact_store. method delete_container_registry delete_container_registry(key: str) \u2192 None Delete a container registry. Args: key : Unique key of the container registry. method delete_metadata_store delete_metadata_store(key: str) \u2192 None Delete a metadata store. Args: key : Unique key of metadata store. method delete_orchestrator delete_orchestrator(key: str) \u2192 None Delete a orchestrator. Args: key : Unique key of orchestrator. method delete_stack delete_stack(key: str) \u2192 None Delete a stack specified with a key. Args: key : Unique key of stack. method get_active_stack_key get_active_stack_key() \u2192 str Returns the active stack key. method get_artifact_store get_artifact_store(key: str) \u2192 BaseArtifactStore Return a single artifact store based on key. Args: key : Unique key of artifact store. Returns: Stack specified by key. method get_container_registry get_container_registry(key: str) \u2192 BaseContainerRegistry Return a single container registry based on key. Args: key : Unique key of a container registry. Returns: Container registry specified by key. method get_metadata_store get_metadata_store(key: str) \u2192 BaseMetadataStore Return a single metadata store based on key. Args: key : Unique key of metadata store. Returns: Metadata store specified by key. method get_orchestrator get_orchestrator(key: str) \u2192 BaseOrchestrator Return a single orchestrator based on key. Args: key : Unique key of orchestrator. Returns: Orchestrator specified by key. method get_serialization_file_name get_serialization_file_name() \u2192 str Return the name of the file where object is serialized. method get_stack get_stack(key: str) \u2192 BaseStack Return a single stack based on key. Args: key : Unique key of stack. Returns: Stack specified by key. method register_artifact_store register_artifact_store(key: str, artifact_store: 'BaseArtifactStore') \u2192 None Register an artifact store. Args: artifact_store : Artifact store to be registered. key : Unique key for the artifact store. method register_metadata_store register_metadata_store(key: str, metadata_store: 'BaseMetadataStore') \u2192 None Register a metadata store. Args: metadata_store : Metadata store to be registered. key : Unique key for the metadata store. method register_orchestrator register_orchestrator(key: str, orchestrator: 'BaseOrchestrator') \u2192 None Register an orchestrator. Args: orchestrator : Orchestrator to be registered. key : Unique key for the orchestrator. method set_active_stack_key set_active_stack_key(stack_key: str) \u2192 None Sets the active stack key. This file was automatically generated via lazydocs .","title":"Core.local service"},{"location":"api_docs/core.local_service/#module-corelocal_service","text":"","title":"module core.local_service"},{"location":"api_docs/core.local_service/#global-variables","text":"TYPE_CHECKING REGISTERED_ARTIFACT_STORE REGISTERED_CONTAINER_REGISTRY REGISTERED_METADATA_STORE REGISTERED_ORCHESTRATOR REGISTERED_STACK","title":"Global Variables"},{"location":"api_docs/core.local_service/#class-localservice","text":"Definition of a local service that keeps track of all ZenML components.","title":"class LocalService"},{"location":"api_docs/core.local_service/#method-__init__","text":"__init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a LocalService instance. Args: repo_path : Path to the repository of this service.","title":"method __init__"},{"location":"api_docs/core.local_service/#property-artifact_stores","text":"Returns all registered artifact stores.","title":"property artifact_stores"},{"location":"api_docs/core.local_service/#property-container_registries","text":"Returns all registered container registries.","title":"property container_registries"},{"location":"api_docs/core.local_service/#property-metadata_stores","text":"Returns all registered metadata stores.","title":"property metadata_stores"},{"location":"api_docs/core.local_service/#property-orchestrators","text":"Returns all registered orchestrators.","title":"property orchestrators"},{"location":"api_docs/core.local_service/#method-delete","text":"delete() \u2192 None Deletes the entire service. Dangerous operation","title":"method delete"},{"location":"api_docs/core.local_service/#method-delete_artifact_store","text":"delete_artifact_store(key: str) \u2192 None Delete an artifact_store. Args: key : Unique key of artifact_store.","title":"method delete_artifact_store"},{"location":"api_docs/core.local_service/#method-delete_container_registry","text":"delete_container_registry(key: str) \u2192 None Delete a container registry. Args: key : Unique key of the container registry.","title":"method delete_container_registry"},{"location":"api_docs/core.local_service/#method-delete_metadata_store","text":"delete_metadata_store(key: str) \u2192 None Delete a metadata store. Args: key : Unique key of metadata store.","title":"method delete_metadata_store"},{"location":"api_docs/core.local_service/#method-delete_orchestrator","text":"delete_orchestrator(key: str) \u2192 None Delete a orchestrator. Args: key : Unique key of orchestrator.","title":"method delete_orchestrator"},{"location":"api_docs/core.local_service/#method-delete_stack","text":"delete_stack(key: str) \u2192 None Delete a stack specified with a key. Args: key : Unique key of stack.","title":"method delete_stack"},{"location":"api_docs/core.local_service/#method-get_active_stack_key","text":"get_active_stack_key() \u2192 str Returns the active stack key.","title":"method get_active_stack_key"},{"location":"api_docs/core.local_service/#method-get_artifact_store","text":"get_artifact_store(key: str) \u2192 BaseArtifactStore Return a single artifact store based on key. Args: key : Unique key of artifact store. Returns: Stack specified by key.","title":"method get_artifact_store"},{"location":"api_docs/core.local_service/#method-get_container_registry","text":"get_container_registry(key: str) \u2192 BaseContainerRegistry Return a single container registry based on key. Args: key : Unique key of a container registry. Returns: Container registry specified by key.","title":"method get_container_registry"},{"location":"api_docs/core.local_service/#method-get_metadata_store","text":"get_metadata_store(key: str) \u2192 BaseMetadataStore Return a single metadata store based on key. Args: key : Unique key of metadata store. Returns: Metadata store specified by key.","title":"method get_metadata_store"},{"location":"api_docs/core.local_service/#method-get_orchestrator","text":"get_orchestrator(key: str) \u2192 BaseOrchestrator Return a single orchestrator based on key. Args: key : Unique key of orchestrator. Returns: Orchestrator specified by key.","title":"method get_orchestrator"},{"location":"api_docs/core.local_service/#method-get_serialization_file_name","text":"get_serialization_file_name() \u2192 str Return the name of the file where object is serialized.","title":"method get_serialization_file_name"},{"location":"api_docs/core.local_service/#method-get_stack","text":"get_stack(key: str) \u2192 BaseStack Return a single stack based on key. Args: key : Unique key of stack. Returns: Stack specified by key.","title":"method get_stack"},{"location":"api_docs/core.local_service/#method-register_artifact_store","text":"register_artifact_store(key: str, artifact_store: 'BaseArtifactStore') \u2192 None Register an artifact store. Args: artifact_store : Artifact store to be registered. key : Unique key for the artifact store.","title":"method register_artifact_store"},{"location":"api_docs/core.local_service/#method-register_metadata_store","text":"register_metadata_store(key: str, metadata_store: 'BaseMetadataStore') \u2192 None Register a metadata store. Args: metadata_store : Metadata store to be registered. key : Unique key for the metadata store.","title":"method register_metadata_store"},{"location":"api_docs/core.local_service/#method-register_orchestrator","text":"register_orchestrator(key: str, orchestrator: 'BaseOrchestrator') \u2192 None Register an orchestrator. Args: orchestrator : Orchestrator to be registered. key : Unique key for the orchestrator.","title":"method register_orchestrator"},{"location":"api_docs/core.local_service/#method-set_active_stack_key","text":"set_active_stack_key(stack_key: str) \u2192 None Sets the active stack key. This file was automatically generated via lazydocs .","title":"method set_active_stack_key"},{"location":"api_docs/core.mapping_utils/","text":"module core.mapping_utils function get_key_from_uuid get_key_from_uuid(uuid: UUID, mapping: Dict[str, UUIDSourceTuple]) \u2192 str Return the key that points to a certain uuid in a mapping. Args: uuid : uuid to query. mapping : Dict mapping keys to UUIDs and source information. Returns: Returns the key from the mapping. function get_component_from_key get_component_from_key( key: str, mapping: Dict[str, UUIDSourceTuple], repo_path: str ) \u2192 BaseComponent Given a key and a mapping, return an initialized component. Args: key : Unique key. mapping : Dict of type str -> UUIDSourceTuple. repo_path : Path to the repo from which to load the component. Returns: An object which is a subclass of type BaseComponent. function get_components_from_store get_components_from_store( store_name: str, mapping: Dict[str, UUIDSourceTuple], repo_path: str ) \u2192 Dict[str, BaseComponent] Returns a list of components from a store. Args: store_name : Name of the store. mapping : Dict of type str -> UUIDSourceTuple. repo_path : Path to the repo from which to load the components. Returns: A dict of objects which are a subclass of type BaseComponent. class UUIDSourceTuple Container used to store UUID and source information of a single BaseComponent subclass. Attributes: uuid : Identifier of the BaseComponent source : Contains the fully qualified class name and information about a git hash/tag. E.g. foo.bar.BaseComponentSubclass@git_tag This file was automatically generated via lazydocs .","title":"Core.mapping utils"},{"location":"api_docs/core.mapping_utils/#module-coremapping_utils","text":"","title":"module core.mapping_utils"},{"location":"api_docs/core.mapping_utils/#function-get_key_from_uuid","text":"get_key_from_uuid(uuid: UUID, mapping: Dict[str, UUIDSourceTuple]) \u2192 str Return the key that points to a certain uuid in a mapping. Args: uuid : uuid to query. mapping : Dict mapping keys to UUIDs and source information. Returns: Returns the key from the mapping.","title":"function get_key_from_uuid"},{"location":"api_docs/core.mapping_utils/#function-get_component_from_key","text":"get_component_from_key( key: str, mapping: Dict[str, UUIDSourceTuple], repo_path: str ) \u2192 BaseComponent Given a key and a mapping, return an initialized component. Args: key : Unique key. mapping : Dict of type str -> UUIDSourceTuple. repo_path : Path to the repo from which to load the component. Returns: An object which is a subclass of type BaseComponent.","title":"function get_component_from_key"},{"location":"api_docs/core.mapping_utils/#function-get_components_from_store","text":"get_components_from_store( store_name: str, mapping: Dict[str, UUIDSourceTuple], repo_path: str ) \u2192 Dict[str, BaseComponent] Returns a list of components from a store. Args: store_name : Name of the store. mapping : Dict of type str -> UUIDSourceTuple. repo_path : Path to the repo from which to load the components. Returns: A dict of objects which are a subclass of type BaseComponent.","title":"function get_components_from_store"},{"location":"api_docs/core.mapping_utils/#class-uuidsourcetuple","text":"Container used to store UUID and source information of a single BaseComponent subclass. Attributes: uuid : Identifier of the BaseComponent source : Contains the fully qualified class name and information about a git hash/tag. E.g. foo.bar.BaseComponentSubclass@git_tag This file was automatically generated via lazydocs .","title":"class UUIDSourceTuple"},{"location":"api_docs/core/","text":"module core The core module is where all the base ZenML functionality is defined, including a Pydantic base class for components, a git wrapper and a class for ZenML's own repository methods. This module is also where the local service functionality (which keeps track of all the ZenML components) is defined. Every ZenML project has its own ZenML repository, and the repo module is where associated methods are defined. The repo.init_repo method is where all our functionality is kickstarted when you first initialize everything through the `zenml init CLI command. This file was automatically generated via lazydocs .","title":"Core"},{"location":"api_docs/core/#module-core","text":"The core module is where all the base ZenML functionality is defined, including a Pydantic base class for components, a git wrapper and a class for ZenML's own repository methods. This module is also where the local service functionality (which keeps track of all the ZenML components) is defined. Every ZenML project has its own ZenML repository, and the repo module is where associated methods are defined. The repo.init_repo method is where all our functionality is kickstarted when you first initialize everything through the `zenml init CLI command. This file was automatically generated via lazydocs .","title":"module core"},{"location":"api_docs/core.repo/","text":"module core.repo Base ZenML repository Global Variables ZENML_DIR_NAME GET_PIPELINE GET_PIPELINES SET_STACK class Repository ZenML repository definition. Every ZenML project exists inside a ZenML repository. method __init__ __init__(path: Optional[str] = None) Construct reference to a ZenML repository. Args: path (str): Path to root of repository method clean clean() \u2192 None Deletes associated metadata store, pipelines dir and artifacts method get_active_stack get_active_stack() \u2192 BaseStack Get the active stack from global config. Returns: Currently active stack. method get_active_stack_key get_active_stack_key() \u2192 str Get the active stack key from global config. Returns: Currently active stacks key. method get_git_wrapper get_git_wrapper() \u2192 GitWrapper Returns the git wrapper for the repo. method get_service get_service() \u2192 LocalService Returns the active service. For now, always local. method init_repo init_repo(path: str = '/home/apenner/PycharmProjects/zenml') \u2192 None Initializes a ZenML repository. Args: path : Path where the ZenML repository should be created. Raises: InitializationException : If a ZenML repository already exists at the given path. This file was automatically generated via lazydocs .","title":"Core.repo"},{"location":"api_docs/core.repo/#module-corerepo","text":"Base ZenML repository","title":"module core.repo"},{"location":"api_docs/core.repo/#global-variables","text":"ZENML_DIR_NAME GET_PIPELINE GET_PIPELINES SET_STACK","title":"Global Variables"},{"location":"api_docs/core.repo/#class-repository","text":"ZenML repository definition. Every ZenML project exists inside a ZenML repository.","title":"class Repository"},{"location":"api_docs/core.repo/#method-__init__","text":"__init__(path: Optional[str] = None) Construct reference to a ZenML repository. Args: path (str): Path to root of repository","title":"method __init__"},{"location":"api_docs/core.repo/#method-clean","text":"clean() \u2192 None Deletes associated metadata store, pipelines dir and artifacts","title":"method clean"},{"location":"api_docs/core.repo/#method-get_active_stack","text":"get_active_stack() \u2192 BaseStack Get the active stack from global config. Returns: Currently active stack.","title":"method get_active_stack"},{"location":"api_docs/core.repo/#method-get_active_stack_key","text":"get_active_stack_key() \u2192 str Get the active stack key from global config. Returns: Currently active stacks key.","title":"method get_active_stack_key"},{"location":"api_docs/core.repo/#method-get_git_wrapper","text":"get_git_wrapper() \u2192 GitWrapper Returns the git wrapper for the repo.","title":"method get_git_wrapper"},{"location":"api_docs/core.repo/#method-get_service","text":"get_service() \u2192 LocalService Returns the active service. For now, always local.","title":"method get_service"},{"location":"api_docs/core.repo/#method-init_repo","text":"init_repo(path: str = '/home/apenner/PycharmProjects/zenml') \u2192 None Initializes a ZenML repository. Args: path : Path where the ZenML repository should be created. Raises: InitializationException : If a ZenML repository already exists at the given path. This file was automatically generated via lazydocs .","title":"method init_repo"},{"location":"api_docs/core.utils/","text":"module core.utils function define_json_config_settings_source define_json_config_settings_source( config_dir: str, config_name: str ) \u2192 Callable[[ForwardRef('BaseSettings')], Dict[str, Any]] Define a function to essentially deserialize a model from a serialized json config. Args: config_dir : A path to a dir where we want the config file to exist. config_name : Full name of config file. Returns: A json_config_settings_source callable reading from the passed path. function generate_customise_sources generate_customise_sources( file_dir: str, file_name: str ) \u2192 Callable[[Type[Config], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]]], Tuple[Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], ]] Generate a customise_sources function as defined here: https://pydantic-docs.helpmanual.io/usage/settings/. This function generates a function that configures the priorities of the sources through which the model is loaded. The important thing to note here is that the define_json_config_settings_source is dynamically generated with the provided file_dir and file_name. This allows us to dynamically generate a file name for the serialization and deserialization of the model. Args: file_dir : Dir where file is stored. file_name : Name of the file to persist. Returns: A customise_sources class method to be defined the a Pydantic BaseSettings inner Config class. This file was automatically generated via lazydocs .","title":"Core.utils"},{"location":"api_docs/core.utils/#module-coreutils","text":"","title":"module core.utils"},{"location":"api_docs/core.utils/#function-define_json_config_settings_source","text":"define_json_config_settings_source( config_dir: str, config_name: str ) \u2192 Callable[[ForwardRef('BaseSettings')], Dict[str, Any]] Define a function to essentially deserialize a model from a serialized json config. Args: config_dir : A path to a dir where we want the config file to exist. config_name : Full name of config file. Returns: A json_config_settings_source callable reading from the passed path.","title":"function define_json_config_settings_source"},{"location":"api_docs/core.utils/#function-generate_customise_sources","text":"generate_customise_sources( file_dir: str, file_name: str ) \u2192 Callable[[Type[Config], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], Callable[[ForwardRef('BaseSettings')], Dict[str, Any]]], Tuple[Callable[[ForwardRef('BaseSettings')], Dict[str, Any]], ]] Generate a customise_sources function as defined here: https://pydantic-docs.helpmanual.io/usage/settings/. This function generates a function that configures the priorities of the sources through which the model is loaded. The important thing to note here is that the define_json_config_settings_source is dynamically generated with the provided file_dir and file_name. This allows us to dynamically generate a file name for the serialization and deserialization of the model. Args: file_dir : Dir where file is stored. file_name : Name of the file to persist. Returns: A customise_sources class method to be defined the a Pydantic BaseSettings inner Config class. This file was automatically generated via lazydocs .","title":"function generate_customise_sources"},{"location":"api_docs/enums/","text":"module enums class ArtifactStoreTypes All supported Artifact Store types. class MLMetadataTypes All supported ML Metadata types. class OrchestratorTypes All supported Orchestrator types class StackTypes All supported Stack types. class ExecutionStatus Enum that represents the current status of a step or pipeline run. class LoggingLevels Enum for logging levels. This file was automatically generated via lazydocs .","title":"Enums"},{"location":"api_docs/enums/#module-enums","text":"","title":"module enums"},{"location":"api_docs/enums/#class-artifactstoretypes","text":"All supported Artifact Store types.","title":"class ArtifactStoreTypes"},{"location":"api_docs/enums/#class-mlmetadatatypes","text":"All supported ML Metadata types.","title":"class MLMetadataTypes"},{"location":"api_docs/enums/#class-orchestratortypes","text":"All supported Orchestrator types","title":"class OrchestratorTypes"},{"location":"api_docs/enums/#class-stacktypes","text":"All supported Stack types.","title":"class StackTypes"},{"location":"api_docs/enums/#class-executionstatus","text":"Enum that represents the current status of a step or pipeline run.","title":"class ExecutionStatus"},{"location":"api_docs/enums/#class-logginglevels","text":"Enum for logging levels. This file was automatically generated via lazydocs .","title":"class LoggingLevels"},{"location":"api_docs/exceptions/","text":"module exceptions ZenML specific exception definitions Global Variables TYPE_CHECKING class InitializationException Raises exception when a function is run before zenml initialization. method __init__ __init__(message: str = 'ZenML config is none. Did you do `zenml init`?') class EmptyDatasourceException Raises exception when a datasource data is accessed without running an associated pipeline. method __init__ __init__( message: str = 'This datasource has not been used in any pipelines, therefore the associated data has no versions. Please use this datasource in any ZenML pipeline with `pipeline.add_datasource(datasource)`' ) class DoesNotExistException Raises exception when the entity does not exist in the system but an action is being done that requires it to be present. method __init__ __init__(message: str) class AlreadyExistsException Raises exception when the name already exist in the system but an action is trying to create a resource with the same name. method __init__ __init__(message: Optional[str] = None, name: str = '', resource_type: str = '') class PipelineNotSucceededException Raises exception when trying to fetch artifacts from a not succeeded pipeline. method __init__ __init__(name: str = '', message: str = '{} is not yet completed successfully.') class GitException Raises exception when a problem occurs in git resolution. method __init__ __init__( message: str = 'There is a problem with git resolution. Please make sure that all relevant files are committed.' ) class StepInterfaceError Raises exception when interacting with the Step interface in an unsupported way. class StepContextError Raises exception when interacting with a StepContext in an unsupported way. class PipelineInterfaceError Raises exception when interacting with the Pipeline interface in an unsupported way. class ArtifactInterfaceError Raises exception when interacting with the Artifact interface in an unsupported way. class PipelineConfigurationError Raises exceptions when a pipeline configuration contains invalid values. class MissingStepParameterError Raises exceptions when a step parameter is missing when running a pipeline. method __init__ __init__( step_name: str, missing_parameters: List[str], config_class: Type[ForwardRef('BaseStepConfig')] ) Initializes a MissingStepParameterError object. Args: step_name : Name of the step for which one or more parameters are missing. missing_parameters : Names of all parameters which are missing. config_class : Class of the configuration object for which the parameters are missing. class IntegrationError Raises exceptions when a requested integration can not be activated. class DuplicateRunNameError Raises exception when a run with the same name already exists. method __init__ __init__( message: str = 'Unable to run a pipeline with a run name that already exists.' ) This file was automatically generated via lazydocs .","title":"Exceptions"},{"location":"api_docs/exceptions/#module-exceptions","text":"ZenML specific exception definitions","title":"module exceptions"},{"location":"api_docs/exceptions/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/exceptions/#class-initializationexception","text":"Raises exception when a function is run before zenml initialization.","title":"class InitializationException"},{"location":"api_docs/exceptions/#method-__init__","text":"__init__(message: str = 'ZenML config is none. Did you do `zenml init`?')","title":"method __init__"},{"location":"api_docs/exceptions/#class-emptydatasourceexception","text":"Raises exception when a datasource data is accessed without running an associated pipeline.","title":"class EmptyDatasourceException"},{"location":"api_docs/exceptions/#method-__init___1","text":"__init__( message: str = 'This datasource has not been used in any pipelines, therefore the associated data has no versions. Please use this datasource in any ZenML pipeline with `pipeline.add_datasource(datasource)`' )","title":"method __init__"},{"location":"api_docs/exceptions/#class-doesnotexistexception","text":"Raises exception when the entity does not exist in the system but an action is being done that requires it to be present.","title":"class DoesNotExistException"},{"location":"api_docs/exceptions/#method-__init___2","text":"__init__(message: str)","title":"method __init__"},{"location":"api_docs/exceptions/#class-alreadyexistsexception","text":"Raises exception when the name already exist in the system but an action is trying to create a resource with the same name.","title":"class AlreadyExistsException"},{"location":"api_docs/exceptions/#method-__init___3","text":"__init__(message: Optional[str] = None, name: str = '', resource_type: str = '')","title":"method __init__"},{"location":"api_docs/exceptions/#class-pipelinenotsucceededexception","text":"Raises exception when trying to fetch artifacts from a not succeeded pipeline.","title":"class PipelineNotSucceededException"},{"location":"api_docs/exceptions/#method-__init___4","text":"__init__(name: str = '', message: str = '{} is not yet completed successfully.')","title":"method __init__"},{"location":"api_docs/exceptions/#class-gitexception","text":"Raises exception when a problem occurs in git resolution.","title":"class GitException"},{"location":"api_docs/exceptions/#method-__init___5","text":"__init__( message: str = 'There is a problem with git resolution. Please make sure that all relevant files are committed.' )","title":"method __init__"},{"location":"api_docs/exceptions/#class-stepinterfaceerror","text":"Raises exception when interacting with the Step interface in an unsupported way.","title":"class StepInterfaceError"},{"location":"api_docs/exceptions/#class-stepcontexterror","text":"Raises exception when interacting with a StepContext in an unsupported way.","title":"class StepContextError"},{"location":"api_docs/exceptions/#class-pipelineinterfaceerror","text":"Raises exception when interacting with the Pipeline interface in an unsupported way.","title":"class PipelineInterfaceError"},{"location":"api_docs/exceptions/#class-artifactinterfaceerror","text":"Raises exception when interacting with the Artifact interface in an unsupported way.","title":"class ArtifactInterfaceError"},{"location":"api_docs/exceptions/#class-pipelineconfigurationerror","text":"Raises exceptions when a pipeline configuration contains invalid values.","title":"class PipelineConfigurationError"},{"location":"api_docs/exceptions/#class-missingstepparametererror","text":"Raises exceptions when a step parameter is missing when running a pipeline.","title":"class MissingStepParameterError"},{"location":"api_docs/exceptions/#method-__init___6","text":"__init__( step_name: str, missing_parameters: List[str], config_class: Type[ForwardRef('BaseStepConfig')] ) Initializes a MissingStepParameterError object. Args: step_name : Name of the step for which one or more parameters are missing. missing_parameters : Names of all parameters which are missing. config_class : Class of the configuration object for which the parameters are missing.","title":"method __init__"},{"location":"api_docs/exceptions/#class-integrationerror","text":"Raises exceptions when a requested integration can not be activated.","title":"class IntegrationError"},{"location":"api_docs/exceptions/#class-duplicaterunnameerror","text":"Raises exception when a run with the same name already exists.","title":"class DuplicateRunNameError"},{"location":"api_docs/exceptions/#method-__init___7","text":"__init__( message: str = 'Unable to run a pipeline with a run name that already exists.' ) This file was automatically generated via lazydocs .","title":"method __init__"},{"location":"api_docs/integrations.airflow/","text":"module integrations.airflow The Airflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Airflow orchestrator with the CLI tool, then bootstrap using the zenml orchestrator up command. Global Variables AIRFLOW class AirflowIntegration Definition of Airflow Integration for ZenML. classmethod activate activate() Activates all classes required for the airflow integration. This file was automatically generated via lazydocs .","title":"Integrations.airflow"},{"location":"api_docs/integrations.airflow/#module-integrationsairflow","text":"The Airflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Airflow orchestrator with the CLI tool, then bootstrap using the zenml orchestrator up command.","title":"module integrations.airflow"},{"location":"api_docs/integrations.airflow/#global-variables","text":"AIRFLOW","title":"Global Variables"},{"location":"api_docs/integrations.airflow/#class-airflowintegration","text":"Definition of Airflow Integration for ZenML.","title":"class AirflowIntegration"},{"location":"api_docs/integrations.airflow/#classmethod-activate","text":"activate() Activates all classes required for the airflow integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/","text":"module integrations.airflow.orchestrators.airflow_component Definition for Airflow component for TFX. class AirflowComponent Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow. method __init__ __init__( parent_dag: DAG, pipeline_node: PipelineNode, mlmd_connection: Metadata, pipeline_info: PipelineInfo, pipeline_runtime_spec: PipelineRuntimeSpec, executor_spec: Optional[Message] = None, custom_driver_spec: Optional[Message] = None ) \u2192 None Constructs an Airflow implementation of TFX component. Args: parent_dag : The airflow DAG that this component is contained in. pipeline_node : The specification of the node to launch. mlmd_connection : ML metadata connection info. pipeline_info : The information of the pipeline that this node runs in. pipeline_runtime_spec : The runtime information of the pipeline that this node runs in. executor_spec : Specification for the executor of the node. custom_driver_spec : Specification for custom driver. property dag Returns the Operator's DAG if set, otherwise raises an error property dag_id Returns dag id if it has one or an adhoc + owner property downstream_list @property: list of tasks directly downstream property downstream_task_ids @property: set of ids of tasks directly downstream property inherits_from_dummy_operator Used to determine if an Operator is inherited from DummyOperator property leaves Required by TaskMixin property log Returns a logger. property output Returns reference to XCom pushed by current operator property priority_weight_total Total priority weight for the task. It might include all upstream or downstream tasks. depending on the weight rule. WeightRule.ABSOLUTE - only own weight WeightRule.DOWNSTREAM - adds priority weight of all downstream tasks WeightRule.UPSTREAM - adds priority weight of all upstream tasks property roots Required by TaskMixin property task_type @property: type of the task property upstream_list @property: list of tasks directly upstream property upstream_task_ids @property: set of ids of tasks directly upstream This file was automatically generated via lazydocs .","title":"Integrations.airflow.orchestrators.airflow component"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#module-integrationsairfloworchestratorsairflow_component","text":"Definition for Airflow component for TFX.","title":"module integrations.airflow.orchestrators.airflow_component"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#class-airflowcomponent","text":"Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow.","title":"class AirflowComponent"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#method-__init__","text":"__init__( parent_dag: DAG, pipeline_node: PipelineNode, mlmd_connection: Metadata, pipeline_info: PipelineInfo, pipeline_runtime_spec: PipelineRuntimeSpec, executor_spec: Optional[Message] = None, custom_driver_spec: Optional[Message] = None ) \u2192 None Constructs an Airflow implementation of TFX component. Args: parent_dag : The airflow DAG that this component is contained in. pipeline_node : The specification of the node to launch. mlmd_connection : ML metadata connection info. pipeline_info : The information of the pipeline that this node runs in. pipeline_runtime_spec : The runtime information of the pipeline that this node runs in. executor_spec : Specification for the executor of the node. custom_driver_spec : Specification for custom driver.","title":"method __init__"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-dag","text":"Returns the Operator's DAG if set, otherwise raises an error","title":"property dag"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-dag_id","text":"Returns dag id if it has one or an adhoc + owner","title":"property dag_id"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-downstream_list","text":"@property: list of tasks directly downstream","title":"property downstream_list"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-downstream_task_ids","text":"@property: set of ids of tasks directly downstream","title":"property downstream_task_ids"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-inherits_from_dummy_operator","text":"Used to determine if an Operator is inherited from DummyOperator","title":"property inherits_from_dummy_operator"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-leaves","text":"Required by TaskMixin","title":"property leaves"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-log","text":"Returns a logger.","title":"property log"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-output","text":"Returns reference to XCom pushed by current operator","title":"property output"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-priority_weight_total","text":"Total priority weight for the task. It might include all upstream or downstream tasks. depending on the weight rule. WeightRule.ABSOLUTE - only own weight WeightRule.DOWNSTREAM - adds priority weight of all downstream tasks WeightRule.UPSTREAM - adds priority weight of all upstream tasks","title":"property priority_weight_total"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-roots","text":"Required by TaskMixin","title":"property roots"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-task_type","text":"@property: type of the task","title":"property task_type"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-upstream_list","text":"@property: list of tasks directly upstream","title":"property upstream_list"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_component/#property-upstream_task_ids","text":"@property: set of ids of tasks directly upstream This file was automatically generated via lazydocs .","title":"property upstream_task_ids"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_dag_runner/","text":"module integrations.airflow.orchestrators.airflow_dag_runner Definition of Airflow TFX runner. This is an unmodified copy from the TFX source code (outside of superficial, stylistic changes) Global Variables TYPE_CHECKING class AirflowPipelineConfig Pipeline config for AirflowDagRunner. method __init__ __init__(airflow_dag_config: Optional[Dict[str, Any]] = None, **kwargs: Any) Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config : Configs of Airflow DAG model. See https : //airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs : keyword args for PipelineConfig. class AirflowDagRunner Tfx runner on Airflow. method __init__ __init__(config: Optional[Dict[str, Any], AirflowPipelineConfig] = None) Creates an instance of AirflowDagRunner. Args: config : Optional Airflow pipeline config for customizing the launching of each component. property config method run run(pipeline: Pipeline, run_name: str = '') \u2192 DAG Deploys given logical pipeline on Airflow. Args: pipeline : Logical pipeline containing pipeline args and comps. run_name : Optional name for the run. Returns: An Airflow DAG. This file was automatically generated via lazydocs .","title":"Integrations.airflow.orchestrators.airflow dag runner"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_dag_runner/#module-integrationsairfloworchestratorsairflow_dag_runner","text":"Definition of Airflow TFX runner. This is an unmodified copy from the TFX source code (outside of superficial, stylistic changes)","title":"module integrations.airflow.orchestrators.airflow_dag_runner"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_dag_runner/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_dag_runner/#class-airflowpipelineconfig","text":"Pipeline config for AirflowDagRunner.","title":"class AirflowPipelineConfig"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_dag_runner/#method-__init__","text":"__init__(airflow_dag_config: Optional[Dict[str, Any]] = None, **kwargs: Any) Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config : Configs of Airflow DAG model. See https : //airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs : keyword args for PipelineConfig.","title":"method __init__"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_dag_runner/#class-airflowdagrunner","text":"Tfx runner on Airflow.","title":"class AirflowDagRunner"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_dag_runner/#method-__init___1","text":"__init__(config: Optional[Dict[str, Any], AirflowPipelineConfig] = None) Creates an instance of AirflowDagRunner. Args: config : Optional Airflow pipeline config for customizing the launching of each component.","title":"method __init__"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_dag_runner/#property-config","text":"","title":"property config"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_dag_runner/#method-run","text":"run(pipeline: Pipeline, run_name: str = '') \u2192 DAG Deploys given logical pipeline on Airflow. Args: pipeline : Logical pipeline containing pipeline args and comps. run_name : Optional name for the run. Returns: An Airflow DAG. This file was automatically generated via lazydocs .","title":"method run"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/","text":"module integrations.airflow.orchestrators.airflow_orchestrator Global Variables TYPE_CHECKING AIRFLOW_ROOT_DIR class AirflowOrchestrator Orchestrator responsible for running pipelines using Airflow. method __init__ __init__(**values: Any) Sets environment variables to configure airflow. property dags_directory Returns path to the airflow dags directory. property is_running Returns whether the airflow daemon is currently running. property log_file Returns path to the airflow log file. property password_file Returns path to the webserver password file. property pid_file Returns path to the daemon PID file. method down down() \u2192 None Stops the airflow daemon if necessary and tears down resources. method pre_run pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Args: pipeline : Pipeline that will be run. caller_filepath : Path to the file in which pipeline.run() was called. This contains the airflow DAG that is returned by the run() method. Raises: RuntimeError : If airflow is not running. method run run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 DAG Prepares the pipeline so it can be run in Airflow. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Unused argument to conform with base class signature. classmethod set_airflow_home set_airflow_home(values: Dict[str, Any]) \u2192 Dict[str, Any] Sets airflow home according to orchestrator UUID. method up up() \u2192 None Ensures that Airflow is running. This file was automatically generated via lazydocs .","title":"Integrations.airflow.orchestrators.airflow orchestrator"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#module-integrationsairfloworchestratorsairflow_orchestrator","text":"","title":"module integrations.airflow.orchestrators.airflow_orchestrator"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#global-variables","text":"TYPE_CHECKING AIRFLOW_ROOT_DIR","title":"Global Variables"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#class-airfloworchestrator","text":"Orchestrator responsible for running pipelines using Airflow.","title":"class AirflowOrchestrator"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#method-__init__","text":"__init__(**values: Any) Sets environment variables to configure airflow.","title":"method __init__"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#property-dags_directory","text":"Returns path to the airflow dags directory.","title":"property dags_directory"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#property-is_running","text":"Returns whether the airflow daemon is currently running.","title":"property is_running"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#property-log_file","text":"Returns path to the airflow log file.","title":"property log_file"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#property-password_file","text":"Returns path to the webserver password file.","title":"property password_file"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#property-pid_file","text":"Returns path to the daemon PID file.","title":"property pid_file"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#method-down","text":"down() \u2192 None Stops the airflow daemon if necessary and tears down resources.","title":"method down"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#method-pre_run","text":"pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Args: pipeline : Pipeline that will be run. caller_filepath : Path to the file in which pipeline.run() was called. This contains the airflow DAG that is returned by the run() method. Raises: RuntimeError : If airflow is not running.","title":"method pre_run"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#method-run","text":"run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 DAG Prepares the pipeline so it can be run in Airflow. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Unused argument to conform with base class signature.","title":"method run"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#classmethod-set_airflow_home","text":"set_airflow_home(values: Dict[str, Any]) \u2192 Dict[str, Any] Sets airflow home according to orchestrator UUID.","title":"classmethod set_airflow_home"},{"location":"api_docs/integrations.airflow.orchestrators.airflow_orchestrator/#method-up","text":"up() \u2192 None Ensures that Airflow is running. This file was automatically generated via lazydocs .","title":"method up"},{"location":"api_docs/integrations.airflow.orchestrators/","text":"module integrations.airflow.orchestrators This file was automatically generated via lazydocs .","title":"Integrations.airflow.orchestrators"},{"location":"api_docs/integrations.airflow.orchestrators/#module-integrationsairfloworchestrators","text":"This file was automatically generated via lazydocs .","title":"module integrations.airflow.orchestrators"},{"location":"api_docs/integrations.constants/","text":"module integrations.constants Global Variables AIRFLOW DASH EVIDENTLY FACETS GCP GRAPHVIZ KUBEFLOW MLFLOW PLOTLY PYTORCH PYTORCH_L SKLEARN TENSORFLOW This file was automatically generated via lazydocs .","title":"Integrations.constants"},{"location":"api_docs/integrations.constants/#module-integrationsconstants","text":"","title":"module integrations.constants"},{"location":"api_docs/integrations.constants/#global-variables","text":"AIRFLOW DASH EVIDENTLY FACETS GCP GRAPHVIZ KUBEFLOW MLFLOW PLOTLY PYTORCH PYTORCH_L SKLEARN TENSORFLOW This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/integrations.dash/","text":"module integrations.dash Global Variables DASH class DashIntegration Definition of Dash integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.dash"},{"location":"api_docs/integrations.dash/#module-integrationsdash","text":"","title":"module integrations.dash"},{"location":"api_docs/integrations.dash/#global-variables","text":"DASH","title":"Global Variables"},{"location":"api_docs/integrations.dash/#class-dashintegration","text":"Definition of Dash integration for ZenML. This file was automatically generated via lazydocs .","title":"class DashIntegration"},{"location":"api_docs/integrations.dash.visualizers/","text":"module integrations.dash.visualizers This file was automatically generated via lazydocs .","title":"Integrations.dash.visualizers"},{"location":"api_docs/integrations.dash.visualizers/#module-integrationsdashvisualizers","text":"This file was automatically generated via lazydocs .","title":"module integrations.dash.visualizers"},{"location":"api_docs/integrations.dash.visualizers.pipeline_run_lineage_visualizer/","text":"module integrations.dash.visualizers.pipeline_run_lineage_visualizer Global Variables OVERALL_STYLE COLOR_RED COLOR_BLUE COLOR_YELLOW COLOR_GREEN STYLESHEET class PipelineRunLineageVisualizer Implementation of a lineage diagram via the dash and dash-cyctoscape library. method visualize visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 Dash Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. This file was automatically generated via lazydocs .","title":"Integrations.dash.visualizers.pipeline run lineage visualizer"},{"location":"api_docs/integrations.dash.visualizers.pipeline_run_lineage_visualizer/#module-integrationsdashvisualizerspipeline_run_lineage_visualizer","text":"","title":"module integrations.dash.visualizers.pipeline_run_lineage_visualizer"},{"location":"api_docs/integrations.dash.visualizers.pipeline_run_lineage_visualizer/#global-variables","text":"OVERALL_STYLE COLOR_RED COLOR_BLUE COLOR_YELLOW COLOR_GREEN STYLESHEET","title":"Global Variables"},{"location":"api_docs/integrations.dash.visualizers.pipeline_run_lineage_visualizer/#class-pipelinerunlineagevisualizer","text":"Implementation of a lineage diagram via the dash and dash-cyctoscape library.","title":"class PipelineRunLineageVisualizer"},{"location":"api_docs/integrations.dash.visualizers.pipeline_run_lineage_visualizer/#method-visualize","text":"visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 Dash Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"api_docs/integrations.evidently/","text":"module integrations.evidently The Evidently integration provides a way to monitor your models in production. It includes a way to detect data drift and different kinds of model performance issues. The results of Evidently calculations can either be exported as an interactive dashboard (visualized as an html file or in your Jupyter notebook), or as a JSON file. Global Variables EVIDENTLY class EvidentlyIntegration Definition of Evidently integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.evidently"},{"location":"api_docs/integrations.evidently/#module-integrationsevidently","text":"The Evidently integration provides a way to monitor your models in production. It includes a way to detect data drift and different kinds of model performance issues. The results of Evidently calculations can either be exported as an interactive dashboard (visualized as an html file or in your Jupyter notebook), or as a JSON file.","title":"module integrations.evidently"},{"location":"api_docs/integrations.evidently/#global-variables","text":"EVIDENTLY","title":"Global Variables"},{"location":"api_docs/integrations.evidently/#class-evidentlyintegration","text":"Definition of Evidently integration for ZenML. This file was automatically generated via lazydocs .","title":"class EvidentlyIntegration"},{"location":"api_docs/integrations.evidently.steps.evidently_profile/","text":"module integrations.evidently.steps.evidently_profile Global Variables profile_mapper dashboard_mapper class EvidentlyProfileConfig Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" method get_profile_sections_and_tabs get_profile_sections_and_tabs() \u2192 Tuple[List[ProfileSection], List[Tab]] class EvidentlyProfileStep Simple step implementation which implements Evidently's functionality for creating a profile. property component Returns a TFX component. method entrypoint entrypoint( reference_dataset: DataArtifact, comparison_dataset: DataArtifact, config: EvidentlyProfileConfig, context: StepContext ) \u2192 <Output object at 0x7f4c6fb74940> Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset : a Pandas dataframe comparison_dataset : a Pandas dataframe of new data you wish to compare against the reference data config : the configuration for the step context : the context of the step Returns: profile : dictionary report extracted from an Evidently Profile generated for the data drift dashboard : HTML report extracted from an Evidently Dashboard generated for the data drift This file was automatically generated via lazydocs .","title":"Integrations.evidently.steps.evidently profile"},{"location":"api_docs/integrations.evidently.steps.evidently_profile/#module-integrationsevidentlystepsevidently_profile","text":"","title":"module integrations.evidently.steps.evidently_profile"},{"location":"api_docs/integrations.evidently.steps.evidently_profile/#global-variables","text":"profile_mapper dashboard_mapper","title":"Global Variables"},{"location":"api_docs/integrations.evidently.steps.evidently_profile/#class-evidentlyprofileconfig","text":"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\"","title":"class EvidentlyProfileConfig"},{"location":"api_docs/integrations.evidently.steps.evidently_profile/#method-get_profile_sections_and_tabs","text":"get_profile_sections_and_tabs() \u2192 Tuple[List[ProfileSection], List[Tab]]","title":"method get_profile_sections_and_tabs"},{"location":"api_docs/integrations.evidently.steps.evidently_profile/#class-evidentlyprofilestep","text":"Simple step implementation which implements Evidently's functionality for creating a profile.","title":"class EvidentlyProfileStep"},{"location":"api_docs/integrations.evidently.steps.evidently_profile/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/integrations.evidently.steps.evidently_profile/#method-entrypoint","text":"entrypoint( reference_dataset: DataArtifact, comparison_dataset: DataArtifact, config: EvidentlyProfileConfig, context: StepContext ) \u2192 <Output object at 0x7f4c6fb74940> Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset : a Pandas dataframe comparison_dataset : a Pandas dataframe of new data you wish to compare against the reference data config : the configuration for the step context : the context of the step Returns: profile : dictionary report extracted from an Evidently Profile generated for the data drift dashboard : HTML report extracted from an Evidently Dashboard generated for the data drift This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/integrations.evidently.steps/","text":"module integrations.evidently.steps This file was automatically generated via lazydocs .","title":"Integrations.evidently.steps"},{"location":"api_docs/integrations.evidently.steps/#module-integrationsevidentlysteps","text":"This file was automatically generated via lazydocs .","title":"module integrations.evidently.steps"},{"location":"api_docs/integrations.evidently.visualizers.evidently_visualizer/","text":"module integrations.evidently.visualizers.evidently_visualizer class EvidentlyVisualizer The implementation of an Evidently Visualizer. method generate_facet generate_facet(html_: str) \u2192 None Generate a Facet Overview Args: h : HTML represented as a string. method visualize visualize(object: StepView, *args: Any, **kwargs: Any) \u2192 None Method to visualize components Args: object : StepView fetched from run.get_step(). This file was automatically generated via lazydocs .","title":"Integrations.evidently.visualizers.evidently visualizer"},{"location":"api_docs/integrations.evidently.visualizers.evidently_visualizer/#module-integrationsevidentlyvisualizersevidently_visualizer","text":"","title":"module integrations.evidently.visualizers.evidently_visualizer"},{"location":"api_docs/integrations.evidently.visualizers.evidently_visualizer/#class-evidentlyvisualizer","text":"The implementation of an Evidently Visualizer.","title":"class EvidentlyVisualizer"},{"location":"api_docs/integrations.evidently.visualizers.evidently_visualizer/#method-generate_facet","text":"generate_facet(html_: str) \u2192 None Generate a Facet Overview Args: h : HTML represented as a string.","title":"method generate_facet"},{"location":"api_docs/integrations.evidently.visualizers.evidently_visualizer/#method-visualize","text":"visualize(object: StepView, *args: Any, **kwargs: Any) \u2192 None Method to visualize components Args: object : StepView fetched from run.get_step(). This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"api_docs/integrations.evidently.visualizers/","text":"module integrations.evidently.visualizers This file was automatically generated via lazydocs .","title":"Integrations.evidently.visualizers"},{"location":"api_docs/integrations.evidently.visualizers/#module-integrationsevidentlyvisualizers","text":"This file was automatically generated via lazydocs .","title":"module integrations.evidently.visualizers"},{"location":"api_docs/integrations.facets/","text":"module integrations.facets The Facets integration provides a simple way to visualize post-execution objects like PipelineView , PipelineRunView and StepView . These objects can be extended using the BaseVisualization class. This integration requires facets-overview be installed in your Python environment. Global Variables FACETS class FacetsIntegration Definition of Facet integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.facets"},{"location":"api_docs/integrations.facets/#module-integrationsfacets","text":"The Facets integration provides a simple way to visualize post-execution objects like PipelineView , PipelineRunView and StepView . These objects can be extended using the BaseVisualization class. This integration requires facets-overview be installed in your Python environment.","title":"module integrations.facets"},{"location":"api_docs/integrations.facets/#global-variables","text":"FACETS","title":"Global Variables"},{"location":"api_docs/integrations.facets/#class-facetsintegration","text":"Definition of Facet integration for ZenML. This file was automatically generated via lazydocs .","title":"class FacetsIntegration"},{"location":"api_docs/integrations.facets.visualizers.facet_statistics_visualizer/","text":"module integrations.facets.visualizers.facet_statistics_visualizer class FacetStatisticsVisualizer The base implementation of a ZenML Visualizer. method generate_facet generate_facet(html_: str, magic: bool = False) \u2192 None Generate a Facet Overview Args: h : HTML represented as a string. magic : Whether to magically materialize facet in a notebook. method generate_html generate_html(datasets: List[Dict[str, DataFrame]]) \u2192 str Generates html for facet. Args: datasets : List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded. method visualize visualize( object: StepView, magic: bool = False, *args: Any, **kwargs: Any ) \u2192 None Method to visualize components Args: object : StepView fetched from run.get_step(). magic : Whether to render in a Jupyter notebook or not. This file was automatically generated via lazydocs .","title":"Integrations.facets.visualizers.facet statistics visualizer"},{"location":"api_docs/integrations.facets.visualizers.facet_statistics_visualizer/#module-integrationsfacetsvisualizersfacet_statistics_visualizer","text":"","title":"module integrations.facets.visualizers.facet_statistics_visualizer"},{"location":"api_docs/integrations.facets.visualizers.facet_statistics_visualizer/#class-facetstatisticsvisualizer","text":"The base implementation of a ZenML Visualizer.","title":"class FacetStatisticsVisualizer"},{"location":"api_docs/integrations.facets.visualizers.facet_statistics_visualizer/#method-generate_facet","text":"generate_facet(html_: str, magic: bool = False) \u2192 None Generate a Facet Overview Args: h : HTML represented as a string. magic : Whether to magically materialize facet in a notebook.","title":"method generate_facet"},{"location":"api_docs/integrations.facets.visualizers.facet_statistics_visualizer/#method-generate_html","text":"generate_html(datasets: List[Dict[str, DataFrame]]) \u2192 str Generates html for facet. Args: datasets : List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded.","title":"method generate_html"},{"location":"api_docs/integrations.facets.visualizers.facet_statistics_visualizer/#method-visualize","text":"visualize( object: StepView, magic: bool = False, *args: Any, **kwargs: Any ) \u2192 None Method to visualize components Args: object : StepView fetched from run.get_step(). magic : Whether to render in a Jupyter notebook or not. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"api_docs/integrations.facets.visualizers/","text":"module integrations.facets.visualizers This file was automatically generated via lazydocs .","title":"Integrations.facets.visualizers"},{"location":"api_docs/integrations.facets.visualizers/#module-integrationsfacetsvisualizers","text":"This file was automatically generated via lazydocs .","title":"module integrations.facets.visualizers"},{"location":"api_docs/integrations.gcp.artifact_stores.gcp_artifact_store/","text":"module integrations.gcp.artifact_stores.gcp_artifact_store class GCPArtifactStore Artifact Store for Google Cloud Storage based artifacts. classmethod must_be_gcs_path must_be_gcs_path(v: str) \u2192 str Validates that the path is a valid gcs path. This file was automatically generated via lazydocs .","title":"Integrations.gcp.artifact stores.gcp artifact store"},{"location":"api_docs/integrations.gcp.artifact_stores.gcp_artifact_store/#module-integrationsgcpartifact_storesgcp_artifact_store","text":"","title":"module integrations.gcp.artifact_stores.gcp_artifact_store"},{"location":"api_docs/integrations.gcp.artifact_stores.gcp_artifact_store/#class-gcpartifactstore","text":"Artifact Store for Google Cloud Storage based artifacts.","title":"class GCPArtifactStore"},{"location":"api_docs/integrations.gcp.artifact_stores.gcp_artifact_store/#classmethod-must_be_gcs_path","text":"must_be_gcs_path(v: str) \u2192 str Validates that the path is a valid gcs path. This file was automatically generated via lazydocs .","title":"classmethod must_be_gcs_path"},{"location":"api_docs/integrations.gcp.artifact_stores/","text":"module integrations.gcp.artifact_stores This file was automatically generated via lazydocs .","title":"Integrations.gcp.artifact stores"},{"location":"api_docs/integrations.gcp.artifact_stores/#module-integrationsgcpartifact_stores","text":"This file was automatically generated via lazydocs .","title":"module integrations.gcp.artifact_stores"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/","text":"module integrations.gcp.io.gcs_plugin Plugin which is created to add Google Cloud Store support to ZenML. It inherits from the base Filesystem created by TFX and overwrites the corresponding functions thanks to gcsfs. class ZenGCS Filesystem that delegates to Google Cloud Store using gcsfs. Note : To allow TFX to check for various error conditions, we need to raise their custom NotFoundError instead of the builtin python FileNotFoundError. method copy copy( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Copy a file. Args: src : The path to copy from. dst : The path to copy to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileNotFoundError : If the source file does not exist. FileExistsError : If a file already exists at the destination and overwrite is not set to True . method exists exists(path: Union[bytes, str]) \u2192 bool Check whether a path exists. method glob glob(pattern: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return all paths that match the given glob pattern. The glob pattern may include: - ' ' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - ' ' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/ */some_file) Args: pattern : The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern. method isdir isdir(path: Union[bytes, str]) \u2192 bool Check whether a path is a directory. method listdir listdir(path: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return a list of files in a directory. method makedirs makedirs(path: Union[bytes, str]) \u2192 None Create a directory at the given path. If needed also create missing parent directories. method mkdir mkdir(path: Union[bytes, str]) \u2192 None Create a directory at the given path. method open open(path: Union[bytes, str], mode: str = 'r') \u2192 Any Open a file at the given path. Args: path : Path of the file to open. mode : Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. method remove remove(path: Union[bytes, str]) \u2192 None Remove the file at the given path. method rename rename( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Rename source file to destination file. Args: src : The path of the file to rename. dst : The path to rename the source file to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileNotFoundError : If the source file does not exist. FileExistsError : If a file already exists at the destination and overwrite is not set to True . method rmtree rmtree(path: Union[bytes, str]) \u2192 None Remove the given directory. method stat stat(path: Union[bytes, str]) \u2192 Dict[str, Any] Return stat info for the given path. method walk walk( top: Union[bytes, str], topdown: bool = True, onerror: Optional[Callable[], NoneType] = None ) \u2192 Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] Return an iterator that walks the contents of the given directory. Args: top : Path of directory to walk. topdown : Unused argument to conform to interface. onerror : Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. This file was automatically generated via lazydocs .","title":"Integrations.gcp.io.gcs plugin"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#module-integrationsgcpiogcs_plugin","text":"Plugin which is created to add Google Cloud Store support to ZenML. It inherits from the base Filesystem created by TFX and overwrites the corresponding functions thanks to gcsfs.","title":"module integrations.gcp.io.gcs_plugin"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#class-zengcs","text":"Filesystem that delegates to Google Cloud Store using gcsfs. Note : To allow TFX to check for various error conditions, we need to raise their custom NotFoundError instead of the builtin python FileNotFoundError.","title":"class ZenGCS"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-copy","text":"copy( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Copy a file. Args: src : The path to copy from. dst : The path to copy to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileNotFoundError : If the source file does not exist. FileExistsError : If a file already exists at the destination and overwrite is not set to True .","title":"method copy"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-exists","text":"exists(path: Union[bytes, str]) \u2192 bool Check whether a path exists.","title":"method exists"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-glob","text":"glob(pattern: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return all paths that match the given glob pattern. The glob pattern may include: - ' ' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - ' ' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/ */some_file) Args: pattern : The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern.","title":"method glob"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-isdir","text":"isdir(path: Union[bytes, str]) \u2192 bool Check whether a path is a directory.","title":"method isdir"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-listdir","text":"listdir(path: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return a list of files in a directory.","title":"method listdir"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-makedirs","text":"makedirs(path: Union[bytes, str]) \u2192 None Create a directory at the given path. If needed also create missing parent directories.","title":"method makedirs"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-mkdir","text":"mkdir(path: Union[bytes, str]) \u2192 None Create a directory at the given path.","title":"method mkdir"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-open","text":"open(path: Union[bytes, str], mode: str = 'r') \u2192 Any Open a file at the given path. Args: path : Path of the file to open. mode : Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported.","title":"method open"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-remove","text":"remove(path: Union[bytes, str]) \u2192 None Remove the file at the given path.","title":"method remove"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-rename","text":"rename( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Rename source file to destination file. Args: src : The path of the file to rename. dst : The path to rename the source file to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileNotFoundError : If the source file does not exist. FileExistsError : If a file already exists at the destination and overwrite is not set to True .","title":"method rename"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-rmtree","text":"rmtree(path: Union[bytes, str]) \u2192 None Remove the given directory.","title":"method rmtree"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-stat","text":"stat(path: Union[bytes, str]) \u2192 Dict[str, Any] Return stat info for the given path.","title":"method stat"},{"location":"api_docs/integrations.gcp.io.gcs_plugin/#method-walk","text":"walk( top: Union[bytes, str], topdown: bool = True, onerror: Optional[Callable[], NoneType] = None ) \u2192 Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] Return an iterator that walks the contents of the given directory. Args: top : Path of directory to walk. topdown : Unused argument to conform to interface. onerror : Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. This file was automatically generated via lazydocs .","title":"method walk"},{"location":"api_docs/integrations.gcp.io/","text":"module integrations.gcp.io This file was automatically generated via lazydocs .","title":"Integrations.gcp.io"},{"location":"api_docs/integrations.gcp.io/#module-integrationsgcpio","text":"This file was automatically generated via lazydocs .","title":"module integrations.gcp.io"},{"location":"api_docs/integrations.gcp/","text":"module integrations.gcp The GCP integration submodule provides a way to run ZenML pipelines in a cloud environment. Specifically, it allows the use of cloud artifact stores, metadata stores, and an io module to handle file operations on Google Cloud Storage (GCS). Global Variables GCP class GcpIntegration Definition of Google Cloud Platform integration for ZenML. classmethod activate activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"Integrations.gcp"},{"location":"api_docs/integrations.gcp/#module-integrationsgcp","text":"The GCP integration submodule provides a way to run ZenML pipelines in a cloud environment. Specifically, it allows the use of cloud artifact stores, metadata stores, and an io module to handle file operations on Google Cloud Storage (GCS).","title":"module integrations.gcp"},{"location":"api_docs/integrations.gcp/#global-variables","text":"GCP","title":"Global Variables"},{"location":"api_docs/integrations.gcp/#class-gcpintegration","text":"Definition of Google Cloud Platform integration for ZenML.","title":"class GcpIntegration"},{"location":"api_docs/integrations.gcp/#classmethod-activate","text":"activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"api_docs/integrations.graphviz/","text":"module integrations.graphviz Global Variables GRAPHVIZ class GraphvizIntegration Definition of Graphviz integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.graphviz"},{"location":"api_docs/integrations.graphviz/#module-integrationsgraphviz","text":"","title":"module integrations.graphviz"},{"location":"api_docs/integrations.graphviz/#global-variables","text":"GRAPHVIZ","title":"Global Variables"},{"location":"api_docs/integrations.graphviz/#class-graphvizintegration","text":"Definition of Graphviz integration for ZenML. This file was automatically generated via lazydocs .","title":"class GraphvizIntegration"},{"location":"api_docs/integrations.graphviz.visualizers/","text":"module integrations.graphviz.visualizers This file was automatically generated via lazydocs .","title":"Integrations.graphviz.visualizers"},{"location":"api_docs/integrations.graphviz.visualizers/#module-integrationsgraphvizvisualizers","text":"This file was automatically generated via lazydocs .","title":"module integrations.graphviz.visualizers"},{"location":"api_docs/integrations.graphviz.visualizers.pipeline_run_dag_visualizer/","text":"module integrations.graphviz.visualizers.pipeline_run_dag_visualizer class PipelineRunDagVisualizer Visualize the lineage of runs in a pipeline. method visualize visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 Digraph Creates a pipeline lineage diagram using graphviz. This file was automatically generated via lazydocs .","title":"Integrations.graphviz.visualizers.pipeline run dag visualizer"},{"location":"api_docs/integrations.graphviz.visualizers.pipeline_run_dag_visualizer/#module-integrationsgraphvizvisualizerspipeline_run_dag_visualizer","text":"","title":"module integrations.graphviz.visualizers.pipeline_run_dag_visualizer"},{"location":"api_docs/integrations.graphviz.visualizers.pipeline_run_dag_visualizer/#class-pipelinerundagvisualizer","text":"Visualize the lineage of runs in a pipeline.","title":"class PipelineRunDagVisualizer"},{"location":"api_docs/integrations.graphviz.visualizers.pipeline_run_dag_visualizer/#method-visualize","text":"visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 Digraph Creates a pipeline lineage diagram using graphviz. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"api_docs/integrations.integration/","text":"module integrations.integration class IntegrationMeta Metaclass responsible for registering different Integration subclasses class Integration Base class for integration in ZenML method activate activate() \u2192 None Abstract method to activate the integration classmethod check_installation check_installation() \u2192 bool Method to check whether the required packages are installed This file was automatically generated via lazydocs .","title":"Integrations.integration"},{"location":"api_docs/integrations.integration/#module-integrationsintegration","text":"","title":"module integrations.integration"},{"location":"api_docs/integrations.integration/#class-integrationmeta","text":"Metaclass responsible for registering different Integration subclasses","title":"class IntegrationMeta"},{"location":"api_docs/integrations.integration/#class-integration","text":"Base class for integration in ZenML","title":"class Integration"},{"location":"api_docs/integrations.integration/#method-activate","text":"activate() \u2192 None Abstract method to activate the integration","title":"method activate"},{"location":"api_docs/integrations.integration/#classmethod-check_installation","text":"check_installation() \u2192 bool Method to check whether the required packages are installed This file was automatically generated via lazydocs .","title":"classmethod check_installation"},{"location":"api_docs/integrations.kubeflow.container_entrypoint/","text":"module integrations.kubeflow.container_entrypoint Main entrypoint for containers with Kubeflow TFX component executors. function main main() \u2192 None Runs a single step defined by the command line arguments. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.container entrypoint"},{"location":"api_docs/integrations.kubeflow.container_entrypoint/#module-integrationskubeflowcontainer_entrypoint","text":"Main entrypoint for containers with Kubeflow TFX component executors.","title":"module integrations.kubeflow.container_entrypoint"},{"location":"api_docs/integrations.kubeflow.container_entrypoint/#function-main","text":"main() \u2192 None Runs a single step defined by the command line arguments. This file was automatically generated via lazydocs .","title":"function main"},{"location":"api_docs/integrations.kubeflow.docker_utils/","text":"module integrations.kubeflow.docker_utils Global Variables DEFAULT_BASE_IMAGE function generate_dockerfile_contents generate_dockerfile_contents( base_image: str, command: Optional[str] = None, requirements: Optional[List[str]] = None ) \u2192 str Generates a Dockerfile. Args: base_image : The image to use as base for the dockerfile. command : The default command that gets executed when running a container of an image created by this dockerfile. requirements : Optional list of pip requirements to install. Returns: Content of a dockerfile. function create_custom_build_context create_custom_build_context( build_context_path: str, dockerfile_contents: str, dockerignore_path: Optional[str] = None ) \u2192 Any Creates a docker build context. Args: build_context_path : Path to a directory that will be sent to the docker daemon as build context. dockerfile_contents : File contents of the Dockerfile to use for the build. dockerignore_path : Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. Returns: Docker build context that can be passed when building a docker image. function get_current_environment_requirements get_current_environment_requirements() \u2192 Dict[str, str] Returns a dict of package requirements for the environment that the current python process is running in. function build_docker_image build_docker_image( build_context_path: str, image_name: str, dockerfile_path: Optional[str] = None, dockerignore_path: Optional[str] = None, requirements: Optional[List[str]] = None, use_local_requirements: bool = False, base_image: Optional[str] = None ) \u2192 None Builds a docker image. Args: build_context_path : Path to a directory that will be sent to the docker daemon as build context. image_name : The name to use for the created docker image. dockerfile_path : Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. dockerignore_path : Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. requirements : Optional list of pip requirements to install. This will only be used if no value is given for dockerfile_path . use_local_requirements : If True and no values are given for dockerfile_path and requirements , then the packages installed in the environment of the current python processed will be installed in the docker image. base_image : The image to use as base for the docker image. function push_docker_image push_docker_image(image_name: str) \u2192 None Pushes a docker image to a container registry. Args: image_name : The full name (including a tag) of the image to push. function get_image_digest get_image_digest(image_name: str) \u2192 Union[str, NoneType] Gets the digest of a docker image. Args: image_name : Name of the image to get the digest for. Returns: Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns None . This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.docker utils"},{"location":"api_docs/integrations.kubeflow.docker_utils/#module-integrationskubeflowdocker_utils","text":"","title":"module integrations.kubeflow.docker_utils"},{"location":"api_docs/integrations.kubeflow.docker_utils/#global-variables","text":"DEFAULT_BASE_IMAGE","title":"Global Variables"},{"location":"api_docs/integrations.kubeflow.docker_utils/#function-generate_dockerfile_contents","text":"generate_dockerfile_contents( base_image: str, command: Optional[str] = None, requirements: Optional[List[str]] = None ) \u2192 str Generates a Dockerfile. Args: base_image : The image to use as base for the dockerfile. command : The default command that gets executed when running a container of an image created by this dockerfile. requirements : Optional list of pip requirements to install. Returns: Content of a dockerfile.","title":"function generate_dockerfile_contents"},{"location":"api_docs/integrations.kubeflow.docker_utils/#function-create_custom_build_context","text":"create_custom_build_context( build_context_path: str, dockerfile_contents: str, dockerignore_path: Optional[str] = None ) \u2192 Any Creates a docker build context. Args: build_context_path : Path to a directory that will be sent to the docker daemon as build context. dockerfile_contents : File contents of the Dockerfile to use for the build. dockerignore_path : Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. Returns: Docker build context that can be passed when building a docker image.","title":"function create_custom_build_context"},{"location":"api_docs/integrations.kubeflow.docker_utils/#function-get_current_environment_requirements","text":"get_current_environment_requirements() \u2192 Dict[str, str] Returns a dict of package requirements for the environment that the current python process is running in.","title":"function get_current_environment_requirements"},{"location":"api_docs/integrations.kubeflow.docker_utils/#function-build_docker_image","text":"build_docker_image( build_context_path: str, image_name: str, dockerfile_path: Optional[str] = None, dockerignore_path: Optional[str] = None, requirements: Optional[List[str]] = None, use_local_requirements: bool = False, base_image: Optional[str] = None ) \u2192 None Builds a docker image. Args: build_context_path : Path to a directory that will be sent to the docker daemon as build context. image_name : The name to use for the created docker image. dockerfile_path : Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. dockerignore_path : Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. requirements : Optional list of pip requirements to install. This will only be used if no value is given for dockerfile_path . use_local_requirements : If True and no values are given for dockerfile_path and requirements , then the packages installed in the environment of the current python processed will be installed in the docker image. base_image : The image to use as base for the docker image.","title":"function build_docker_image"},{"location":"api_docs/integrations.kubeflow.docker_utils/#function-push_docker_image","text":"push_docker_image(image_name: str) \u2192 None Pushes a docker image to a container registry. Args: image_name : The full name (including a tag) of the image to push.","title":"function push_docker_image"},{"location":"api_docs/integrations.kubeflow.docker_utils/#function-get_image_digest","text":"get_image_digest(image_name: str) \u2192 Union[str, NoneType] Gets the digest of a docker image. Args: image_name : Name of the image to get the digest for. Returns: Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns None . This file was automatically generated via lazydocs .","title":"function get_image_digest"},{"location":"api_docs/integrations.kubeflow/","text":"module integrations.kubeflow The Kubeflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Kubeflow orchestrator with the CLI tool. Global Variables KUBEFLOW class KubeflowIntegration Definition of Kubeflow Integration for ZenML. classmethod activate activate() \u2192 None Activates all classes required for the airflow integration. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow"},{"location":"api_docs/integrations.kubeflow/#module-integrationskubeflow","text":"The Kubeflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Kubeflow orchestrator with the CLI tool.","title":"module integrations.kubeflow"},{"location":"api_docs/integrations.kubeflow/#global-variables","text":"KUBEFLOW","title":"Global Variables"},{"location":"api_docs/integrations.kubeflow/#class-kubeflowintegration","text":"Definition of Kubeflow Integration for ZenML.","title":"class KubeflowIntegration"},{"location":"api_docs/integrations.kubeflow/#classmethod-activate","text":"activate() \u2192 None Activates all classes required for the airflow integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"api_docs/integrations.kubeflow.metadata.kubeflow_metadata_store/","text":"module integrations.kubeflow.metadata.kubeflow_metadata_store class KubeflowMetadataStore Kubeflow MySQL backend for ZenML metadata store. property step_type_mapping Maps type_id's to step names. property store General property that hooks into TFX metadata store. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.metadata.kubeflow metadata store"},{"location":"api_docs/integrations.kubeflow.metadata.kubeflow_metadata_store/#module-integrationskubeflowmetadatakubeflow_metadata_store","text":"","title":"module integrations.kubeflow.metadata.kubeflow_metadata_store"},{"location":"api_docs/integrations.kubeflow.metadata.kubeflow_metadata_store/#class-kubeflowmetadatastore","text":"Kubeflow MySQL backend for ZenML metadata store.","title":"class KubeflowMetadataStore"},{"location":"api_docs/integrations.kubeflow.metadata.kubeflow_metadata_store/#property-step_type_mapping","text":"Maps type_id's to step names.","title":"property step_type_mapping"},{"location":"api_docs/integrations.kubeflow.metadata.kubeflow_metadata_store/#property-store","text":"General property that hooks into TFX metadata store. This file was automatically generated via lazydocs .","title":"property store"},{"location":"api_docs/integrations.kubeflow.metadata/","text":"module integrations.kubeflow.metadata This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.metadata"},{"location":"api_docs/integrations.kubeflow.metadata/#module-integrationskubeflowmetadata","text":"This file was automatically generated via lazydocs .","title":"module integrations.kubeflow.metadata"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_component/","text":"module integrations.kubeflow.orchestrators.kubeflow_component Kubeflow Pipelines based implementation of TFX components. These components are lightweight wrappers around the KFP DSL's ContainerOp, and ensure that the container gets called with the right set of input arguments. It also ensures that each component exports named output attributes that are consistent with those provided by the native TFX components, thus ensuring that both types of pipeline definitions are compatible. Note: This requires Kubeflow Pipelines SDK to be installed. Global Variables ENV_ZENML_PREVENT_PIPELINE_EXECUTION CONTAINER_ENTRYPOINT_COMMAND class KubeflowComponent Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components. method __init__ __init__( component: BaseComponent, depends_on: Set[ContainerOp], image: str, tfx_ir: Pipeline, pod_labels_to_attach: Dict[str, str], main_module: str, step_module: str, step_function_name: str, runtime_parameters: List[RuntimeParameter], metadata_ui_path: str = '/tmp/mlpipeline-ui-metadata.json' ) Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component : The logical TFX component to wrap. depends_on : The set of upstream KFP ContainerOp components that this component will depend on. image : The container image to use for this component. tfx_ir : The TFX intermedia representation of the pipeline. pod_labels_to_attach : Dict of pod labels to attach to the GKE pod. runtime_parameters : Runtime parameters of the pipeline. metadata_ui_path : File location for metadata-ui-metadata.json file. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators.kubeflow component"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_component/#module-integrationskubefloworchestratorskubeflow_component","text":"Kubeflow Pipelines based implementation of TFX components. These components are lightweight wrappers around the KFP DSL's ContainerOp, and ensure that the container gets called with the right set of input arguments. It also ensures that each component exports named output attributes that are consistent with those provided by the native TFX components, thus ensuring that both types of pipeline definitions are compatible. Note: This requires Kubeflow Pipelines SDK to be installed.","title":"module integrations.kubeflow.orchestrators.kubeflow_component"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_component/#global-variables","text":"ENV_ZENML_PREVENT_PIPELINE_EXECUTION CONTAINER_ENTRYPOINT_COMMAND","title":"Global Variables"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_component/#class-kubeflowcomponent","text":"Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components.","title":"class KubeflowComponent"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_component/#method-__init__","text":"__init__( component: BaseComponent, depends_on: Set[ContainerOp], image: str, tfx_ir: Pipeline, pod_labels_to_attach: Dict[str, str], main_module: str, step_module: str, step_function_name: str, runtime_parameters: List[RuntimeParameter], metadata_ui_path: str = '/tmp/mlpipeline-ui-metadata.json' ) Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component : The logical TFX component to wrap. depends_on : The set of upstream KFP ContainerOp components that this component will depend on. image : The container image to use for this component. tfx_ir : The TFX intermedia representation of the pipeline. pod_labels_to_attach : Dict of pod labels to attach to the GKE pod. runtime_parameters : Runtime parameters of the pipeline. metadata_ui_path : File location for metadata-ui-metadata.json file. This file was automatically generated via lazydocs .","title":"method __init__"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_dag_runner/","text":"module integrations.kubeflow.orchestrators.kubeflow_dag_runner The below code is copied from the TFX source repo with minor changes. All credits goes to the TFX team for the core implementation function get_default_pipeline_operator_funcs get_default_pipeline_operator_funcs( use_gcp_sa: bool = False ) \u2192 List[Callable[[ContainerOp], Union[ContainerOp, NoneType]]] Returns a default list of pipeline operator functions. Args: use_gcp_sa : If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. Returns: A list of functions with type OpFunc. function get_default_pod_labels get_default_pod_labels() \u2192 Dict[str, str] Returns the default pod label dict for Kubeflow. class KubeflowDagRunnerConfig Runtime configuration parameters specific to execution on Kubeflow. method __init__ __init__( image: str, pipeline_operator_funcs: Optional[List[Callable[[ContainerOp], Union[ContainerOp]]], NoneType] = None, supported_launcher_classes: Optional[List[Type[BaseComponentLauncher]]] = None, metadata_ui_path: str = '/tmp/mlpipeline-ui-metadata.json', **kwargs: Any ) Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image : The docker image to use in the pipeline. pipeline_operator_funcs : A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes : A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path : File location for metadata-ui-metadata.json file. **kwargs : keyword args for PipelineConfig. class KubeflowDagRunner Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline. method __init__ __init__( config: KubeflowDagRunnerConfig, output_path: str, pod_labels_to_attach: Optional[Dict[str, str]] = None ) Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config : A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path : Path where the pipeline definition file will be stored. pod_labels_to_attach : Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env : true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. property config method run run(pipeline: Pipeline) \u2192 None Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline : The logical TFX pipeline to use when building the Kubeflow pipeline. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators.kubeflow dag runner"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_dag_runner/#module-integrationskubefloworchestratorskubeflow_dag_runner","text":"The below code is copied from the TFX source repo with minor changes. All credits goes to the TFX team for the core implementation","title":"module integrations.kubeflow.orchestrators.kubeflow_dag_runner"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_dag_runner/#function-get_default_pipeline_operator_funcs","text":"get_default_pipeline_operator_funcs( use_gcp_sa: bool = False ) \u2192 List[Callable[[ContainerOp], Union[ContainerOp, NoneType]]] Returns a default list of pipeline operator functions. Args: use_gcp_sa : If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. Returns: A list of functions with type OpFunc.","title":"function get_default_pipeline_operator_funcs"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_dag_runner/#function-get_default_pod_labels","text":"get_default_pod_labels() \u2192 Dict[str, str] Returns the default pod label dict for Kubeflow.","title":"function get_default_pod_labels"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_dag_runner/#class-kubeflowdagrunnerconfig","text":"Runtime configuration parameters specific to execution on Kubeflow.","title":"class KubeflowDagRunnerConfig"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_dag_runner/#method-__init__","text":"__init__( image: str, pipeline_operator_funcs: Optional[List[Callable[[ContainerOp], Union[ContainerOp]]], NoneType] = None, supported_launcher_classes: Optional[List[Type[BaseComponentLauncher]]] = None, metadata_ui_path: str = '/tmp/mlpipeline-ui-metadata.json', **kwargs: Any ) Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image : The docker image to use in the pipeline. pipeline_operator_funcs : A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes : A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path : File location for metadata-ui-metadata.json file. **kwargs : keyword args for PipelineConfig.","title":"method __init__"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_dag_runner/#class-kubeflowdagrunner","text":"Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline.","title":"class KubeflowDagRunner"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_dag_runner/#method-__init___1","text":"__init__( config: KubeflowDagRunnerConfig, output_path: str, pod_labels_to_attach: Optional[Dict[str, str]] = None ) Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config : A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path : Path where the pipeline definition file will be stored. pod_labels_to_attach : Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env : true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking.","title":"method __init__"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_dag_runner/#property-config","text":"","title":"property config"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_dag_runner/#method-run","text":"run(pipeline: Pipeline) \u2192 None Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline : The logical TFX pipeline to use when building the Kubeflow pipeline. This file was automatically generated via lazydocs .","title":"method run"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/","text":"module integrations.kubeflow.orchestrators.kubeflow_orchestrator Global Variables TYPE_CHECKING KFP_VERSION DEFAULT_KFP_UI_PORT class KubeflowOrchestrator Orchestrator responsible for running pipelines using Kubeflow. property is_running Returns whether the orchestrator is running. property log_file Path of the daemon log file. property pipeline_directory Returns path to a directory in which the kubeflow pipeline files are stored. property root_directory Returns path to the root directory for all files concerning this orchestrator. method down down() \u2192 None Tears down a local Kubeflow Pipelines deployment. method get_docker_image_name get_docker_image_name(pipeline_name: str) \u2192 str Returns the full docker image name including registry and tag. method list_manual_setup_steps list_manual_setup_steps( container_registry_name: str, container_registry_path: str ) \u2192 None Logs manual steps needed to setup the Kubeflow local orchestrator. method pre_run pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Builds a docker image for the current environment and uploads it to a container registry if configured. method run run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 None Runs the pipeline on Kubeflow. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Unused kwargs to conform with base signature method up up() \u2192 None Spins up a local Kubeflow Pipelines deployment. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators.kubeflow orchestrator"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#module-integrationskubefloworchestratorskubeflow_orchestrator","text":"","title":"module integrations.kubeflow.orchestrators.kubeflow_orchestrator"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#global-variables","text":"TYPE_CHECKING KFP_VERSION DEFAULT_KFP_UI_PORT","title":"Global Variables"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#class-kubefloworchestrator","text":"Orchestrator responsible for running pipelines using Kubeflow.","title":"class KubeflowOrchestrator"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#property-is_running","text":"Returns whether the orchestrator is running.","title":"property is_running"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#property-log_file","text":"Path of the daemon log file.","title":"property log_file"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#property-pipeline_directory","text":"Returns path to a directory in which the kubeflow pipeline files are stored.","title":"property pipeline_directory"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#property-root_directory","text":"Returns path to the root directory for all files concerning this orchestrator.","title":"property root_directory"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-down","text":"down() \u2192 None Tears down a local Kubeflow Pipelines deployment.","title":"method down"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-get_docker_image_name","text":"get_docker_image_name(pipeline_name: str) \u2192 str Returns the full docker image name including registry and tag.","title":"method get_docker_image_name"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-list_manual_setup_steps","text":"list_manual_setup_steps( container_registry_name: str, container_registry_path: str ) \u2192 None Logs manual steps needed to setup the Kubeflow local orchestrator.","title":"method list_manual_setup_steps"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-pre_run","text":"pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Builds a docker image for the current environment and uploads it to a container registry if configured.","title":"method pre_run"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-run","text":"run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 None Runs the pipeline on Kubeflow. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Unused kwargs to conform with base signature","title":"method run"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_orchestrator/#method-up","text":"up() \u2192 None Spins up a local Kubeflow Pipelines deployment. This file was automatically generated via lazydocs .","title":"method up"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_utils/","text":"module integrations.kubeflow.orchestrators.kubeflow_utils Common utility for Kubeflow-based orchestrator. function replace_placeholder replace_placeholder(component: BaseNode) \u2192 None Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators.kubeflow utils"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_utils/#module-integrationskubefloworchestratorskubeflow_utils","text":"Common utility for Kubeflow-based orchestrator.","title":"module integrations.kubeflow.orchestrators.kubeflow_utils"},{"location":"api_docs/integrations.kubeflow.orchestrators.kubeflow_utils/#function-replace_placeholder","text":"replace_placeholder(component: BaseNode) \u2192 None Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. This file was automatically generated via lazydocs .","title":"function replace_placeholder"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/","text":"module integrations.kubeflow.orchestrators.local_deployment_utils Global Variables KFP_VERSION function check_prerequisites check_prerequisites() \u2192 bool Checks whether all prerequisites for a local kubeflow pipelines deployment are installed. function write_local_registry_yaml write_local_registry_yaml( yaml_path: str, registry_name: str, registry_uri: str ) \u2192 None Writes a K3D registry config file. Args: yaml_path : Path where the config file should be written to. registry_name : Name of the registry. registry_uri : URI of the registry. function k3d_cluster_exists k3d_cluster_exists(cluster_name: str) \u2192 bool Checks whether there exists a K3D cluster with the given name. function create_k3d_cluster create_k3d_cluster( cluster_name: str, registry_name: str, registry_config_path: str ) \u2192 None Creates a K3D cluster. Args: cluster_name : Name of the cluster to create. registry_name : Name of the registry to create for this cluster. registry_config_path : Path to the registry config file. function delete_k3d_cluster delete_k3d_cluster(cluster_name: str) \u2192 None Deletes a K3D cluster with the given name. function kubeflow_pipelines_ready kubeflow_pipelines_ready(kubernetes_context: str) \u2192 bool Returns whether all Kubeflow Pipelines pods are ready. Args: kubernetes_context : The kubernetes context in which the pods should be checked. function deploy_kubeflow_pipelines deploy_kubeflow_pipelines(kubernetes_context: str) \u2192 None Deploys Kubeflow Pipelines. Args: kubernetes_context : The kubernetes context on which Kubeflow Pipelines should be deployed. function start_kfp_ui_daemon start_kfp_ui_daemon(pid_file_path: str, log_file_path: str, port: int) \u2192 None Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Args: pid_file_path : Path where the file with the daemons process ID should be written. log_file_path : Path to a file where the daemon logs should be written. port : Port on which the UI should be accessible. This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators.local deployment utils"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/#module-integrationskubefloworchestratorslocal_deployment_utils","text":"","title":"module integrations.kubeflow.orchestrators.local_deployment_utils"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/#global-variables","text":"KFP_VERSION","title":"Global Variables"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/#function-check_prerequisites","text":"check_prerequisites() \u2192 bool Checks whether all prerequisites for a local kubeflow pipelines deployment are installed.","title":"function check_prerequisites"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/#function-write_local_registry_yaml","text":"write_local_registry_yaml( yaml_path: str, registry_name: str, registry_uri: str ) \u2192 None Writes a K3D registry config file. Args: yaml_path : Path where the config file should be written to. registry_name : Name of the registry. registry_uri : URI of the registry.","title":"function write_local_registry_yaml"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/#function-k3d_cluster_exists","text":"k3d_cluster_exists(cluster_name: str) \u2192 bool Checks whether there exists a K3D cluster with the given name.","title":"function k3d_cluster_exists"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/#function-create_k3d_cluster","text":"create_k3d_cluster( cluster_name: str, registry_name: str, registry_config_path: str ) \u2192 None Creates a K3D cluster. Args: cluster_name : Name of the cluster to create. registry_name : Name of the registry to create for this cluster. registry_config_path : Path to the registry config file.","title":"function create_k3d_cluster"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/#function-delete_k3d_cluster","text":"delete_k3d_cluster(cluster_name: str) \u2192 None Deletes a K3D cluster with the given name.","title":"function delete_k3d_cluster"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/#function-kubeflow_pipelines_ready","text":"kubeflow_pipelines_ready(kubernetes_context: str) \u2192 bool Returns whether all Kubeflow Pipelines pods are ready. Args: kubernetes_context : The kubernetes context in which the pods should be checked.","title":"function kubeflow_pipelines_ready"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/#function-deploy_kubeflow_pipelines","text":"deploy_kubeflow_pipelines(kubernetes_context: str) \u2192 None Deploys Kubeflow Pipelines. Args: kubernetes_context : The kubernetes context on which Kubeflow Pipelines should be deployed.","title":"function deploy_kubeflow_pipelines"},{"location":"api_docs/integrations.kubeflow.orchestrators.local_deployment_utils/#function-start_kfp_ui_daemon","text":"start_kfp_ui_daemon(pid_file_path: str, log_file_path: str, port: int) \u2192 None Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Args: pid_file_path : Path where the file with the daemons process ID should be written. log_file_path : Path to a file where the daemon logs should be written. port : Port on which the UI should be accessible. This file was automatically generated via lazydocs .","title":"function start_kfp_ui_daemon"},{"location":"api_docs/integrations.kubeflow.orchestrators/","text":"module integrations.kubeflow.orchestrators This file was automatically generated via lazydocs .","title":"Integrations.kubeflow.orchestrators"},{"location":"api_docs/integrations.kubeflow.orchestrators/#module-integrationskubefloworchestrators","text":"This file was automatically generated via lazydocs .","title":"module integrations.kubeflow.orchestrators"},{"location":"api_docs/integrations/","text":"module integrations The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch. This file was automatically generated via lazydocs .","title":"Integrations"},{"location":"api_docs/integrations/#module-integrations","text":"The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch. This file was automatically generated via lazydocs .","title":"module integrations"},{"location":"api_docs/integrations.mlflow/","text":"module integrations.mlflow The mlflow integrations currently enables you to use mlflow tracking as a convenient way to visualize your experiment runs within the mlflow ui Global Variables MLFLOW class MlflowIntegration Definition of Plotly integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.mlflow"},{"location":"api_docs/integrations.mlflow/#module-integrationsmlflow","text":"The mlflow integrations currently enables you to use mlflow tracking as a convenient way to visualize your experiment runs within the mlflow ui","title":"module integrations.mlflow"},{"location":"api_docs/integrations.mlflow/#global-variables","text":"MLFLOW","title":"Global Variables"},{"location":"api_docs/integrations.mlflow/#class-mlflowintegration","text":"Definition of Plotly integration for ZenML. This file was automatically generated via lazydocs .","title":"class MlflowIntegration"},{"location":"api_docs/integrations.mlflow.mlflow_utils/","text":"module integrations.mlflow.mlflow_utils function local_mlflow_backend local_mlflow_backend() \u2192 str Returns the local mlflow backend inside the global zenml directory function setup_mlflow setup_mlflow( backend_store_uri: Optional[str] = None, experiment_name: str = 'default' ) \u2192 None Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Args: backend_store_uri : The mlflow backend to log to experiment_name : The experiment name under which all runs will be tracked function enable_mlflow_init enable_mlflow_init( original_init: Callable[[BasePipeline, BaseStep, Any], NoneType], experiment: Optional[str] = None ) \u2192 Callable[, NoneType] Outer decorator function for extending the init method for pipelines that should be run using mlflow Args: original_init : The init method that should be extended experiment : The users chosen experiment name to use for mlflow Returns: the inner decorator which extends the init method function enable_mlflow_run enable_mlflow_run(run: Callable[, Any]) \u2192 Callable[, Any] Outer decorator function for extending the run method for pipelines that should be run using mlflow Args: run : The run method that should be extended Returns: the inner decorator which extends the run method function enable_mlflow enable_mlflow( _pipeline: Type[BasePipeline], experiment_name: Optional[str] = None ) \u2192 Type[BasePipeline] Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a pipeline run. For this, the init and run method need to be extended accordingly. Args: _pipeline : The decorated pipeline experiment_name : Experiment name to use for mlflow Returns: the inner decorator which has a pipeline with the two methods extended This file was automatically generated via lazydocs .","title":"Integrations.mlflow.mlflow utils"},{"location":"api_docs/integrations.mlflow.mlflow_utils/#module-integrationsmlflowmlflow_utils","text":"","title":"module integrations.mlflow.mlflow_utils"},{"location":"api_docs/integrations.mlflow.mlflow_utils/#function-local_mlflow_backend","text":"local_mlflow_backend() \u2192 str Returns the local mlflow backend inside the global zenml directory","title":"function local_mlflow_backend"},{"location":"api_docs/integrations.mlflow.mlflow_utils/#function-setup_mlflow","text":"setup_mlflow( backend_store_uri: Optional[str] = None, experiment_name: str = 'default' ) \u2192 None Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Args: backend_store_uri : The mlflow backend to log to experiment_name : The experiment name under which all runs will be tracked","title":"function setup_mlflow"},{"location":"api_docs/integrations.mlflow.mlflow_utils/#function-enable_mlflow_init","text":"enable_mlflow_init( original_init: Callable[[BasePipeline, BaseStep, Any], NoneType], experiment: Optional[str] = None ) \u2192 Callable[, NoneType] Outer decorator function for extending the init method for pipelines that should be run using mlflow Args: original_init : The init method that should be extended experiment : The users chosen experiment name to use for mlflow Returns: the inner decorator which extends the init method","title":"function enable_mlflow_init"},{"location":"api_docs/integrations.mlflow.mlflow_utils/#function-enable_mlflow_run","text":"enable_mlflow_run(run: Callable[, Any]) \u2192 Callable[, Any] Outer decorator function for extending the run method for pipelines that should be run using mlflow Args: run : The run method that should be extended Returns: the inner decorator which extends the run method","title":"function enable_mlflow_run"},{"location":"api_docs/integrations.mlflow.mlflow_utils/#function-enable_mlflow","text":"enable_mlflow( _pipeline: Type[BasePipeline], experiment_name: Optional[str] = None ) \u2192 Type[BasePipeline] Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a pipeline run. For this, the init and run method need to be extended accordingly. Args: _pipeline : The decorated pipeline experiment_name : Experiment name to use for mlflow Returns: the inner decorator which has a pipeline with the two methods extended This file was automatically generated via lazydocs .","title":"function enable_mlflow"},{"location":"api_docs/integrations.plotly/","text":"module integrations.plotly Global Variables PLOTLY class PlotlyIntegration Definition of Plotly integration for ZenML. This file was automatically generated via lazydocs .","title":"Integrations.plotly"},{"location":"api_docs/integrations.plotly/#module-integrationsplotly","text":"","title":"module integrations.plotly"},{"location":"api_docs/integrations.plotly/#global-variables","text":"PLOTLY","title":"Global Variables"},{"location":"api_docs/integrations.plotly/#class-plotlyintegration","text":"Definition of Plotly integration for ZenML. This file was automatically generated via lazydocs .","title":"class PlotlyIntegration"},{"location":"api_docs/integrations.plotly.visualizers/","text":"module integrations.plotly.visualizers This file was automatically generated via lazydocs .","title":"Integrations.plotly.visualizers"},{"location":"api_docs/integrations.plotly.visualizers/#module-integrationsplotlyvisualizers","text":"This file was automatically generated via lazydocs .","title":"module integrations.plotly.visualizers"},{"location":"api_docs/integrations.plotly.visualizers.pipeline_lineage_visualizer/","text":"module integrations.plotly.visualizers.pipeline_lineage_visualizer class PipelineLineageVisualizer Visualize the lineage of runs in a pipeline using plotly. method visualize visualize(object: PipelineView, *args: Any, **kwargs: Any) \u2192 Figure Creates a pipeline lineage diagram using plotly. This file was automatically generated via lazydocs .","title":"Integrations.plotly.visualizers.pipeline lineage visualizer"},{"location":"api_docs/integrations.plotly.visualizers.pipeline_lineage_visualizer/#module-integrationsplotlyvisualizerspipeline_lineage_visualizer","text":"","title":"module integrations.plotly.visualizers.pipeline_lineage_visualizer"},{"location":"api_docs/integrations.plotly.visualizers.pipeline_lineage_visualizer/#class-pipelinelineagevisualizer","text":"Visualize the lineage of runs in a pipeline using plotly.","title":"class PipelineLineageVisualizer"},{"location":"api_docs/integrations.plotly.visualizers.pipeline_lineage_visualizer/#method-visualize","text":"visualize(object: PipelineView, *args: Any, **kwargs: Any) \u2192 Figure Creates a pipeline lineage diagram using plotly. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"api_docs/integrations.pytorch.materializers/","text":"module integrations.pytorch.materializers This file was automatically generated via lazydocs .","title":"Integrations.pytorch.materializers"},{"location":"api_docs/integrations.pytorch.materializers/#module-integrationspytorchmaterializers","text":"This file was automatically generated via lazydocs .","title":"module integrations.pytorch.materializers"},{"location":"api_docs/integrations.pytorch.materializers.pytorch_materializer/","text":"module integrations.pytorch.materializers.pytorch_materializer Global Variables DEFAULT_FILENAME class PyTorchMaterializer Materializer to read/write Pytorch models. method handle_input handle_input(data_type: Type[Any]) \u2192 Union[Module, TorchDict] Reads and returns a PyTorch model. Returns: A loaded pytorch model. method handle_return handle_return(model: Union[Module, TorchDict]) \u2192 None Writes a PyTorch model. Args: model : A torch.nn.Module or a dict to pass into model.save This file was automatically generated via lazydocs .","title":"Integrations.pytorch.materializers.pytorch materializer"},{"location":"api_docs/integrations.pytorch.materializers.pytorch_materializer/#module-integrationspytorchmaterializerspytorch_materializer","text":"","title":"module integrations.pytorch.materializers.pytorch_materializer"},{"location":"api_docs/integrations.pytorch.materializers.pytorch_materializer/#global-variables","text":"DEFAULT_FILENAME","title":"Global Variables"},{"location":"api_docs/integrations.pytorch.materializers.pytorch_materializer/#class-pytorchmaterializer","text":"Materializer to read/write Pytorch models.","title":"class PyTorchMaterializer"},{"location":"api_docs/integrations.pytorch.materializers.pytorch_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Union[Module, TorchDict] Reads and returns a PyTorch model. Returns: A loaded pytorch model.","title":"method handle_input"},{"location":"api_docs/integrations.pytorch.materializers.pytorch_materializer/#method-handle_return","text":"handle_return(model: Union[Module, TorchDict]) \u2192 None Writes a PyTorch model. Args: model : A torch.nn.Module or a dict to pass into model.save This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"api_docs/integrations.pytorch.materializers.pytorch_types/","text":"module integrations.pytorch.materializers.pytorch_types class TorchDict A type of dict that represents saving a model. This file was automatically generated via lazydocs .","title":"Integrations.pytorch.materializers.pytorch types"},{"location":"api_docs/integrations.pytorch.materializers.pytorch_types/#module-integrationspytorchmaterializerspytorch_types","text":"","title":"module integrations.pytorch.materializers.pytorch_types"},{"location":"api_docs/integrations.pytorch.materializers.pytorch_types/#class-torchdict","text":"A type of dict that represents saving a model. This file was automatically generated via lazydocs .","title":"class TorchDict"},{"location":"api_docs/integrations.pytorch/","text":"module integrations.pytorch Global Variables PYTORCH class PytorchIntegration Definition of PyTorch integration for ZenML. classmethod activate activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"Integrations.pytorch"},{"location":"api_docs/integrations.pytorch/#module-integrationspytorch","text":"","title":"module integrations.pytorch"},{"location":"api_docs/integrations.pytorch/#global-variables","text":"PYTORCH","title":"Global Variables"},{"location":"api_docs/integrations.pytorch/#class-pytorchintegration","text":"Definition of PyTorch integration for ZenML.","title":"class PytorchIntegration"},{"location":"api_docs/integrations.pytorch/#classmethod-activate","text":"activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"api_docs/integrations.pytorch_lightning.materializers/","text":"module integrations.pytorch_lightning.materializers This file was automatically generated via lazydocs .","title":"Integrations.pytorch lightning.materializers"},{"location":"api_docs/integrations.pytorch_lightning.materializers/#module-integrationspytorch_lightningmaterializers","text":"This file was automatically generated via lazydocs .","title":"module integrations.pytorch_lightning.materializers"},{"location":"api_docs/integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/","text":"module integrations.pytorch_lightning.materializers.pytorch_lightning_materializer Global Variables CHECKPOINT_NAME class PyTorchLightningMaterializer Materializer to read/write Pytorch models. method handle_input handle_input(data_type: Type[Any]) \u2192 Trainer Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object. method handle_return handle_return(trainer: Trainer) \u2192 None Writes a PyTorch Lightning trainer. Args: trainer : A PyTorch Lightning trainer object. This file was automatically generated via lazydocs .","title":"Integrations.pytorch lightning.materializers.pytorch lightning materializer"},{"location":"api_docs/integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/#module-integrationspytorch_lightningmaterializerspytorch_lightning_materializer","text":"","title":"module integrations.pytorch_lightning.materializers.pytorch_lightning_materializer"},{"location":"api_docs/integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/#global-variables","text":"CHECKPOINT_NAME","title":"Global Variables"},{"location":"api_docs/integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/#class-pytorchlightningmaterializer","text":"Materializer to read/write Pytorch models.","title":"class PyTorchLightningMaterializer"},{"location":"api_docs/integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Trainer Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object.","title":"method handle_input"},{"location":"api_docs/integrations.pytorch_lightning.materializers.pytorch_lightning_materializer/#method-handle_return","text":"handle_return(trainer: Trainer) \u2192 None Writes a PyTorch Lightning trainer. Args: trainer : A PyTorch Lightning trainer object. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"api_docs/integrations.pytorch_lightning/","text":"module integrations.pytorch_lightning Global Variables PYTORCH_L class PytorchLightningIntegration Definition of PyTorch Lightning integration for ZenML. classmethod activate activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"Integrations.pytorch lightning"},{"location":"api_docs/integrations.pytorch_lightning/#module-integrationspytorch_lightning","text":"","title":"module integrations.pytorch_lightning"},{"location":"api_docs/integrations.pytorch_lightning/#global-variables","text":"PYTORCH_L","title":"Global Variables"},{"location":"api_docs/integrations.pytorch_lightning/#class-pytorchlightningintegration","text":"Definition of PyTorch Lightning integration for ZenML.","title":"class PytorchLightningIntegration"},{"location":"api_docs/integrations.pytorch_lightning/#classmethod-activate","text":"activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"api_docs/integrations.registry/","text":"module integrations.registry Global Variables TYPE_CHECKING integration_registry class IntegrationRegistry Registry to keep track of ZenML Integrations method __init__ __init__() \u2192 None Initializing the integration registry property integrations Method to get integrations dictionary. Returns: A dict of integration key to type of Integration . property list_integration_names Get a list of all possible integrations method activate_integrations activate_integrations() \u2192 None Method to activate the integrations with are registered in the registry method is_installed is_installed(integration_name: Optional[str] = None) \u2192 bool Checks if all requirements for an integration are installed method register_integration register_integration(key: str, type_: Type[ForwardRef('Integration')]) \u2192 None Method to register an integration with a given name method select_integration_requirements select_integration_requirements( integration_name: Optional[str] = None ) \u2192 List[str] Select the requirements for a given integration or all integrations This file was automatically generated via lazydocs .","title":"Integrations.registry"},{"location":"api_docs/integrations.registry/#module-integrationsregistry","text":"","title":"module integrations.registry"},{"location":"api_docs/integrations.registry/#global-variables","text":"TYPE_CHECKING integration_registry","title":"Global Variables"},{"location":"api_docs/integrations.registry/#class-integrationregistry","text":"Registry to keep track of ZenML Integrations","title":"class IntegrationRegistry"},{"location":"api_docs/integrations.registry/#method-__init__","text":"__init__() \u2192 None Initializing the integration registry","title":"method __init__"},{"location":"api_docs/integrations.registry/#property-integrations","text":"Method to get integrations dictionary. Returns: A dict of integration key to type of Integration .","title":"property integrations"},{"location":"api_docs/integrations.registry/#property-list_integration_names","text":"Get a list of all possible integrations","title":"property list_integration_names"},{"location":"api_docs/integrations.registry/#method-activate_integrations","text":"activate_integrations() \u2192 None Method to activate the integrations with are registered in the registry","title":"method activate_integrations"},{"location":"api_docs/integrations.registry/#method-is_installed","text":"is_installed(integration_name: Optional[str] = None) \u2192 bool Checks if all requirements for an integration are installed","title":"method is_installed"},{"location":"api_docs/integrations.registry/#method-register_integration","text":"register_integration(key: str, type_: Type[ForwardRef('Integration')]) \u2192 None Method to register an integration with a given name","title":"method register_integration"},{"location":"api_docs/integrations.registry/#method-select_integration_requirements","text":"select_integration_requirements( integration_name: Optional[str] = None ) \u2192 List[str] Select the requirements for a given integration or all integrations This file was automatically generated via lazydocs .","title":"method select_integration_requirements"},{"location":"api_docs/integrations.sklearn.helpers.digits/","text":"module integrations.sklearn.helpers.digits function get_digits get_digits() \u2192 Tuple[ndarray, ndarray, ndarray, ndarray] Returns the digits dataset in the form of a tuple of numpy arrays. function get_digits_model get_digits_model() \u2192 ClassifierMixin Creates a support vector classifier for digits dataset. This file was automatically generated via lazydocs .","title":"Integrations.sklearn.helpers.digits"},{"location":"api_docs/integrations.sklearn.helpers.digits/#module-integrationssklearnhelpersdigits","text":"","title":"module integrations.sklearn.helpers.digits"},{"location":"api_docs/integrations.sklearn.helpers.digits/#function-get_digits","text":"get_digits() \u2192 Tuple[ndarray, ndarray, ndarray, ndarray] Returns the digits dataset in the form of a tuple of numpy arrays.","title":"function get_digits"},{"location":"api_docs/integrations.sklearn.helpers.digits/#function-get_digits_model","text":"get_digits_model() \u2192 ClassifierMixin Creates a support vector classifier for digits dataset. This file was automatically generated via lazydocs .","title":"function get_digits_model"},{"location":"api_docs/integrations.sklearn.helpers/","text":"module integrations.sklearn.helpers This file was automatically generated via lazydocs .","title":"Integrations.sklearn.helpers"},{"location":"api_docs/integrations.sklearn.helpers/#module-integrationssklearnhelpers","text":"This file was automatically generated via lazydocs .","title":"module integrations.sklearn.helpers"},{"location":"api_docs/integrations.sklearn.materializers/","text":"module integrations.sklearn.materializers This file was automatically generated via lazydocs .","title":"Integrations.sklearn.materializers"},{"location":"api_docs/integrations.sklearn.materializers/#module-integrationssklearnmaterializers","text":"This file was automatically generated via lazydocs .","title":"module integrations.sklearn.materializers"},{"location":"api_docs/integrations.sklearn.materializers.sklearn_materializer/","text":"module integrations.sklearn.materializers.sklearn_materializer Global Variables DEFAULT_FILENAME class SklearnMaterializer Materializer to read data to and from sklearn. method handle_input handle_input( data_type: Type[Any] ) \u2192 Union[BaseEstimator, ClassifierMixin, ClusterMixin, BiclusterMixin, OutlierMixin, RegressorMixin, MetaEstimatorMixin, MultiOutputMixin, DensityMixin, TransformerMixin] Reads a base sklearn model from a pickle file. method handle_return handle_return( clf: Union[BaseEstimator, ClassifierMixin, ClusterMixin, BiclusterMixin, OutlierMixin, RegressorMixin, MetaEstimatorMixin, MultiOutputMixin, DensityMixin, TransformerMixin] ) \u2192 None Creates a pickle for a sklearn model. Args: clf : A sklearn model. This file was automatically generated via lazydocs .","title":"Integrations.sklearn.materializers.sklearn materializer"},{"location":"api_docs/integrations.sklearn.materializers.sklearn_materializer/#module-integrationssklearnmaterializerssklearn_materializer","text":"","title":"module integrations.sklearn.materializers.sklearn_materializer"},{"location":"api_docs/integrations.sklearn.materializers.sklearn_materializer/#global-variables","text":"DEFAULT_FILENAME","title":"Global Variables"},{"location":"api_docs/integrations.sklearn.materializers.sklearn_materializer/#class-sklearnmaterializer","text":"Materializer to read data to and from sklearn.","title":"class SklearnMaterializer"},{"location":"api_docs/integrations.sklearn.materializers.sklearn_materializer/#method-handle_input","text":"handle_input( data_type: Type[Any] ) \u2192 Union[BaseEstimator, ClassifierMixin, ClusterMixin, BiclusterMixin, OutlierMixin, RegressorMixin, MetaEstimatorMixin, MultiOutputMixin, DensityMixin, TransformerMixin] Reads a base sklearn model from a pickle file.","title":"method handle_input"},{"location":"api_docs/integrations.sklearn.materializers.sklearn_materializer/#method-handle_return","text":"handle_return( clf: Union[BaseEstimator, ClassifierMixin, ClusterMixin, BiclusterMixin, OutlierMixin, RegressorMixin, MetaEstimatorMixin, MultiOutputMixin, DensityMixin, TransformerMixin] ) \u2192 None Creates a pickle for a sklearn model. Args: clf : A sklearn model. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"api_docs/integrations.sklearn/","text":"module integrations.sklearn Global Variables SKLEARN class SklearnIntegration Definition of sklearn integration for ZenML. classmethod activate activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"Integrations.sklearn"},{"location":"api_docs/integrations.sklearn/#module-integrationssklearn","text":"","title":"module integrations.sklearn"},{"location":"api_docs/integrations.sklearn/#global-variables","text":"SKLEARN","title":"Global Variables"},{"location":"api_docs/integrations.sklearn/#class-sklearnintegration","text":"Definition of sklearn integration for ZenML.","title":"class SklearnIntegration"},{"location":"api_docs/integrations.sklearn/#classmethod-activate","text":"activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"api_docs/integrations.sklearn.steps/","text":"module integrations.sklearn.steps This file was automatically generated via lazydocs .","title":"Integrations.sklearn.steps"},{"location":"api_docs/integrations.sklearn.steps/#module-integrationssklearnsteps","text":"This file was automatically generated via lazydocs .","title":"module integrations.sklearn.steps"},{"location":"api_docs/integrations.sklearn.steps.sklearn_evaluator/","text":"module integrations.sklearn.steps.sklearn_evaluator class SklearnEvaluatorConfig Config class for the sklearn evaluator class SklearnEvaluator A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset property component Returns a TFX component. method entrypoint entrypoint( dataset: DataFrame, model: Model, config: SklearnEvaluatorConfig ) \u2192 dict Method which is responsible for the computation of the evaluation Args: dataset : a pandas Dataframe which represents the test dataset model : a trained tensorflow Keras model config : the configuration for the step Returns: a dictionary which has the evaluation report This file was automatically generated via lazydocs .","title":"Integrations.sklearn.steps.sklearn evaluator"},{"location":"api_docs/integrations.sklearn.steps.sklearn_evaluator/#module-integrationssklearnstepssklearn_evaluator","text":"","title":"module integrations.sklearn.steps.sklearn_evaluator"},{"location":"api_docs/integrations.sklearn.steps.sklearn_evaluator/#class-sklearnevaluatorconfig","text":"Config class for the sklearn evaluator","title":"class SklearnEvaluatorConfig"},{"location":"api_docs/integrations.sklearn.steps.sklearn_evaluator/#class-sklearnevaluator","text":"A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset","title":"class SklearnEvaluator"},{"location":"api_docs/integrations.sklearn.steps.sklearn_evaluator/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/integrations.sklearn.steps.sklearn_evaluator/#method-entrypoint","text":"entrypoint( dataset: DataFrame, model: Model, config: SklearnEvaluatorConfig ) \u2192 dict Method which is responsible for the computation of the evaluation Args: dataset : a pandas Dataframe which represents the test dataset model : a trained tensorflow Keras model config : the configuration for the step Returns: a dictionary which has the evaluation report This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/integrations.sklearn.steps.sklearn_splitter/","text":"module integrations.sklearn.steps.sklearn_splitter class SklearnSplitterConfig Config class for the sklearn splitter class SklearnSplitter A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits property component Returns a TFX component. method entrypoint entrypoint( dataset: DataFrame, config: SklearnSplitterConfig ) \u2192 <Output object at 0x7f4c09096c70> Method which is responsible for the splitting logic Args: dataset : a pandas Dataframe which entire dataset config : the configuration for the step Returns: three dataframes representing the splits This file was automatically generated via lazydocs .","title":"Integrations.sklearn.steps.sklearn splitter"},{"location":"api_docs/integrations.sklearn.steps.sklearn_splitter/#module-integrationssklearnstepssklearn_splitter","text":"","title":"module integrations.sklearn.steps.sklearn_splitter"},{"location":"api_docs/integrations.sklearn.steps.sklearn_splitter/#class-sklearnsplitterconfig","text":"Config class for the sklearn splitter","title":"class SklearnSplitterConfig"},{"location":"api_docs/integrations.sklearn.steps.sklearn_splitter/#class-sklearnsplitter","text":"A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits","title":"class SklearnSplitter"},{"location":"api_docs/integrations.sklearn.steps.sklearn_splitter/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/integrations.sklearn.steps.sklearn_splitter/#method-entrypoint","text":"entrypoint( dataset: DataFrame, config: SklearnSplitterConfig ) \u2192 <Output object at 0x7f4c09096c70> Method which is responsible for the splitting logic Args: dataset : a pandas Dataframe which entire dataset config : the configuration for the step Returns: three dataframes representing the splits This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/integrations.sklearn.steps.sklearn_standard_scaler/","text":"module integrations.sklearn.steps.sklearn_standard_scaler class SklearnStandardScalerConfig Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset class SklearnStandardScaler Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame property component Returns a TFX component. method entrypoint entrypoint( train_dataset: DataFrame, test_dataset: DataFrame, validation_dataset: DataFrame, statistics: DataFrame, schema: DataFrame, config: SklearnStandardScalerConfig ) \u2192 <Output object at 0x7f4c090a4550> Main entrypoint function for the StandardScaler Args: train_dataset : pd.DataFrame, the training dataset test_dataset : pd.DataFrame, the test dataset validation_dataset : pd.DataFrame, the validation dataset statistics : pd.DataFrame, the statistics over the train dataset schema : pd.DataFrame, the detected schema of the dataset config : the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames This file was automatically generated via lazydocs .","title":"Integrations.sklearn.steps.sklearn standard scaler"},{"location":"api_docs/integrations.sklearn.steps.sklearn_standard_scaler/#module-integrationssklearnstepssklearn_standard_scaler","text":"","title":"module integrations.sklearn.steps.sklearn_standard_scaler"},{"location":"api_docs/integrations.sklearn.steps.sklearn_standard_scaler/#class-sklearnstandardscalerconfig","text":"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset","title":"class SklearnStandardScalerConfig"},{"location":"api_docs/integrations.sklearn.steps.sklearn_standard_scaler/#class-sklearnstandardscaler","text":"Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame","title":"class SklearnStandardScaler"},{"location":"api_docs/integrations.sklearn.steps.sklearn_standard_scaler/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/integrations.sklearn.steps.sklearn_standard_scaler/#method-entrypoint","text":"entrypoint( train_dataset: DataFrame, test_dataset: DataFrame, validation_dataset: DataFrame, statistics: DataFrame, schema: DataFrame, config: SklearnStandardScalerConfig ) \u2192 <Output object at 0x7f4c090a4550> Main entrypoint function for the StandardScaler Args: train_dataset : pd.DataFrame, the training dataset test_dataset : pd.DataFrame, the test dataset validation_dataset : pd.DataFrame, the validation dataset statistics : pd.DataFrame, the statistics over the train dataset schema : pd.DataFrame, the detected schema of the dataset config : the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/integrations.tensorflow.materializers.keras_materializer/","text":"module integrations.tensorflow.materializers.keras_materializer Global Variables DEFAULT_FILENAME class KerasMaterializer Materializer to read/write Keras models. method handle_input handle_input(data_type: Type[Any]) \u2192 Model Reads and returns a Keras model. Returns: A tf.keras.Model model. method handle_return handle_return(model: Model) \u2192 None Writes a keras model. Args: model : A tf.keras.Model model. This file was automatically generated via lazydocs .","title":"Integrations.tensorflow.materializers.keras materializer"},{"location":"api_docs/integrations.tensorflow.materializers.keras_materializer/#module-integrationstensorflowmaterializerskeras_materializer","text":"","title":"module integrations.tensorflow.materializers.keras_materializer"},{"location":"api_docs/integrations.tensorflow.materializers.keras_materializer/#global-variables","text":"DEFAULT_FILENAME","title":"Global Variables"},{"location":"api_docs/integrations.tensorflow.materializers.keras_materializer/#class-kerasmaterializer","text":"Materializer to read/write Keras models.","title":"class KerasMaterializer"},{"location":"api_docs/integrations.tensorflow.materializers.keras_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Model Reads and returns a Keras model. Returns: A tf.keras.Model model.","title":"method handle_input"},{"location":"api_docs/integrations.tensorflow.materializers.keras_materializer/#method-handle_return","text":"handle_return(model: Model) \u2192 None Writes a keras model. Args: model : A tf.keras.Model model. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"api_docs/integrations.tensorflow.materializers/","text":"module integrations.tensorflow.materializers This file was automatically generated via lazydocs .","title":"Integrations.tensorflow.materializers"},{"location":"api_docs/integrations.tensorflow.materializers/#module-integrationstensorflowmaterializers","text":"This file was automatically generated via lazydocs .","title":"module integrations.tensorflow.materializers"},{"location":"api_docs/integrations.tensorflow.materializers.tf_dataset_materializer/","text":"module integrations.tensorflow.materializers.tf_dataset_materializer Global Variables DEFAULT_FILENAME class TensorflowDatasetMaterializer Materializer to read data to and from tf.data.Dataset. method handle_input handle_input(data_type: Type[Any]) \u2192 Any Reads data into tf.data.Dataset method handle_return handle_return(dataset: DatasetV2) \u2192 None Persists a tf.data.Dataset object. This file was automatically generated via lazydocs .","title":"Integrations.tensorflow.materializers.tf dataset materializer"},{"location":"api_docs/integrations.tensorflow.materializers.tf_dataset_materializer/#module-integrationstensorflowmaterializerstf_dataset_materializer","text":"","title":"module integrations.tensorflow.materializers.tf_dataset_materializer"},{"location":"api_docs/integrations.tensorflow.materializers.tf_dataset_materializer/#global-variables","text":"DEFAULT_FILENAME","title":"Global Variables"},{"location":"api_docs/integrations.tensorflow.materializers.tf_dataset_materializer/#class-tensorflowdatasetmaterializer","text":"Materializer to read data to and from tf.data.Dataset.","title":"class TensorflowDatasetMaterializer"},{"location":"api_docs/integrations.tensorflow.materializers.tf_dataset_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Any Reads data into tf.data.Dataset","title":"method handle_input"},{"location":"api_docs/integrations.tensorflow.materializers.tf_dataset_materializer/#method-handle_return","text":"handle_return(dataset: DatasetV2) \u2192 None Persists a tf.data.Dataset object. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"api_docs/integrations.tensorflow/","text":"module integrations.tensorflow Global Variables TENSORFLOW class TensorflowIntegration Definition of Tensorflow integration for ZenML. classmethod activate activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"Integrations.tensorflow"},{"location":"api_docs/integrations.tensorflow/#module-integrationstensorflow","text":"","title":"module integrations.tensorflow"},{"location":"api_docs/integrations.tensorflow/#global-variables","text":"TENSORFLOW","title":"Global Variables"},{"location":"api_docs/integrations.tensorflow/#class-tensorflowintegration","text":"Definition of Tensorflow integration for ZenML.","title":"class TensorflowIntegration"},{"location":"api_docs/integrations.tensorflow/#classmethod-activate","text":"activate() \u2192 None Activates the integration. This file was automatically generated via lazydocs .","title":"classmethod activate"},{"location":"api_docs/integrations.tensorflow.steps/","text":"module integrations.tensorflow.steps This file was automatically generated via lazydocs .","title":"Integrations.tensorflow.steps"},{"location":"api_docs/integrations.tensorflow.steps/#module-integrationstensorflowsteps","text":"This file was automatically generated via lazydocs .","title":"module integrations.tensorflow.steps"},{"location":"api_docs/integrations.tensorflow.steps.tensorflow_trainer/","text":"module integrations.tensorflow.steps.tensorflow_trainer class TensorflowBinaryClassifierConfig Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch class TensorflowBinaryClassifier Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset property component Returns a TFX component. method entrypoint entrypoint( train_dataset: DataFrame, validation_dataset: DataFrame, config: TensorflowBinaryClassifierConfig ) \u2192 Model Main entrypoint for the tensorflow trainer Args: train_dataset : pd.DataFrame, the training dataset validation_dataset : pd.DataFrame, the validation dataset config : the configuration of the step Returns: the trained tf.keras.Model This file was automatically generated via lazydocs .","title":"Integrations.tensorflow.steps.tensorflow trainer"},{"location":"api_docs/integrations.tensorflow.steps.tensorflow_trainer/#module-integrationstensorflowstepstensorflow_trainer","text":"","title":"module integrations.tensorflow.steps.tensorflow_trainer"},{"location":"api_docs/integrations.tensorflow.steps.tensorflow_trainer/#class-tensorflowbinaryclassifierconfig","text":"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch","title":"class TensorflowBinaryClassifierConfig"},{"location":"api_docs/integrations.tensorflow.steps.tensorflow_trainer/#class-tensorflowbinaryclassifier","text":"Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset","title":"class TensorflowBinaryClassifier"},{"location":"api_docs/integrations.tensorflow.steps.tensorflow_trainer/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/integrations.tensorflow.steps.tensorflow_trainer/#method-entrypoint","text":"entrypoint( train_dataset: DataFrame, validation_dataset: DataFrame, config: TensorflowBinaryClassifierConfig ) \u2192 Model Main entrypoint for the tensorflow trainer Args: train_dataset : pd.DataFrame, the training dataset validation_dataset : pd.DataFrame, the validation dataset config : the configuration of the step Returns: the trained tf.keras.Model This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/integrations.utils/","text":"module integrations.utils function get_integration_for_module get_integration_for_module( module_name: str ) \u2192 Union[Type[Integration], NoneType] Gets the integration class for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return None . If it is part of a ZenML integration, it will return the integration class found inside the integration init file. function get_requirements_for_module get_requirements_for_module(module_name: str) \u2192 List[str] Gets requirements for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration init file. This file was automatically generated via lazydocs .","title":"Integrations.utils"},{"location":"api_docs/integrations.utils/#module-integrationsutils","text":"","title":"module integrations.utils"},{"location":"api_docs/integrations.utils/#function-get_integration_for_module","text":"get_integration_for_module( module_name: str ) \u2192 Union[Type[Integration], NoneType] Gets the integration class for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return None . If it is part of a ZenML integration, it will return the integration class found inside the integration init file.","title":"function get_integration_for_module"},{"location":"api_docs/integrations.utils/#function-get_requirements_for_module","text":"get_requirements_for_module(module_name: str) \u2192 List[str] Gets requirements for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration init file. This file was automatically generated via lazydocs .","title":"function get_requirements_for_module"},{"location":"api_docs/io.fileio/","text":"module io.fileio Global Variables REMOTE_FS_PREFIX function open open(path: Union[bytes, str], mode: str = 'r') \u2192 Any Open a file at the given path. function copy copy( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Copy a file from the source to the destination. function file_exists file_exists(path: Union[bytes, str]) \u2192 bool Returns True if the given path exists. function remove remove(path: Union[bytes, str]) \u2192 None Remove the file at the given path. Dangerous operation. function glob glob(pattern: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return the paths that match a glob pattern. function is_dir is_dir(path: Union[bytes, str]) \u2192 bool Returns whether the given path points to a directory. function is_root is_root(path: str) \u2192 bool Returns true if path has no parent in local filesystem. Args: path : Local path in filesystem. Returns: True if root, else False. function list_dir list_dir(dir_path: str, only_file_names: bool = False) \u2192 List[str] Returns a list of files under dir. Args: dir_path : Path in filesystem. only_file_names : Returns only file names if True. Returns: List of full qualified paths. function make_dirs make_dirs(path: Union[bytes, str]) \u2192 None Make a directory at the given path, recursively creating parents. function mkdir mkdir(path: Union[bytes, str]) \u2192 None Make a directory at the given path; parent directory must exist. function rename rename( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Rename source file to destination file. Args: src : The path of the file to rename. dst : The path to rename the source file to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileExistsError : If a file already exists at the destination and overwrite is not set to True . function rm_dir rm_dir(dir_path: str) \u2192 None Deletes dir recursively. Dangerous operation. Args: dir_path : Dir to delete. function stat stat(path: Union[bytes, str]) \u2192 Any Return the stat descriptor for a given file path. function walk walk( top: Union[bytes, str], topdown: bool = True, onerror: Optional[Callable[], NoneType] = None ) \u2192 Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] Return an iterator that walks the contents of the given directory. Args: top : Path of directory to walk. topdown : Whether to walk directories topdown or bottom-up. onerror : Callable that gets called if an error occurs. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. function find_files find_files(dir_path: Union[bytes, str], pattern: str) \u2192 Iterable[str] Find files in a directory that match pattern. Args: dir_path : Path to directory. pattern : pattern like *.png. Yields: All matching filenames if found, else None. function is_remote is_remote(path: str) \u2192 bool Returns True if path exists remotely. Args: path : Any path as a string. Returns: True if remote path, else False. function create_file_if_not_exists create_file_if_not_exists(file_path: str, file_contents: str = '{}') \u2192 None Creates file if it does not exist. Args: file_path : Local path in filesystem. file_contents : Contents of file. function append_file append_file(file_path: str, file_contents: str) \u2192 None Appends file_contents to file. Args: file_path : Local path in filesystem. file_contents : Contents of file. function create_dir_if_not_exists create_dir_if_not_exists(dir_path: str) \u2192 None Creates directory if it does not exist. Args: dir_path (str): Local path in filesystem. function create_dir_recursive_if_not_exists create_dir_recursive_if_not_exists(dir_path: str) \u2192 None Creates directory recursively if it does not exist. Args: dir_path : Local path in filesystem. function resolve_relative_path resolve_relative_path(path: str) \u2192 str Takes relative path and resolves it absolutely. Args: path : Local path in filesystem. Returns: Resolved path. function copy_dir copy_dir(source_dir: str, destination_dir: str, overwrite: bool = False) \u2192 None Copies dir from source to destination. Args: source_dir : Path to copy from. destination_dir : Path to copy to. overwrite : Boolean. If false, function throws an error before overwrite. function move move(source: str, destination: str, overwrite: bool = False) \u2192 None Moves dir or file from source to destination. Can be used to rename. Args: source : Local path to copy from. destination : Local path to copy to. overwrite : boolean, if false, then throws an error before overwrite. function get_grandparent get_grandparent(dir_path: str) \u2192 str Get grandparent of dir. Args: dir_path : Path to directory. Returns: The input paths parents parent. function get_parent get_parent(dir_path: str) \u2192 str Get parent of dir. Args: dir_path (str): Path to directory. Returns: Parent (stem) of the dir as a string. function convert_to_str convert_to_str(path: Union[bytes, str]) \u2192 str Converts a PathType to a str using UTF-8. This file was automatically generated via lazydocs .","title":"Io.fileio"},{"location":"api_docs/io.fileio/#module-iofileio","text":"","title":"module io.fileio"},{"location":"api_docs/io.fileio/#global-variables","text":"REMOTE_FS_PREFIX","title":"Global Variables"},{"location":"api_docs/io.fileio/#function-open","text":"open(path: Union[bytes, str], mode: str = 'r') \u2192 Any Open a file at the given path.","title":"function open"},{"location":"api_docs/io.fileio/#function-copy","text":"copy( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Copy a file from the source to the destination.","title":"function copy"},{"location":"api_docs/io.fileio/#function-file_exists","text":"file_exists(path: Union[bytes, str]) \u2192 bool Returns True if the given path exists.","title":"function file_exists"},{"location":"api_docs/io.fileio/#function-remove","text":"remove(path: Union[bytes, str]) \u2192 None Remove the file at the given path. Dangerous operation.","title":"function remove"},{"location":"api_docs/io.fileio/#function-glob","text":"glob(pattern: Union[bytes, str]) \u2192 List[Union[bytes, str]] Return the paths that match a glob pattern.","title":"function glob"},{"location":"api_docs/io.fileio/#function-is_dir","text":"is_dir(path: Union[bytes, str]) \u2192 bool Returns whether the given path points to a directory.","title":"function is_dir"},{"location":"api_docs/io.fileio/#function-is_root","text":"is_root(path: str) \u2192 bool Returns true if path has no parent in local filesystem. Args: path : Local path in filesystem. Returns: True if root, else False.","title":"function is_root"},{"location":"api_docs/io.fileio/#function-list_dir","text":"list_dir(dir_path: str, only_file_names: bool = False) \u2192 List[str] Returns a list of files under dir. Args: dir_path : Path in filesystem. only_file_names : Returns only file names if True. Returns: List of full qualified paths.","title":"function list_dir"},{"location":"api_docs/io.fileio/#function-make_dirs","text":"make_dirs(path: Union[bytes, str]) \u2192 None Make a directory at the given path, recursively creating parents.","title":"function make_dirs"},{"location":"api_docs/io.fileio/#function-mkdir","text":"mkdir(path: Union[bytes, str]) \u2192 None Make a directory at the given path; parent directory must exist.","title":"function mkdir"},{"location":"api_docs/io.fileio/#function-rename","text":"rename( src: Union[bytes, str], dst: Union[bytes, str], overwrite: bool = False ) \u2192 None Rename source file to destination file. Args: src : The path of the file to rename. dst : The path to rename the source file to. overwrite : If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. Raises: FileExistsError : If a file already exists at the destination and overwrite is not set to True .","title":"function rename"},{"location":"api_docs/io.fileio/#function-rm_dir","text":"rm_dir(dir_path: str) \u2192 None Deletes dir recursively. Dangerous operation. Args: dir_path : Dir to delete.","title":"function rm_dir"},{"location":"api_docs/io.fileio/#function-stat","text":"stat(path: Union[bytes, str]) \u2192 Any Return the stat descriptor for a given file path.","title":"function stat"},{"location":"api_docs/io.fileio/#function-walk","text":"walk( top: Union[bytes, str], topdown: bool = True, onerror: Optional[Callable[], NoneType] = None ) \u2192 Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] Return an iterator that walks the contents of the given directory. Args: top : Path of directory to walk. topdown : Whether to walk directories topdown or bottom-up. onerror : Callable that gets called if an error occurs. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory.","title":"function walk"},{"location":"api_docs/io.fileio/#function-find_files","text":"find_files(dir_path: Union[bytes, str], pattern: str) \u2192 Iterable[str] Find files in a directory that match pattern. Args: dir_path : Path to directory. pattern : pattern like *.png. Yields: All matching filenames if found, else None.","title":"function find_files"},{"location":"api_docs/io.fileio/#function-is_remote","text":"is_remote(path: str) \u2192 bool Returns True if path exists remotely. Args: path : Any path as a string. Returns: True if remote path, else False.","title":"function is_remote"},{"location":"api_docs/io.fileio/#function-create_file_if_not_exists","text":"create_file_if_not_exists(file_path: str, file_contents: str = '{}') \u2192 None Creates file if it does not exist. Args: file_path : Local path in filesystem. file_contents : Contents of file.","title":"function create_file_if_not_exists"},{"location":"api_docs/io.fileio/#function-append_file","text":"append_file(file_path: str, file_contents: str) \u2192 None Appends file_contents to file. Args: file_path : Local path in filesystem. file_contents : Contents of file.","title":"function append_file"},{"location":"api_docs/io.fileio/#function-create_dir_if_not_exists","text":"create_dir_if_not_exists(dir_path: str) \u2192 None Creates directory if it does not exist. Args: dir_path (str): Local path in filesystem.","title":"function create_dir_if_not_exists"},{"location":"api_docs/io.fileio/#function-create_dir_recursive_if_not_exists","text":"create_dir_recursive_if_not_exists(dir_path: str) \u2192 None Creates directory recursively if it does not exist. Args: dir_path : Local path in filesystem.","title":"function create_dir_recursive_if_not_exists"},{"location":"api_docs/io.fileio/#function-resolve_relative_path","text":"resolve_relative_path(path: str) \u2192 str Takes relative path and resolves it absolutely. Args: path : Local path in filesystem. Returns: Resolved path.","title":"function resolve_relative_path"},{"location":"api_docs/io.fileio/#function-copy_dir","text":"copy_dir(source_dir: str, destination_dir: str, overwrite: bool = False) \u2192 None Copies dir from source to destination. Args: source_dir : Path to copy from. destination_dir : Path to copy to. overwrite : Boolean. If false, function throws an error before overwrite.","title":"function copy_dir"},{"location":"api_docs/io.fileio/#function-move","text":"move(source: str, destination: str, overwrite: bool = False) \u2192 None Moves dir or file from source to destination. Can be used to rename. Args: source : Local path to copy from. destination : Local path to copy to. overwrite : boolean, if false, then throws an error before overwrite.","title":"function move"},{"location":"api_docs/io.fileio/#function-get_grandparent","text":"get_grandparent(dir_path: str) \u2192 str Get grandparent of dir. Args: dir_path : Path to directory. Returns: The input paths parents parent.","title":"function get_grandparent"},{"location":"api_docs/io.fileio/#function-get_parent","text":"get_parent(dir_path: str) \u2192 str Get parent of dir. Args: dir_path (str): Path to directory. Returns: Parent (stem) of the dir as a string.","title":"function get_parent"},{"location":"api_docs/io.fileio/#function-convert_to_str","text":"convert_to_str(path: Union[bytes, str]) \u2192 str Converts a PathType to a str using UTF-8. This file was automatically generated via lazydocs .","title":"function convert_to_str"},{"location":"api_docs/io.fileio_registry/","text":"module io.fileio_registry Filesystem registry managing filesystem plugins. Global Variables default_fileio_registry class FileIORegistry Registry of pluggable filesystem implementations used in TFX components. method __init__ __init__() \u2192 None method get_filesystem_for_path get_filesystem_for_path(path: Union[bytes, str]) \u2192 Type[Filesystem] Get filesystem plugin for given path. method get_filesystem_for_scheme get_filesystem_for_scheme(scheme: Union[bytes, str]) \u2192 Type[Filesystem] Get filesystem plugin for given scheme string. method register register(filesystem_cls: Type[Filesystem]) \u2192 None Register a filesystem implementation. Args: filesystem_cls : Subclass of tfx.dsl.io.filesystem.Filesystem . This file was automatically generated via lazydocs .","title":"Io.fileio registry"},{"location":"api_docs/io.fileio_registry/#module-iofileio_registry","text":"Filesystem registry managing filesystem plugins.","title":"module io.fileio_registry"},{"location":"api_docs/io.fileio_registry/#global-variables","text":"default_fileio_registry","title":"Global Variables"},{"location":"api_docs/io.fileio_registry/#class-fileioregistry","text":"Registry of pluggable filesystem implementations used in TFX components.","title":"class FileIORegistry"},{"location":"api_docs/io.fileio_registry/#method-__init__","text":"__init__() \u2192 None","title":"method __init__"},{"location":"api_docs/io.fileio_registry/#method-get_filesystem_for_path","text":"get_filesystem_for_path(path: Union[bytes, str]) \u2192 Type[Filesystem] Get filesystem plugin for given path.","title":"method get_filesystem_for_path"},{"location":"api_docs/io.fileio_registry/#method-get_filesystem_for_scheme","text":"get_filesystem_for_scheme(scheme: Union[bytes, str]) \u2192 Type[Filesystem] Get filesystem plugin for given scheme string.","title":"method get_filesystem_for_scheme"},{"location":"api_docs/io.fileio_registry/#method-register","text":"register(filesystem_cls: Type[Filesystem]) \u2192 None Register a filesystem implementation. Args: filesystem_cls : Subclass of tfx.dsl.io.filesystem.Filesystem . This file was automatically generated via lazydocs .","title":"method register"},{"location":"api_docs/io.filesystem/","text":"module io.filesystem class NotFoundError Auxiliary not found error class FileSystemMeta Metaclass which is responsible for registering the defined filesystem in the default fileio registry. class Filesystem Abstract Filesystem class. This file was automatically generated via lazydocs .","title":"Io.filesystem"},{"location":"api_docs/io.filesystem/#module-iofilesystem","text":"","title":"module io.filesystem"},{"location":"api_docs/io.filesystem/#class-notfounderror","text":"Auxiliary not found error","title":"class NotFoundError"},{"location":"api_docs/io.filesystem/#class-filesystemmeta","text":"Metaclass which is responsible for registering the defined filesystem in the default fileio registry.","title":"class FileSystemMeta"},{"location":"api_docs/io.filesystem/#class-filesystem","text":"Abstract Filesystem class. This file was automatically generated via lazydocs .","title":"class Filesystem"},{"location":"api_docs/io/","text":"module io The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx . Global Variables DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END class BufferedIOBase Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one. class IOBase The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!') class RawIOBase Base class for raw binary I/O. class TextIOBase Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor. class UnsupportedOperation This file was automatically generated via lazydocs .","title":"Io"},{"location":"api_docs/io/#module-io","text":"The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx .","title":"module io"},{"location":"api_docs/io/#global-variables","text":"DEFAULT_BUFFER_SIZE SEEK_SET SEEK_CUR SEEK_END","title":"Global Variables"},{"location":"api_docs/io/#class-bufferediobase","text":"Base class for buffered IO objects. The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. A typical implementation should not inherit from a RawIOBase implementation, but wrap one.","title":"class BufferedIOBase"},{"location":"api_docs/io/#class-iobase","text":"The abstract base class for all I/O classes, acting on streams of bytes. There is no public constructor. This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. IOBase also supports the :keyword: with statement. In this example, fp is closed after the suite of the with statement is complete: with open('spam.txt', 'r') as fp: fp.write('Spam and eggs!')","title":"class IOBase"},{"location":"api_docs/io/#class-rawiobase","text":"Base class for raw binary I/O.","title":"class RawIOBase"},{"location":"api_docs/io/#class-textiobase","text":"Base class for text I/O. This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. There is no public constructor.","title":"class TextIOBase"},{"location":"api_docs/io/#class-unsupportedoperation","text":"This file was automatically generated via lazydocs .","title":"class UnsupportedOperation"},{"location":"api_docs/io.utils/","text":"module io.utils Global Variables APP_NAME ENV_ZENML_REPOSITORY_PATH ZENML_DIR_NAME function create_tarfile create_tarfile( source_dir: str, output_filename: str = 'zipped.tar.gz', exclude_function: Optional[Callable[[TarInfo], Union[TarInfo]], NoneType] = None ) \u2192 None Create a compressed representation of source_dir. Args: source_dir : Path to source dir. output_filename : Name of outputted gz. exclude_function : Function that determines whether to exclude file. function extract_tarfile extract_tarfile(source_tar: str, output_dir: str) \u2192 None Extracts all files in a compressed tar file to output_dir. Args: source_tar : Path to a tar compressed file. output_dir : Directory where to extract. function is_zenml_dir is_zenml_dir(path: str) \u2192 bool Check if dir is a zenml dir or not. Args: path : Path to the root. Returns: True if path contains a zenml dir, False if not. function get_zenml_config_dir get_zenml_config_dir(path: Optional[str] = None) \u2192 str Recursive function to find the zenml config starting from path. Args: path (Default value = os.getcwd()): Path to check. Returns: The full path with the resolved zenml directory. Raises: InitializationException if directory not found until root of OS. function get_zenml_dir get_zenml_dir(path: Optional[str] = None) \u2192 str Returns path to a ZenML repository directory. Args: path : Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable ZENML_REPOSITORY_PATH (if set) and recursively searching in the parent directories of the current working directory. Returns: Absolute path to a ZenML repository directory. Raises: InitializationException : If no ZenML repository is found. function get_global_config_directory get_global_config_directory() \u2192 str Returns the global config directory for ZenML. function write_file_contents_as_string write_file_contents_as_string(file_path: str, content: str) \u2192 None Writes contents of file. Args: file_path : Path to file. content : Contents of file. function read_file_contents_as_string read_file_contents_as_string(file_path: str) \u2192 str Reads contents of file. Args: file_path : Path to file. function is_gcs_path is_gcs_path(path: str) \u2192 bool Returns True if path is on Google Cloud Storage. Args: path : Any path as a string. Returns: True if gcs path, else False. This file was automatically generated via lazydocs .","title":"Io.utils"},{"location":"api_docs/io.utils/#module-ioutils","text":"","title":"module io.utils"},{"location":"api_docs/io.utils/#global-variables","text":"APP_NAME ENV_ZENML_REPOSITORY_PATH ZENML_DIR_NAME","title":"Global Variables"},{"location":"api_docs/io.utils/#function-create_tarfile","text":"create_tarfile( source_dir: str, output_filename: str = 'zipped.tar.gz', exclude_function: Optional[Callable[[TarInfo], Union[TarInfo]], NoneType] = None ) \u2192 None Create a compressed representation of source_dir. Args: source_dir : Path to source dir. output_filename : Name of outputted gz. exclude_function : Function that determines whether to exclude file.","title":"function create_tarfile"},{"location":"api_docs/io.utils/#function-extract_tarfile","text":"extract_tarfile(source_tar: str, output_dir: str) \u2192 None Extracts all files in a compressed tar file to output_dir. Args: source_tar : Path to a tar compressed file. output_dir : Directory where to extract.","title":"function extract_tarfile"},{"location":"api_docs/io.utils/#function-is_zenml_dir","text":"is_zenml_dir(path: str) \u2192 bool Check if dir is a zenml dir or not. Args: path : Path to the root. Returns: True if path contains a zenml dir, False if not.","title":"function is_zenml_dir"},{"location":"api_docs/io.utils/#function-get_zenml_config_dir","text":"get_zenml_config_dir(path: Optional[str] = None) \u2192 str Recursive function to find the zenml config starting from path. Args: path (Default value = os.getcwd()): Path to check. Returns: The full path with the resolved zenml directory. Raises: InitializationException if directory not found until root of OS.","title":"function get_zenml_config_dir"},{"location":"api_docs/io.utils/#function-get_zenml_dir","text":"get_zenml_dir(path: Optional[str] = None) \u2192 str Returns path to a ZenML repository directory. Args: path : Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable ZENML_REPOSITORY_PATH (if set) and recursively searching in the parent directories of the current working directory. Returns: Absolute path to a ZenML repository directory. Raises: InitializationException : If no ZenML repository is found.","title":"function get_zenml_dir"},{"location":"api_docs/io.utils/#function-get_global_config_directory","text":"get_global_config_directory() \u2192 str Returns the global config directory for ZenML.","title":"function get_global_config_directory"},{"location":"api_docs/io.utils/#function-write_file_contents_as_string","text":"write_file_contents_as_string(file_path: str, content: str) \u2192 None Writes contents of file. Args: file_path : Path to file. content : Contents of file.","title":"function write_file_contents_as_string"},{"location":"api_docs/io.utils/#function-read_file_contents_as_string","text":"read_file_contents_as_string(file_path: str) \u2192 str Reads contents of file. Args: file_path : Path to file.","title":"function read_file_contents_as_string"},{"location":"api_docs/io.utils/#function-is_gcs_path","text":"is_gcs_path(path: str) \u2192 bool Returns True if path is on Google Cloud Storage. Args: path : Any path as a string. Returns: True if gcs path, else False. This file was automatically generated via lazydocs .","title":"function is_gcs_path"},{"location":"api_docs/logger/","text":"module logger Global Variables ZENML_LOGGING_VERBOSITY ABSL_LOGGING_VERBOSITY APP_NAME LOG_FILE function get_logging_level get_logging_level() \u2192 <enum 'LoggingLevels'> Get logging level from the env variable. function set_root_verbosity set_root_verbosity() \u2192 None Set the root verbosity. function get_console_handler get_console_handler() \u2192 Any Get console handler for logging. function get_file_handler get_file_handler() \u2192 Any Return a file handler for logging. function get_logger get_logger(logger_name: str) \u2192 Logger Main function to get logger name,. Args: logger_name : Name of logger to initialize. Returns: A logger object. function init_logging init_logging() \u2192 None Initialize logging with default levels. class CustomFormatter Formats logs according to custom specifications. method format format(record: LogRecord) \u2192 str Converts a log record to a (colored) string Args: record : LogRecord generated by the code. Returns: A string formatted according to specifications. This file was automatically generated via lazydocs .","title":"Logger"},{"location":"api_docs/logger/#module-logger","text":"","title":"module logger"},{"location":"api_docs/logger/#global-variables","text":"ZENML_LOGGING_VERBOSITY ABSL_LOGGING_VERBOSITY APP_NAME LOG_FILE","title":"Global Variables"},{"location":"api_docs/logger/#function-get_logging_level","text":"get_logging_level() \u2192 <enum 'LoggingLevels'> Get logging level from the env variable.","title":"function get_logging_level"},{"location":"api_docs/logger/#function-set_root_verbosity","text":"set_root_verbosity() \u2192 None Set the root verbosity.","title":"function set_root_verbosity"},{"location":"api_docs/logger/#function-get_console_handler","text":"get_console_handler() \u2192 Any Get console handler for logging.","title":"function get_console_handler"},{"location":"api_docs/logger/#function-get_file_handler","text":"get_file_handler() \u2192 Any Return a file handler for logging.","title":"function get_file_handler"},{"location":"api_docs/logger/#function-get_logger","text":"get_logger(logger_name: str) \u2192 Logger Main function to get logger name,. Args: logger_name : Name of logger to initialize. Returns: A logger object.","title":"function get_logger"},{"location":"api_docs/logger/#function-init_logging","text":"init_logging() \u2192 None Initialize logging with default levels.","title":"function init_logging"},{"location":"api_docs/logger/#class-customformatter","text":"Formats logs according to custom specifications.","title":"class CustomFormatter"},{"location":"api_docs/logger/#method-format","text":"format(record: LogRecord) \u2192 str Converts a log record to a (colored) string Args: record : LogRecord generated by the code. Returns: A string formatted according to specifications. This file was automatically generated via lazydocs .","title":"method format"},{"location":"api_docs/materializers.base_materializer/","text":"module materializers.base_materializer Global Variables TYPE_CHECKING class BaseMaterializerMeta Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts. class BaseMaterializer Base Materializer to realize artifact data. method __init__ __init__(artifact: 'BaseArtifact') Initializes a materializer with the given artifact. method handle_input handle_input(data_type: Type[Any]) \u2192 Any Write logic here to handle input of the step function. Args: data_type : What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step. method handle_return handle_return(data: Any) \u2192 None Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. This file was automatically generated via lazydocs .","title":"Materializers.base materializer"},{"location":"api_docs/materializers.base_materializer/#module-materializersbase_materializer","text":"","title":"module materializers.base_materializer"},{"location":"api_docs/materializers.base_materializer/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/materializers.base_materializer/#class-basematerializermeta","text":"Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts.","title":"class BaseMaterializerMeta"},{"location":"api_docs/materializers.base_materializer/#class-basematerializer","text":"Base Materializer to realize artifact data.","title":"class BaseMaterializer"},{"location":"api_docs/materializers.base_materializer/#method-__init__","text":"__init__(artifact: 'BaseArtifact') Initializes a materializer with the given artifact.","title":"method __init__"},{"location":"api_docs/materializers.base_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Any Write logic here to handle input of the step function. Args: data_type : What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step.","title":"method handle_input"},{"location":"api_docs/materializers.base_materializer/#method-handle_return","text":"handle_return(data: Any) \u2192 None Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"api_docs/materializers.beam_materializer/","text":"module materializers.beam_materializer class BeamMaterializer Materializer to read data to and from beam. method handle_input handle_input(data_type: Type[Any]) \u2192 Any Reads all files inside the artifact directory and materializes them as a beam compatible output. method handle_return handle_return(pipeline: Pipeline) \u2192 None Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline : A beam.pipeline object. This file was automatically generated via lazydocs .","title":"Materializers.beam materializer"},{"location":"api_docs/materializers.beam_materializer/#module-materializersbeam_materializer","text":"","title":"module materializers.beam_materializer"},{"location":"api_docs/materializers.beam_materializer/#class-beammaterializer","text":"Materializer to read data to and from beam.","title":"class BeamMaterializer"},{"location":"api_docs/materializers.beam_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Any Reads all files inside the artifact directory and materializes them as a beam compatible output.","title":"method handle_input"},{"location":"api_docs/materializers.beam_materializer/#method-handle_return","text":"handle_return(pipeline: Pipeline) \u2192 None Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline : A beam.pipeline object. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"api_docs/materializers.built_in_materializer/","text":"module materializers.built_in_materializer Global Variables DEFAULT_FILENAME class BuiltInMaterializer Read/Write JSON files. method handle_input handle_input(data_type: Type[Any]) \u2192 Any Reads basic primitive types from json. method handle_return handle_return(data: Any) \u2192 None Handles basic built-in types and stores them as json This file was automatically generated via lazydocs .","title":"Materializers.built in materializer"},{"location":"api_docs/materializers.built_in_materializer/#module-materializersbuilt_in_materializer","text":"","title":"module materializers.built_in_materializer"},{"location":"api_docs/materializers.built_in_materializer/#global-variables","text":"DEFAULT_FILENAME","title":"Global Variables"},{"location":"api_docs/materializers.built_in_materializer/#class-builtinmaterializer","text":"Read/Write JSON files.","title":"class BuiltInMaterializer"},{"location":"api_docs/materializers.built_in_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 Any Reads basic primitive types from json.","title":"method handle_input"},{"location":"api_docs/materializers.built_in_materializer/#method-handle_return","text":"handle_return(data: Any) \u2192 None Handles basic built-in types and stores them as json This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"api_docs/materializers.default_materializer_registry/","text":"module materializers.default_materializer_registry Global Variables TYPE_CHECKING default_materializer_registry class MaterializerRegistry Matches a python type to a default materializer. method __init__ __init__() \u2192 None method get_materializer_types get_materializer_types() \u2192 Dict[Type[Any], Type[ForwardRef('BaseMaterializer')]] Get all registered materializer types. method is_registered is_registered(key: Type[Any]) \u2192 bool Returns if a materializer class is registered for the given type. method register_and_overwrite_type register_and_overwrite_type( key: Type[Any], type_: Type[ForwardRef('BaseMaterializer')] ) \u2192 None Registers a new materializer and also overwrites a default if set. Args: key : Indicates the type of an object. type_ : A BaseMaterializer subclass. method register_materializer_type register_materializer_type( key: Type[Any], type_: Type[ForwardRef('BaseMaterializer')] ) \u2192 None Registers a new materializer. Args: key : Indicates the type of an object. type_ : A BaseMaterializer subclass. This file was automatically generated via lazydocs .","title":"Materializers.default materializer registry"},{"location":"api_docs/materializers.default_materializer_registry/#module-materializersdefault_materializer_registry","text":"","title":"module materializers.default_materializer_registry"},{"location":"api_docs/materializers.default_materializer_registry/#global-variables","text":"TYPE_CHECKING default_materializer_registry","title":"Global Variables"},{"location":"api_docs/materializers.default_materializer_registry/#class-materializerregistry","text":"Matches a python type to a default materializer.","title":"class MaterializerRegistry"},{"location":"api_docs/materializers.default_materializer_registry/#method-__init__","text":"__init__() \u2192 None","title":"method __init__"},{"location":"api_docs/materializers.default_materializer_registry/#method-get_materializer_types","text":"get_materializer_types() \u2192 Dict[Type[Any], Type[ForwardRef('BaseMaterializer')]] Get all registered materializer types.","title":"method get_materializer_types"},{"location":"api_docs/materializers.default_materializer_registry/#method-is_registered","text":"is_registered(key: Type[Any]) \u2192 bool Returns if a materializer class is registered for the given type.","title":"method is_registered"},{"location":"api_docs/materializers.default_materializer_registry/#method-register_and_overwrite_type","text":"register_and_overwrite_type( key: Type[Any], type_: Type[ForwardRef('BaseMaterializer')] ) \u2192 None Registers a new materializer and also overwrites a default if set. Args: key : Indicates the type of an object. type_ : A BaseMaterializer subclass.","title":"method register_and_overwrite_type"},{"location":"api_docs/materializers.default_materializer_registry/#method-register_materializer_type","text":"register_materializer_type( key: Type[Any], type_: Type[ForwardRef('BaseMaterializer')] ) \u2192 None Registers a new materializer. Args: key : Indicates the type of an object. type_ : A BaseMaterializer subclass. This file was automatically generated via lazydocs .","title":"method register_materializer_type"},{"location":"api_docs/materializers/","text":"module materializers Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class. This file was automatically generated via lazydocs .","title":"Materializers"},{"location":"api_docs/materializers/#module-materializers","text":"Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class. This file was automatically generated via lazydocs .","title":"module materializers"},{"location":"api_docs/materializers.numpy_materializer/","text":"module materializers.numpy_materializer Global Variables DATA_FILENAME SHAPE_FILENAME DATA_VAR class NumpyMaterializer Materializer to read data to and from pandas. method handle_input handle_input(data_type: Type[Any]) \u2192 ndarray Reads numpy array from parquet file. method handle_return handle_return(arr: ndarray) \u2192 None Writes a np.ndarray to the artifact store as a parquet file. Args: arr : The numpy array to write. This file was automatically generated via lazydocs .","title":"Materializers.numpy materializer"},{"location":"api_docs/materializers.numpy_materializer/#module-materializersnumpy_materializer","text":"","title":"module materializers.numpy_materializer"},{"location":"api_docs/materializers.numpy_materializer/#global-variables","text":"DATA_FILENAME SHAPE_FILENAME DATA_VAR","title":"Global Variables"},{"location":"api_docs/materializers.numpy_materializer/#class-numpymaterializer","text":"Materializer to read data to and from pandas.","title":"class NumpyMaterializer"},{"location":"api_docs/materializers.numpy_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 ndarray Reads numpy array from parquet file.","title":"method handle_input"},{"location":"api_docs/materializers.numpy_materializer/#method-handle_return","text":"handle_return(arr: ndarray) \u2192 None Writes a np.ndarray to the artifact store as a parquet file. Args: arr : The numpy array to write. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"api_docs/materializers.pandas_materializer/","text":"module materializers.pandas_materializer Global Variables DEFAULT_FILENAME COMPRESSION_TYPE class PandasMaterializer Materializer to read data to and from pandas. method handle_input handle_input(data_type: Type[Any]) \u2192 DataFrame Reads pd.Dataframe from a parquet file. method handle_return handle_return(df: DataFrame) \u2192 None Writes a pandas dataframe to the specified filename. Args: df : The pandas dataframe to write. This file was automatically generated via lazydocs .","title":"Materializers.pandas materializer"},{"location":"api_docs/materializers.pandas_materializer/#module-materializerspandas_materializer","text":"","title":"module materializers.pandas_materializer"},{"location":"api_docs/materializers.pandas_materializer/#global-variables","text":"DEFAULT_FILENAME COMPRESSION_TYPE","title":"Global Variables"},{"location":"api_docs/materializers.pandas_materializer/#class-pandasmaterializer","text":"Materializer to read data to and from pandas.","title":"class PandasMaterializer"},{"location":"api_docs/materializers.pandas_materializer/#method-handle_input","text":"handle_input(data_type: Type[Any]) \u2192 DataFrame Reads pd.Dataframe from a parquet file.","title":"method handle_input"},{"location":"api_docs/materializers.pandas_materializer/#method-handle_return","text":"handle_return(df: DataFrame) \u2192 None Writes a pandas dataframe to the specified filename. Args: df : The pandas dataframe to write. This file was automatically generated via lazydocs .","title":"method handle_return"},{"location":"api_docs/metadata_stores.base_metadata_store/","text":"module metadata_stores.base_metadata_store Global Variables PIPELINE_CONTEXT_TYPE_NAME PIPELINE_RUN_CONTEXT_TYPE_NAME DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY INTERNAL_EXECUTION_PARAMETER_PREFIX PARAM_PIPELINE_PARAMETER_NAME class BaseMetadataStore Metadata store base class to track metadata of zenml first class citizens. method __init__ __init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseMetadataStore instance. Args: repo_path : Path to the repository of this metadata store. property step_type_mapping Maps type_id's to step names. property store General property that hooks into TFX metadata store. method get_pipeline get_pipeline(pipeline_name: str) \u2192 Union[PipelineView, NoneType] Returns a pipeline for the given name. method get_pipeline_run get_pipeline_run( pipeline: PipelineView, run_name: str ) \u2192 Union[PipelineRunView, NoneType] Gets a specific run for the given pipeline. method get_pipeline_run_steps get_pipeline_run_steps(pipeline_run: PipelineRunView) \u2192 Dict[str, StepView] Gets all steps for the given pipeline run. method get_pipeline_runs get_pipeline_runs(pipeline: PipelineView) \u2192 Dict[str, PipelineRunView] Gets all runs for the given pipeline. method get_pipelines get_pipelines() \u2192 List[PipelineView] Returns a list of all pipelines stored in this metadata store. method get_producer_step_from_artifact get_producer_step_from_artifact(artifact: ArtifactView) \u2192 StepView Returns original StepView from an ArtifactView. Args: artifact : ArtifactView to be queried. Returns: Original StepView that produced the artifact. method get_step_artifacts get_step_artifacts( step: StepView ) \u2192 Tuple[Dict[str, ArtifactView], Dict[str, ArtifactView]] Returns input and output artifacts for the given step. Args: step : The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. method get_step_by_id get_step_by_id(step_id: int) \u2192 StepView Gets a StepView by its ID method get_step_status get_step_status(step: StepView) \u2192 <enum 'ExecutionStatus'> Gets the execution status of a single step. method get_tfx_metadata_config get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config. This file was automatically generated via lazydocs .","title":"Metadata stores.base metadata store"},{"location":"api_docs/metadata_stores.base_metadata_store/#module-metadata_storesbase_metadata_store","text":"","title":"module metadata_stores.base_metadata_store"},{"location":"api_docs/metadata_stores.base_metadata_store/#global-variables","text":"PIPELINE_CONTEXT_TYPE_NAME PIPELINE_RUN_CONTEXT_TYPE_NAME DATATYPE_PROPERTY_KEY MATERIALIZER_PROPERTY_KEY INTERNAL_EXECUTION_PARAMETER_PREFIX PARAM_PIPELINE_PARAMETER_NAME","title":"Global Variables"},{"location":"api_docs/metadata_stores.base_metadata_store/#class-basemetadatastore","text":"Metadata store base class to track metadata of zenml first class citizens.","title":"class BaseMetadataStore"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-__init__","text":"__init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseMetadataStore instance. Args: repo_path : Path to the repository of this metadata store.","title":"method __init__"},{"location":"api_docs/metadata_stores.base_metadata_store/#property-step_type_mapping","text":"Maps type_id's to step names.","title":"property step_type_mapping"},{"location":"api_docs/metadata_stores.base_metadata_store/#property-store","text":"General property that hooks into TFX metadata store.","title":"property store"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-get_pipeline","text":"get_pipeline(pipeline_name: str) \u2192 Union[PipelineView, NoneType] Returns a pipeline for the given name.","title":"method get_pipeline"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-get_pipeline_run","text":"get_pipeline_run( pipeline: PipelineView, run_name: str ) \u2192 Union[PipelineRunView, NoneType] Gets a specific run for the given pipeline.","title":"method get_pipeline_run"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-get_pipeline_run_steps","text":"get_pipeline_run_steps(pipeline_run: PipelineRunView) \u2192 Dict[str, StepView] Gets all steps for the given pipeline run.","title":"method get_pipeline_run_steps"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-get_pipeline_runs","text":"get_pipeline_runs(pipeline: PipelineView) \u2192 Dict[str, PipelineRunView] Gets all runs for the given pipeline.","title":"method get_pipeline_runs"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-get_pipelines","text":"get_pipelines() \u2192 List[PipelineView] Returns a list of all pipelines stored in this metadata store.","title":"method get_pipelines"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-get_producer_step_from_artifact","text":"get_producer_step_from_artifact(artifact: ArtifactView) \u2192 StepView Returns original StepView from an ArtifactView. Args: artifact : ArtifactView to be queried. Returns: Original StepView that produced the artifact.","title":"method get_producer_step_from_artifact"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-get_step_artifacts","text":"get_step_artifacts( step: StepView ) \u2192 Tuple[Dict[str, ArtifactView], Dict[str, ArtifactView]] Returns input and output artifacts for the given step. Args: step : The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively.","title":"method get_step_artifacts"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-get_step_by_id","text":"get_step_by_id(step_id: int) \u2192 StepView Gets a StepView by its ID","title":"method get_step_by_id"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-get_step_status","text":"get_step_status(step: StepView) \u2192 <enum 'ExecutionStatus'> Gets the execution status of a single step.","title":"method get_step_status"},{"location":"api_docs/metadata_stores.base_metadata_store/#method-get_tfx_metadata_config","text":"get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config. This file was automatically generated via lazydocs .","title":"method get_tfx_metadata_config"},{"location":"api_docs/metadata_stores/","text":"module metadata_stores The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store. This file was automatically generated via lazydocs .","title":"Metadata stores"},{"location":"api_docs/metadata_stores/#module-metadata_stores","text":"The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store. This file was automatically generated via lazydocs .","title":"module metadata_stores"},{"location":"api_docs/metadata_stores.mysql_metadata_store/","text":"module metadata_stores.mysql_metadata_store class MySQLMetadataStore MySQL backend for ZenML metadata store. property step_type_mapping Maps type_id's to step names. property store General property that hooks into TFX metadata store. method get_tfx_metadata_config get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config for mysql metadata store. This file was automatically generated via lazydocs .","title":"Metadata stores.mysql metadata store"},{"location":"api_docs/metadata_stores.mysql_metadata_store/#module-metadata_storesmysql_metadata_store","text":"","title":"module metadata_stores.mysql_metadata_store"},{"location":"api_docs/metadata_stores.mysql_metadata_store/#class-mysqlmetadatastore","text":"MySQL backend for ZenML metadata store.","title":"class MySQLMetadataStore"},{"location":"api_docs/metadata_stores.mysql_metadata_store/#property-step_type_mapping","text":"Maps type_id's to step names.","title":"property step_type_mapping"},{"location":"api_docs/metadata_stores.mysql_metadata_store/#property-store","text":"General property that hooks into TFX metadata store.","title":"property store"},{"location":"api_docs/metadata_stores.mysql_metadata_store/#method-get_tfx_metadata_config","text":"get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config for mysql metadata store. This file was automatically generated via lazydocs .","title":"method get_tfx_metadata_config"},{"location":"api_docs/metadata_stores.sqlite_metadata_store/","text":"module metadata_stores.sqlite_metadata_store class SQLiteMetadataStore SQLite backend for ZenML metadata store. method __init__ __init__(**data: Any) Constructor for MySQL MetadataStore for ZenML. property step_type_mapping Maps type_id's to step names. property store General property that hooks into TFX metadata store. method get_tfx_metadata_config get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config for sqlite metadata store. classmethod uri_must_be_local uri_must_be_local(v: str) \u2192 str Validator to ensure uri is local This file was automatically generated via lazydocs .","title":"Metadata stores.sqlite metadata store"},{"location":"api_docs/metadata_stores.sqlite_metadata_store/#module-metadata_storessqlite_metadata_store","text":"","title":"module metadata_stores.sqlite_metadata_store"},{"location":"api_docs/metadata_stores.sqlite_metadata_store/#class-sqlitemetadatastore","text":"SQLite backend for ZenML metadata store.","title":"class SQLiteMetadataStore"},{"location":"api_docs/metadata_stores.sqlite_metadata_store/#method-__init__","text":"__init__(**data: Any) Constructor for MySQL MetadataStore for ZenML.","title":"method __init__"},{"location":"api_docs/metadata_stores.sqlite_metadata_store/#property-step_type_mapping","text":"Maps type_id's to step names.","title":"property step_type_mapping"},{"location":"api_docs/metadata_stores.sqlite_metadata_store/#property-store","text":"General property that hooks into TFX metadata store.","title":"property store"},{"location":"api_docs/metadata_stores.sqlite_metadata_store/#method-get_tfx_metadata_config","text":"get_tfx_metadata_config() \u2192 ConnectionConfig Return tfx metadata config for sqlite metadata store.","title":"method get_tfx_metadata_config"},{"location":"api_docs/metadata_stores.sqlite_metadata_store/#classmethod-uri_must_be_local","text":"uri_must_be_local(v: str) \u2192 str Validator to ensure uri is local This file was automatically generated via lazydocs .","title":"classmethod uri_must_be_local"},{"location":"api_docs/orchestrators.base_orchestrator/","text":"module orchestrators.base_orchestrator Global Variables TYPE_CHECKING class BaseOrchestrator Base Orchestrator class to orchestrate ZenML pipelines. method __init__ __init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseOrchestrator instance. Args: repo_path : Path to the repository of this orchestrator. property is_running Returns whether the orchestrator is currently running. property log_file Returns path to a log file if available. method down down() \u2192 None Destroys resources for the orchestrator. method post_run post_run() \u2192 None Should be run after the run() to clean up. method pre_run pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Should be run before the run() function to prepare orchestrator. Args: pipeline : Pipeline that will be run. caller_filepath : Path to the file in which pipeline.run() was called. This is necessary for airflow so we know the file in which the DAG is defined. method run run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 Any Abstract method to run a pipeline. Overwrite this in subclasses with a concrete implementation on how to run the given pipeline. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Potential additional parameters used in subclass implementations. method up up() \u2192 None Provisions resources for the orchestrator. This file was automatically generated via lazydocs .","title":"Orchestrators.base orchestrator"},{"location":"api_docs/orchestrators.base_orchestrator/#module-orchestratorsbase_orchestrator","text":"","title":"module orchestrators.base_orchestrator"},{"location":"api_docs/orchestrators.base_orchestrator/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/orchestrators.base_orchestrator/#class-baseorchestrator","text":"Base Orchestrator class to orchestrate ZenML pipelines.","title":"class BaseOrchestrator"},{"location":"api_docs/orchestrators.base_orchestrator/#method-__init__","text":"__init__(repo_path: str, **kwargs: Any) \u2192 None Initializes a BaseOrchestrator instance. Args: repo_path : Path to the repository of this orchestrator.","title":"method __init__"},{"location":"api_docs/orchestrators.base_orchestrator/#property-is_running","text":"Returns whether the orchestrator is currently running.","title":"property is_running"},{"location":"api_docs/orchestrators.base_orchestrator/#property-log_file","text":"Returns path to a log file if available.","title":"property log_file"},{"location":"api_docs/orchestrators.base_orchestrator/#method-down","text":"down() \u2192 None Destroys resources for the orchestrator.","title":"method down"},{"location":"api_docs/orchestrators.base_orchestrator/#method-post_run","text":"post_run() \u2192 None Should be run after the run() to clean up.","title":"method post_run"},{"location":"api_docs/orchestrators.base_orchestrator/#method-pre_run","text":"pre_run(pipeline: 'BasePipeline', caller_filepath: str) \u2192 None Should be run before the run() function to prepare orchestrator. Args: pipeline : Pipeline that will be run. caller_filepath : Path to the file in which pipeline.run() was called. This is necessary for airflow so we know the file in which the DAG is defined.","title":"method pre_run"},{"location":"api_docs/orchestrators.base_orchestrator/#method-run","text":"run(zenml_pipeline: 'BasePipeline', run_name: str, **kwargs: Any) \u2192 Any Abstract method to run a pipeline. Overwrite this in subclasses with a concrete implementation on how to run the given pipeline. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **kwargs : Potential additional parameters used in subclass implementations.","title":"method run"},{"location":"api_docs/orchestrators.base_orchestrator/#method-up","text":"up() \u2192 None Provisions resources for the orchestrator. This file was automatically generated via lazydocs .","title":"method up"},{"location":"api_docs/orchestrators.local.local_dag_runner/","text":"module orchestrators.local.local_dag_runner Inspired by local dag runner implementation by Google at: https://github.com/tensorflow/tfx/blob/master/tfx/orchestration /local/local_dag_runner.py Global Variables PIPELINE_RUN_ID_PARAMETER_NAME class LocalDagRunner Local TFX DAG runner. method __init__ __init__() \u2192 None Initializes LocalDagRunner as a TFX orchestrator. method run run(pipeline: Pipeline, run_name: str = '') \u2192 None Runs given logical pipeline locally. Args: pipeline : Logical pipeline containing pipeline args and components. run_name : Name of the pipeline run. This file was automatically generated via lazydocs .","title":"Orchestrators.local.local dag runner"},{"location":"api_docs/orchestrators.local.local_dag_runner/#module-orchestratorslocallocal_dag_runner","text":"Inspired by local dag runner implementation by Google at: https://github.com/tensorflow/tfx/blob/master/tfx/orchestration /local/local_dag_runner.py","title":"module orchestrators.local.local_dag_runner"},{"location":"api_docs/orchestrators.local.local_dag_runner/#global-variables","text":"PIPELINE_RUN_ID_PARAMETER_NAME","title":"Global Variables"},{"location":"api_docs/orchestrators.local.local_dag_runner/#class-localdagrunner","text":"Local TFX DAG runner.","title":"class LocalDagRunner"},{"location":"api_docs/orchestrators.local.local_dag_runner/#method-__init__","text":"__init__() \u2192 None Initializes LocalDagRunner as a TFX orchestrator.","title":"method __init__"},{"location":"api_docs/orchestrators.local.local_dag_runner/#method-run","text":"run(pipeline: Pipeline, run_name: str = '') \u2192 None Runs given logical pipeline locally. Args: pipeline : Logical pipeline containing pipeline args and components. run_name : Name of the pipeline run. This file was automatically generated via lazydocs .","title":"method run"},{"location":"api_docs/orchestrators.local.local_orchestrator/","text":"module orchestrators.local.local_orchestrator Global Variables TYPE_CHECKING class LocalOrchestrator Orchestrator responsible for running pipelines locally. property is_running Returns whether the orchestrator is currently running. property log_file Returns path to a log file if available. method run run(zenml_pipeline: 'BasePipeline', run_name: str, **pipeline_args: Any) \u2192 None Runs a pipeline locally. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **pipeline_args : Unused kwargs to conform with base signature. This file was automatically generated via lazydocs .","title":"Orchestrators.local.local orchestrator"},{"location":"api_docs/orchestrators.local.local_orchestrator/#module-orchestratorslocallocal_orchestrator","text":"","title":"module orchestrators.local.local_orchestrator"},{"location":"api_docs/orchestrators.local.local_orchestrator/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/orchestrators.local.local_orchestrator/#class-localorchestrator","text":"Orchestrator responsible for running pipelines locally.","title":"class LocalOrchestrator"},{"location":"api_docs/orchestrators.local.local_orchestrator/#property-is_running","text":"Returns whether the orchestrator is currently running.","title":"property is_running"},{"location":"api_docs/orchestrators.local.local_orchestrator/#property-log_file","text":"Returns path to a log file if available.","title":"property log_file"},{"location":"api_docs/orchestrators.local.local_orchestrator/#method-run","text":"run(zenml_pipeline: 'BasePipeline', run_name: str, **pipeline_args: Any) \u2192 None Runs a pipeline locally. Args: zenml_pipeline : The pipeline to run. run_name : Name of the pipeline run. **pipeline_args : Unused kwargs to conform with base signature. This file was automatically generated via lazydocs .","title":"method run"},{"location":"api_docs/orchestrators.local/","text":"module orchestrators.local This file was automatically generated via lazydocs .","title":"Orchestrators.local"},{"location":"api_docs/orchestrators.local/#module-orchestratorslocal","text":"This file was automatically generated via lazydocs .","title":"module orchestrators.local"},{"location":"api_docs/orchestrators/","text":"module orchestrators An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline. This file was automatically generated via lazydocs .","title":"Orchestrators"},{"location":"api_docs/orchestrators/#module-orchestrators","text":"An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline. This file was automatically generated via lazydocs .","title":"module orchestrators"},{"location":"api_docs/orchestrators.utils/","text":"module orchestrators.utils Global Variables TYPE_CHECKING function create_tfx_pipeline create_tfx_pipeline(zenml_pipeline: 'BasePipeline') \u2192 Pipeline Creates a tfx pipeline from a ZenML pipeline. function execute_step execute_step(tfx_launcher: Launcher) \u2192 Union[ExecutionInfo, NoneType] Executes a tfx component. Args: tfx_launcher : A tfx launcher to execute the component. Returns: Optional execution info returned by the launcher. This file was automatically generated via lazydocs .","title":"Orchestrators.utils"},{"location":"api_docs/orchestrators.utils/#module-orchestratorsutils","text":"","title":"module orchestrators.utils"},{"location":"api_docs/orchestrators.utils/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/orchestrators.utils/#function-create_tfx_pipeline","text":"create_tfx_pipeline(zenml_pipeline: 'BasePipeline') \u2192 Pipeline Creates a tfx pipeline from a ZenML pipeline.","title":"function create_tfx_pipeline"},{"location":"api_docs/orchestrators.utils/#function-execute_step","text":"execute_step(tfx_launcher: Launcher) \u2192 Union[ExecutionInfo, NoneType] Executes a tfx component. Args: tfx_launcher : A tfx launcher to execute the component. Returns: Optional execution info returned by the launcher. This file was automatically generated via lazydocs .","title":"function execute_step"},{"location":"api_docs/pipelines.base_pipeline/","text":"module pipelines.base_pipeline Global Variables ENV_ZENML_PREVENT_PIPELINE_EXECUTION SHOULD_PREVENT_PIPELINE_EXECUTION PIPELINE_INNER_FUNC_NAME PARAM_ENABLE_CACHE PARAM_REQUIREMENTS_FILE PARAM_DOCKERIGNORE_FILE INSTANCE_CONFIGURATION class BasePipelineMeta Pipeline Metaclass responsible for validating the pipeline definition. class BasePipeline Abstract base class for all ZenML pipelines. Attributes: name : The name of this pipeline. enable_cache : A boolean indicating if caching is enabled for this pipeline. requirements_file : Optional path to a pip requirements file that contains all requirements to run the pipeline. method __init__ __init__(*args: BaseStep, **kwargs: Any) \u2192 None property stack Returns the stack for this pipeline. property steps Returns a dictionary of pipeline steps. method connect connect(*args: BaseStep, **kwargs: BaseStep) \u2192 None Function that connects inputs and outputs of the pipeline steps. method run run(run_name: Optional[str] = None) \u2192 Any Runs the pipeline using the orchestrator of the pipeline stack. Args: run_name : Optional name for the run. method with_config with_config( self: ~T, config_file: str, overwrite_step_parameters: bool = False ) \u2192 ~T Configures this pipeline using a yaml file. Args: config_file : Path to a yaml file which contains configuration options for running this pipeline. See https : //docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters : If set to True , values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. This file was automatically generated via lazydocs .","title":"Pipelines.base pipeline"},{"location":"api_docs/pipelines.base_pipeline/#module-pipelinesbase_pipeline","text":"","title":"module pipelines.base_pipeline"},{"location":"api_docs/pipelines.base_pipeline/#global-variables","text":"ENV_ZENML_PREVENT_PIPELINE_EXECUTION SHOULD_PREVENT_PIPELINE_EXECUTION PIPELINE_INNER_FUNC_NAME PARAM_ENABLE_CACHE PARAM_REQUIREMENTS_FILE PARAM_DOCKERIGNORE_FILE INSTANCE_CONFIGURATION","title":"Global Variables"},{"location":"api_docs/pipelines.base_pipeline/#class-basepipelinemeta","text":"Pipeline Metaclass responsible for validating the pipeline definition.","title":"class BasePipelineMeta"},{"location":"api_docs/pipelines.base_pipeline/#class-basepipeline","text":"Abstract base class for all ZenML pipelines. Attributes: name : The name of this pipeline. enable_cache : A boolean indicating if caching is enabled for this pipeline. requirements_file : Optional path to a pip requirements file that contains all requirements to run the pipeline.","title":"class BasePipeline"},{"location":"api_docs/pipelines.base_pipeline/#method-__init__","text":"__init__(*args: BaseStep, **kwargs: Any) \u2192 None","title":"method __init__"},{"location":"api_docs/pipelines.base_pipeline/#property-stack","text":"Returns the stack for this pipeline.","title":"property stack"},{"location":"api_docs/pipelines.base_pipeline/#property-steps","text":"Returns a dictionary of pipeline steps.","title":"property steps"},{"location":"api_docs/pipelines.base_pipeline/#method-connect","text":"connect(*args: BaseStep, **kwargs: BaseStep) \u2192 None Function that connects inputs and outputs of the pipeline steps.","title":"method connect"},{"location":"api_docs/pipelines.base_pipeline/#method-run","text":"run(run_name: Optional[str] = None) \u2192 Any Runs the pipeline using the orchestrator of the pipeline stack. Args: run_name : Optional name for the run.","title":"method run"},{"location":"api_docs/pipelines.base_pipeline/#method-with_config","text":"with_config( self: ~T, config_file: str, overwrite_step_parameters: bool = False ) \u2192 ~T Configures this pipeline using a yaml file. Args: config_file : Path to a yaml file which contains configuration options for running this pipeline. See https : //docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters : If set to True , values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. This file was automatically generated via lazydocs .","title":"method with_config"},{"location":"api_docs/pipelines.builtin_pipelines/","text":"module pipelines.builtin_pipelines This file was automatically generated via lazydocs .","title":"Pipelines.builtin pipelines"},{"location":"api_docs/pipelines.builtin_pipelines/#module-pipelinesbuiltin_pipelines","text":"This file was automatically generated via lazydocs .","title":"module pipelines.builtin_pipelines"},{"location":"api_docs/pipelines.builtin_pipelines.training_pipeline/","text":"module pipelines.builtin_pipelines.training_pipeline class TrainingPipeline Class for the classic training pipeline implementation property stack Returns the stack for this pipeline. property steps Returns a dictionary of pipeline steps. method connect connect( datasource: BaseDatasourceStep, splitter: BaseSplitStep, analyzer: BaseAnalyzerStep, preprocessor: BasePreprocessorStep, trainer: BaseTrainerStep, evaluator: BaseEvaluatorStep ) \u2192 None Main connect method for the standard training pipelines Args: datasource : the step responsible for the data ingestion splitter : the step responsible for splitting the dataset into train, test, val analyzer : the step responsible for extracting the statistics and the schema preprocessor : the step responsible for preprocessing the data trainer : the step responsible for training a model evaluator : the step responsible for computing the evaluation of the trained model This file was automatically generated via lazydocs .","title":"Pipelines.builtin pipelines.training pipeline"},{"location":"api_docs/pipelines.builtin_pipelines.training_pipeline/#module-pipelinesbuiltin_pipelinestraining_pipeline","text":"","title":"module pipelines.builtin_pipelines.training_pipeline"},{"location":"api_docs/pipelines.builtin_pipelines.training_pipeline/#class-trainingpipeline","text":"Class for the classic training pipeline implementation","title":"class TrainingPipeline"},{"location":"api_docs/pipelines.builtin_pipelines.training_pipeline/#property-stack","text":"Returns the stack for this pipeline.","title":"property stack"},{"location":"api_docs/pipelines.builtin_pipelines.training_pipeline/#property-steps","text":"Returns a dictionary of pipeline steps.","title":"property steps"},{"location":"api_docs/pipelines.builtin_pipelines.training_pipeline/#method-connect","text":"connect( datasource: BaseDatasourceStep, splitter: BaseSplitStep, analyzer: BaseAnalyzerStep, preprocessor: BasePreprocessorStep, trainer: BaseTrainerStep, evaluator: BaseEvaluatorStep ) \u2192 None Main connect method for the standard training pipelines Args: datasource : the step responsible for the data ingestion splitter : the step responsible for splitting the dataset into train, test, val analyzer : the step responsible for extracting the statistics and the schema preprocessor : the step responsible for preprocessing the data trainer : the step responsible for training a model evaluator : the step responsible for computing the evaluation of the trained model This file was automatically generated via lazydocs .","title":"method connect"},{"location":"api_docs/pipelines/","text":"module pipelines A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator. This file was automatically generated via lazydocs .","title":"Pipelines"},{"location":"api_docs/pipelines/#module-pipelines","text":"A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator. This file was automatically generated via lazydocs .","title":"module pipelines"},{"location":"api_docs/pipelines.pipeline_decorator/","text":"module pipelines.pipeline_decorator Global Variables INSTANCE_CONFIGURATION PARAM_DOCKERIGNORE_FILE PARAM_ENABLE_CACHE PARAM_REQUIREMENTS_FILE PIPELINE_INNER_FUNC_NAME function pipeline pipeline( _func: Optional[~F] = None, name: Optional[str] = None, enable_cache: bool = True, requirements_file: Optional[str] = None, dockerignore_file: Optional[str] = None ) \u2192 Union[Type[BasePipeline], Callable[[~F], Type[BasePipeline]]] Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Args: _func : The decorated function. name : The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. enable_cache : Whether to use caching or not. requirements_file : Optional path to a pip requirements file that contains requirements to run the pipeline. dockerignore_file : Optional path to a dockerignore file to use when building docker images for running this pipeline. **Note** : If you pass a file, make sure it does not include the .zen directory as it is needed to run ZenML inside the container. Returns: the inner decorator which creates the pipeline class based on the ZenML BasePipeline This file was automatically generated via lazydocs .","title":"Pipelines.pipeline decorator"},{"location":"api_docs/pipelines.pipeline_decorator/#module-pipelinespipeline_decorator","text":"","title":"module pipelines.pipeline_decorator"},{"location":"api_docs/pipelines.pipeline_decorator/#global-variables","text":"INSTANCE_CONFIGURATION PARAM_DOCKERIGNORE_FILE PARAM_ENABLE_CACHE PARAM_REQUIREMENTS_FILE PIPELINE_INNER_FUNC_NAME","title":"Global Variables"},{"location":"api_docs/pipelines.pipeline_decorator/#function-pipeline","text":"pipeline( _func: Optional[~F] = None, name: Optional[str] = None, enable_cache: bool = True, requirements_file: Optional[str] = None, dockerignore_file: Optional[str] = None ) \u2192 Union[Type[BasePipeline], Callable[[~F], Type[BasePipeline]]] Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Args: _func : The decorated function. name : The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. enable_cache : Whether to use caching or not. requirements_file : Optional path to a pip requirements file that contains requirements to run the pipeline. dockerignore_file : Optional path to a dockerignore file to use when building docker images for running this pipeline. **Note** : If you pass a file, make sure it does not include the .zen directory as it is needed to run ZenML inside the container. Returns: the inner decorator which creates the pipeline class based on the ZenML BasePipeline This file was automatically generated via lazydocs .","title":"function pipeline"},{"location":"api_docs/post_execution.artifact/","text":"module post_execution.artifact Global Variables TYPE_CHECKING class ArtifactView Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution. method __init__ __init__( id_: int, type_: str, uri: str, materializer: str, data_type: str, metadata_store: 'BaseMetadataStore', parent_step_id: int ) Initializes a post-execution artifact object. In most cases ArtifactView objects should not be created manually but retrieved from a StepView via the inputs or outputs properties. Args: id_ : The artifact id. type_ : The type of this artifact. uri : Specifies where the artifact data is stored. materializer : Information needed to restore the materializer that was used to write this artifact. data_type : The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id : The ID of the parent step. property data_type Returns the data type of the artifact. property id Returns the artifact id. property is_cached Returns True if artifact was cached in a previous run, else False. property parent_step_id Returns the ID of the parent step. This need not be equivalent to the ID of the producer step. property producer_step Returns the original StepView that produced the artifact. property type Returns the artifact type. property uri Returns the URI where the artifact data is stored. method read read( output_data_type: Optional[Type[Any]] = None, materializer_class: Optional[Type[ForwardRef('BaseMaterializer')]] = None ) \u2192 Any Materializes the data stored in this artifact. Args: output_data_type : The datatype to which the materializer should read, will be passed to the materializers handle_input method. materializer_class : The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. This file was automatically generated via lazydocs .","title":"Post execution.artifact"},{"location":"api_docs/post_execution.artifact/#module-post_executionartifact","text":"","title":"module post_execution.artifact"},{"location":"api_docs/post_execution.artifact/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/post_execution.artifact/#class-artifactview","text":"Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution.","title":"class ArtifactView"},{"location":"api_docs/post_execution.artifact/#method-__init__","text":"__init__( id_: int, type_: str, uri: str, materializer: str, data_type: str, metadata_store: 'BaseMetadataStore', parent_step_id: int ) Initializes a post-execution artifact object. In most cases ArtifactView objects should not be created manually but retrieved from a StepView via the inputs or outputs properties. Args: id_ : The artifact id. type_ : The type of this artifact. uri : Specifies where the artifact data is stored. materializer : Information needed to restore the materializer that was used to write this artifact. data_type : The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id : The ID of the parent step.","title":"method __init__"},{"location":"api_docs/post_execution.artifact/#property-data_type","text":"Returns the data type of the artifact.","title":"property data_type"},{"location":"api_docs/post_execution.artifact/#property-id","text":"Returns the artifact id.","title":"property id"},{"location":"api_docs/post_execution.artifact/#property-is_cached","text":"Returns True if artifact was cached in a previous run, else False.","title":"property is_cached"},{"location":"api_docs/post_execution.artifact/#property-parent_step_id","text":"Returns the ID of the parent step. This need not be equivalent to the ID of the producer step.","title":"property parent_step_id"},{"location":"api_docs/post_execution.artifact/#property-producer_step","text":"Returns the original StepView that produced the artifact.","title":"property producer_step"},{"location":"api_docs/post_execution.artifact/#property-type","text":"Returns the artifact type.","title":"property type"},{"location":"api_docs/post_execution.artifact/#property-uri","text":"Returns the URI where the artifact data is stored.","title":"property uri"},{"location":"api_docs/post_execution.artifact/#method-read","text":"read( output_data_type: Optional[Type[Any]] = None, materializer_class: Optional[Type[ForwardRef('BaseMaterializer')]] = None ) \u2192 Any Materializes the data stored in this artifact. Args: output_data_type : The datatype to which the materializer should read, will be passed to the materializers handle_input method. materializer_class : The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. This file was automatically generated via lazydocs .","title":"method read"},{"location":"api_docs/post_execution/","text":"module post_execution After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object. This file was automatically generated via lazydocs .","title":"Post execution"},{"location":"api_docs/post_execution/#module-post_execution","text":"After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object. This file was automatically generated via lazydocs .","title":"module post_execution"},{"location":"api_docs/post_execution.pipeline/","text":"module post_execution.pipeline Global Variables TYPE_CHECKING class PipelineView Post-execution pipeline class which can be used to query pipeline-related information from the metadata store. method __init__ __init__(id_: int, name: str, metadata_store: 'BaseMetadataStore') Initializes a post-execution pipeline object. In most cases PipelineView objects should not be created manually but retrieved using the get_pipelines() method of a zenml.core.repo.Repository instead. Args: id_ : The context id of this pipeline. name : The name of this pipeline. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline. property name Returns the name of the pipeline. property runs Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list. method get_run get_run(name: str) \u2192 PipelineRunView Returns a run for the given name. Args: name : The name of the run to return. Raises: KeyError : If there is no run with the given name. method get_run_names get_run_names() \u2192 List[str] Returns a list of all run names. This file was automatically generated via lazydocs .","title":"Post execution.pipeline"},{"location":"api_docs/post_execution.pipeline/#module-post_executionpipeline","text":"","title":"module post_execution.pipeline"},{"location":"api_docs/post_execution.pipeline/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/post_execution.pipeline/#class-pipelineview","text":"Post-execution pipeline class which can be used to query pipeline-related information from the metadata store.","title":"class PipelineView"},{"location":"api_docs/post_execution.pipeline/#method-__init__","text":"__init__(id_: int, name: str, metadata_store: 'BaseMetadataStore') Initializes a post-execution pipeline object. In most cases PipelineView objects should not be created manually but retrieved using the get_pipelines() method of a zenml.core.repo.Repository instead. Args: id_ : The context id of this pipeline. name : The name of this pipeline. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline.","title":"method __init__"},{"location":"api_docs/post_execution.pipeline/#property-name","text":"Returns the name of the pipeline.","title":"property name"},{"location":"api_docs/post_execution.pipeline/#property-runs","text":"Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list.","title":"property runs"},{"location":"api_docs/post_execution.pipeline/#method-get_run","text":"get_run(name: str) \u2192 PipelineRunView Returns a run for the given name. Args: name : The name of the run to return. Raises: KeyError : If there is no run with the given name.","title":"method get_run"},{"location":"api_docs/post_execution.pipeline/#method-get_run_names","text":"get_run_names() \u2192 List[str] Returns a list of all run names. This file was automatically generated via lazydocs .","title":"method get_run_names"},{"location":"api_docs/post_execution.pipeline_run/","text":"module post_execution.pipeline_run Global Variables TYPE_CHECKING class PipelineRunView Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution. method __init__ __init__( id_: int, name: str, executions: List[Execution], metadata_store: 'BaseMetadataStore' ) Initializes a post-execution pipeline run object. In most cases PipelineRunView objects should not be created manually but retrieved from a PipelineView object instead. Args: id_ : The context id of this pipeline run. name : The name of this pipeline run. executions : All executions associated with this pipeline run. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline run. property name Returns the name of the pipeline run. property status Returns the current status of the pipeline run. property steps Returns all steps that were executed as part of this pipeline run. method get_step get_step(name: str) \u2192 StepView Returns a step for the given name. Args: name : The name of the step to return. Raises: KeyError : If there is no step with the given name. method get_step_names get_step_names() \u2192 List[str] Returns a list of all step names. This file was automatically generated via lazydocs .","title":"Post execution.pipeline run"},{"location":"api_docs/post_execution.pipeline_run/#module-post_executionpipeline_run","text":"","title":"module post_execution.pipeline_run"},{"location":"api_docs/post_execution.pipeline_run/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/post_execution.pipeline_run/#class-pipelinerunview","text":"Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution.","title":"class PipelineRunView"},{"location":"api_docs/post_execution.pipeline_run/#method-__init__","text":"__init__( id_: int, name: str, executions: List[Execution], metadata_store: 'BaseMetadataStore' ) Initializes a post-execution pipeline run object. In most cases PipelineRunView objects should not be created manually but retrieved from a PipelineView object instead. Args: id_ : The context id of this pipeline run. name : The name of this pipeline run. executions : All executions associated with this pipeline run. metadata_store : The metadata store which should be used to fetch additional information related to this pipeline run.","title":"method __init__"},{"location":"api_docs/post_execution.pipeline_run/#property-name","text":"Returns the name of the pipeline run.","title":"property name"},{"location":"api_docs/post_execution.pipeline_run/#property-status","text":"Returns the current status of the pipeline run.","title":"property status"},{"location":"api_docs/post_execution.pipeline_run/#property-steps","text":"Returns all steps that were executed as part of this pipeline run.","title":"property steps"},{"location":"api_docs/post_execution.pipeline_run/#method-get_step","text":"get_step(name: str) \u2192 StepView Returns a step for the given name. Args: name : The name of the step to return. Raises: KeyError : If there is no step with the given name.","title":"method get_step"},{"location":"api_docs/post_execution.pipeline_run/#method-get_step_names","text":"get_step_names() \u2192 List[str] Returns a list of all step names. This file was automatically generated via lazydocs .","title":"method get_step_names"},{"location":"api_docs/post_execution.step/","text":"module post_execution.step Global Variables TYPE_CHECKING class StepView Post-execution step class which can be used to query artifact information associated with a pipeline step. method __init__ __init__( id_: int, parents_step_ids: List[int], name: str, pipeline_step_name: str, parameters: Dict[str, Any], metadata_store: 'BaseMetadataStore' ) Initializes a post-execution step object. In most cases StepView objects should not be created manually but retrieved from a PipelineRunView object instead. Args: id_ : The execution id of this step. parents_step_ids : The execution ids of the parents of this step. name : The name of this step. pipeline_step_name : The name of this step within the pipeline parameters : Parameters that were used to run this step. metadata_store : The metadata store which should be used to fetch additional information related to this step. property id Returns the step id. property input Returns the input artifact that was used to run this step. Raises: ValueError : If there were zero or multiple inputs to this step. property inputs Returns all input artifacts that were used to run this step. property is_cached Returns whether the step is cached or not. property name Returns the step name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples: # the step name will be \"my_step\" @step(name=\"my_step\") def my_step_function(...) # the step name will be \"my_step_function\" @step def my_step_function(...) property output Returns the output artifact that was written by this step. Raises: ValueError : If there were zero or multiple step outputs. property outputs Returns all output artifacts that were written by this step. property parameters The parameters used to run this step. property parent_steps Returns a list of all parent steps of this step. property parents_step_ids Returns a list of ID's of all parents of this step. property pipeline_step_name Returns the pipeline step name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The pipeline step name will be step_a property status Returns the current status of the step. This file was automatically generated via lazydocs .","title":"Post execution.step"},{"location":"api_docs/post_execution.step/#module-post_executionstep","text":"","title":"module post_execution.step"},{"location":"api_docs/post_execution.step/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/post_execution.step/#class-stepview","text":"Post-execution step class which can be used to query artifact information associated with a pipeline step.","title":"class StepView"},{"location":"api_docs/post_execution.step/#method-__init__","text":"__init__( id_: int, parents_step_ids: List[int], name: str, pipeline_step_name: str, parameters: Dict[str, Any], metadata_store: 'BaseMetadataStore' ) Initializes a post-execution step object. In most cases StepView objects should not be created manually but retrieved from a PipelineRunView object instead. Args: id_ : The execution id of this step. parents_step_ids : The execution ids of the parents of this step. name : The name of this step. pipeline_step_name : The name of this step within the pipeline parameters : Parameters that were used to run this step. metadata_store : The metadata store which should be used to fetch additional information related to this step.","title":"method __init__"},{"location":"api_docs/post_execution.step/#property-id","text":"Returns the step id.","title":"property id"},{"location":"api_docs/post_execution.step/#property-input","text":"Returns the input artifact that was used to run this step. Raises: ValueError : If there were zero or multiple inputs to this step.","title":"property input"},{"location":"api_docs/post_execution.step/#property-inputs","text":"Returns all input artifacts that were used to run this step.","title":"property inputs"},{"location":"api_docs/post_execution.step/#property-is_cached","text":"Returns whether the step is cached or not.","title":"property is_cached"},{"location":"api_docs/post_execution.step/#property-name","text":"Returns the step name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples: # the step name will be \"my_step\" @step(name=\"my_step\") def my_step_function(...) # the step name will be \"my_step_function\" @step def my_step_function(...)","title":"property name"},{"location":"api_docs/post_execution.step/#property-output","text":"Returns the output artifact that was written by this step. Raises: ValueError : If there were zero or multiple step outputs.","title":"property output"},{"location":"api_docs/post_execution.step/#property-outputs","text":"Returns all output artifacts that were written by this step.","title":"property outputs"},{"location":"api_docs/post_execution.step/#property-parameters","text":"The parameters used to run this step.","title":"property parameters"},{"location":"api_docs/post_execution.step/#property-parent_steps","text":"Returns a list of all parent steps of this step.","title":"property parent_steps"},{"location":"api_docs/post_execution.step/#property-parents_step_ids","text":"Returns a list of ID's of all parents of this step.","title":"property parents_step_ids"},{"location":"api_docs/post_execution.step/#property-pipeline_step_name","text":"Returns the pipeline step name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The pipeline step name will be step_a","title":"property pipeline_step_name"},{"location":"api_docs/post_execution.step/#property-status","text":"Returns the current status of the step. This file was automatically generated via lazydocs .","title":"property status"},{"location":"api_docs/stacks.base_stack/","text":"module stacks.base_stack Global Variables TYPE_CHECKING class BaseStack Base stack for ZenML. A ZenML stack brings together a Metadata Store, an Artifact Store, and an Orchestrator, the trifecta of the environment required to run a ZenML pipeline. A ZenML stack also happens to be a pydantic BaseSettings class, which means that there are multiple ways to use it. You can set it via env variables. * You can set it through the config yaml file. * You can set it in code by initializing an object of this class, and passing it to pipelines as a configuration. In the case where a value is specified for the same Settings field in multiple ways, the selected value is determined as follows (in descending order of priority): Arguments passed to the Settings class initializer. * Environment variables, e.g. zenml_var as described above. * Variables loaded from a config yaml file. * The default field values. property artifact_store Returns the artifact store of this stack. property container_registry Returns the optional container registry of this stack. property metadata_store Returns the metadata store of this stack. property orchestrator Returns the orchestrator of this stack. method dict dict(**kwargs: Any) \u2192 Dict[str, Any] Removes private attributes from pydantic dict so they don't get stored in our config files. This file was automatically generated via lazydocs .","title":"Stacks.base stack"},{"location":"api_docs/stacks.base_stack/#module-stacksbase_stack","text":"","title":"module stacks.base_stack"},{"location":"api_docs/stacks.base_stack/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/stacks.base_stack/#class-basestack","text":"Base stack for ZenML. A ZenML stack brings together a Metadata Store, an Artifact Store, and an Orchestrator, the trifecta of the environment required to run a ZenML pipeline. A ZenML stack also happens to be a pydantic BaseSettings class, which means that there are multiple ways to use it. You can set it via env variables. * You can set it through the config yaml file. * You can set it in code by initializing an object of this class, and passing it to pipelines as a configuration. In the case where a value is specified for the same Settings field in multiple ways, the selected value is determined as follows (in descending order of priority): Arguments passed to the Settings class initializer. * Environment variables, e.g. zenml_var as described above. * Variables loaded from a config yaml file. * The default field values.","title":"class BaseStack"},{"location":"api_docs/stacks.base_stack/#property-artifact_store","text":"Returns the artifact store of this stack.","title":"property artifact_store"},{"location":"api_docs/stacks.base_stack/#property-container_registry","text":"Returns the optional container registry of this stack.","title":"property container_registry"},{"location":"api_docs/stacks.base_stack/#property-metadata_store","text":"Returns the metadata store of this stack.","title":"property metadata_store"},{"location":"api_docs/stacks.base_stack/#property-orchestrator","text":"Returns the orchestrator of this stack.","title":"property orchestrator"},{"location":"api_docs/stacks.base_stack/#method-dict","text":"dict(**kwargs: Any) \u2192 Dict[str, Any] Removes private attributes from pydantic dict so they don't get stored in our config files. This file was automatically generated via lazydocs .","title":"method dict"},{"location":"api_docs/stacks.constants/","text":"module stacks.constants Global Variables DEFAULT_STACK_KEY This file was automatically generated via lazydocs .","title":"Stacks.constants"},{"location":"api_docs/stacks.constants/#module-stacksconstants","text":"","title":"module stacks.constants"},{"location":"api_docs/stacks.constants/#global-variables","text":"DEFAULT_STACK_KEY This file was automatically generated via lazydocs .","title":"Global Variables"},{"location":"api_docs/stacks/","text":"module stacks A stack is made up of the following three core components: an Artifact Store, a Metadata Store, and an Orchestrator (backend). A ZenML stack also happens to be a Pydantic BaseSettings class, which means that there are multiple ways to use it. This file was automatically generated via lazydocs .","title":"Stacks"},{"location":"api_docs/stacks/#module-stacks","text":"A stack is made up of the following three core components: an Artifact Store, a Metadata Store, and an Orchestrator (backend). A ZenML stack also happens to be a Pydantic BaseSettings class, which means that there are multiple ways to use it. This file was automatically generated via lazydocs .","title":"module stacks"},{"location":"api_docs/steps.base_step/","text":"module steps.base_step Global Variables INSTANCE_CONFIGURATION INTERNAL_EXECUTION_PARAMETER_PREFIX PARAM_CREATED_BY_FUNCTIONAL_API PARAM_ENABLE_CACHE PARAM_PIPELINE_PARAMETER_NAME SINGLE_RETURN_OUT_NAME STEP_INNER_FUNC_NAME class BaseStepMeta Metaclass for BaseStep . Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class class BaseStep Abstract base class for all ZenML steps. Attributes: name : The name of this step. pipeline_parameter_name : The name of the pipeline parameter for which this step was passed as an argument. enable_cache : A boolean indicating if caching is enabled for this step. requires_context : A boolean indicating if this step requires a StepContext object during execution. method __init__ __init__(*args: Any, **kwargs: Any) \u2192 None property component Returns a TFX component. method entrypoint entrypoint(*args: Any, **kwargs: Any) \u2192 Any Abstract method for core step logic. method get_materializers get_materializers( ensure_complete: bool = False ) \u2192 Dict[str, Type[BaseMaterializer]] Returns available materializers for the outputs of this step. Args: ensure_complete : If set to True , this method will raise a StepInterfaceError if no materializer can be found for an output. Returns: A dictionary mapping output names to BaseMaterializer subclasses. If no explicit materializer was set using step.with_return_materializers(...) , this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError : (Only if ensure_complete is set to True ) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. method with_return_materializers with_return_materializers( self: ~T, materializers: Union[Type[BaseMaterializer], Dict[str, Type[BaseMaterializer]]] ) \u2192 ~T Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers : The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError : If a materializer is not a BaseMaterializer subclass or a materializer for a non-existent output is given. This file was automatically generated via lazydocs .","title":"Steps.base step"},{"location":"api_docs/steps.base_step/#module-stepsbase_step","text":"","title":"module steps.base_step"},{"location":"api_docs/steps.base_step/#global-variables","text":"INSTANCE_CONFIGURATION INTERNAL_EXECUTION_PARAMETER_PREFIX PARAM_CREATED_BY_FUNCTIONAL_API PARAM_ENABLE_CACHE PARAM_PIPELINE_PARAMETER_NAME SINGLE_RETURN_OUT_NAME STEP_INNER_FUNC_NAME","title":"Global Variables"},{"location":"api_docs/steps.base_step/#class-basestepmeta","text":"Metaclass for BaseStep . Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class","title":"class BaseStepMeta"},{"location":"api_docs/steps.base_step/#class-basestep","text":"Abstract base class for all ZenML steps. Attributes: name : The name of this step. pipeline_parameter_name : The name of the pipeline parameter for which this step was passed as an argument. enable_cache : A boolean indicating if caching is enabled for this step. requires_context : A boolean indicating if this step requires a StepContext object during execution.","title":"class BaseStep"},{"location":"api_docs/steps.base_step/#method-__init__","text":"__init__(*args: Any, **kwargs: Any) \u2192 None","title":"method __init__"},{"location":"api_docs/steps.base_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/steps.base_step/#method-entrypoint","text":"entrypoint(*args: Any, **kwargs: Any) \u2192 Any Abstract method for core step logic.","title":"method entrypoint"},{"location":"api_docs/steps.base_step/#method-get_materializers","text":"get_materializers( ensure_complete: bool = False ) \u2192 Dict[str, Type[BaseMaterializer]] Returns available materializers for the outputs of this step. Args: ensure_complete : If set to True , this method will raise a StepInterfaceError if no materializer can be found for an output. Returns: A dictionary mapping output names to BaseMaterializer subclasses. If no explicit materializer was set using step.with_return_materializers(...) , this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError : (Only if ensure_complete is set to True ) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type.","title":"method get_materializers"},{"location":"api_docs/steps.base_step/#method-with_return_materializers","text":"with_return_materializers( self: ~T, materializers: Union[Type[BaseMaterializer], Dict[str, Type[BaseMaterializer]]] ) \u2192 ~T Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers : The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError : If a materializer is not a BaseMaterializer subclass or a materializer for a non-existent output is given. This file was automatically generated via lazydocs .","title":"method with_return_materializers"},{"location":"api_docs/steps.base_step_config/","text":"module steps.base_step_config class BaseStepConfig Base configuration class to pass execution params into a step. This file was automatically generated via lazydocs .","title":"Steps.base step config"},{"location":"api_docs/steps.base_step_config/#module-stepsbase_step_config","text":"","title":"module steps.base_step_config"},{"location":"api_docs/steps.base_step_config/#class-basestepconfig","text":"Base configuration class to pass execution params into a step. This file was automatically generated via lazydocs .","title":"class BaseStepConfig"},{"location":"api_docs/steps.builtin_steps/","text":"module steps.builtin_steps This file was automatically generated via lazydocs .","title":"Steps.builtin steps"},{"location":"api_docs/steps.builtin_steps/#module-stepsbuiltin_steps","text":"This file was automatically generated via lazydocs .","title":"module steps.builtin_steps"},{"location":"api_docs/steps.builtin_steps.pandas_analyzer/","text":"module steps.builtin_steps.pandas_analyzer class PandasAnalyzerConfig Config class for the PandasAnalyzer Config class PandasAnalyzer Simple step implementation which analyzes a given pd.DataFrame property component Returns a TFX component. method entrypoint entrypoint( dataset: DataFrame, config: PandasAnalyzerConfig ) \u2192 <Output object at 0x7f4c0960cdc0> Main entrypoint function for the pandas analyzer Args: dataset : pd.DataFrame, the given dataset config : the configuration of the step Returns: the statistics and the schema of the given dataframe This file was automatically generated via lazydocs .","title":"Steps.builtin steps.pandas analyzer"},{"location":"api_docs/steps.builtin_steps.pandas_analyzer/#module-stepsbuiltin_stepspandas_analyzer","text":"","title":"module steps.builtin_steps.pandas_analyzer"},{"location":"api_docs/steps.builtin_steps.pandas_analyzer/#class-pandasanalyzerconfig","text":"Config class for the PandasAnalyzer Config","title":"class PandasAnalyzerConfig"},{"location":"api_docs/steps.builtin_steps.pandas_analyzer/#class-pandasanalyzer","text":"Simple step implementation which analyzes a given pd.DataFrame","title":"class PandasAnalyzer"},{"location":"api_docs/steps.builtin_steps.pandas_analyzer/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/steps.builtin_steps.pandas_analyzer/#method-entrypoint","text":"entrypoint( dataset: DataFrame, config: PandasAnalyzerConfig ) \u2192 <Output object at 0x7f4c0960cdc0> Main entrypoint function for the pandas analyzer Args: dataset : pd.DataFrame, the given dataset config : the configuration of the step Returns: the statistics and the schema of the given dataframe This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/steps.builtin_steps.pandas_datasource/","text":"module steps.builtin_steps.pandas_datasource class PandasDatasourceConfig Config class for the pandas csv datasource class PandasDatasource Simple step implementation to ingest from a csv file using pandas property component Returns a TFX component. method entrypoint entrypoint(config: PandasDatasourceConfig) \u2192 DataFrame Main entrypoint method for the PandasDatasource Args: config : the configuration of the step Returns: the resulting dataframe This file was automatically generated via lazydocs .","title":"Steps.builtin steps.pandas datasource"},{"location":"api_docs/steps.builtin_steps.pandas_datasource/#module-stepsbuiltin_stepspandas_datasource","text":"","title":"module steps.builtin_steps.pandas_datasource"},{"location":"api_docs/steps.builtin_steps.pandas_datasource/#class-pandasdatasourceconfig","text":"Config class for the pandas csv datasource","title":"class PandasDatasourceConfig"},{"location":"api_docs/steps.builtin_steps.pandas_datasource/#class-pandasdatasource","text":"Simple step implementation to ingest from a csv file using pandas","title":"class PandasDatasource"},{"location":"api_docs/steps.builtin_steps.pandas_datasource/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/steps.builtin_steps.pandas_datasource/#method-entrypoint","text":"entrypoint(config: PandasDatasourceConfig) \u2192 DataFrame Main entrypoint method for the PandasDatasource Args: config : the configuration of the step Returns: the resulting dataframe This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/steps/","text":"module steps A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator. This file was automatically generated via lazydocs .","title":"Steps"},{"location":"api_docs/steps/#module-steps","text":"A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator. This file was automatically generated via lazydocs .","title":"module steps"},{"location":"api_docs/steps.step_context/","text":"module steps.step_context Global Variables TYPE_CHECKING class StepContextOutput Tuple containing materializer class and artifact for a step output. class StepContext Provides additional context inside a step function. This class is used to access materializers and artifact URIs inside a step function. To use it, add a StepContext object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a StepContext object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the StepContext and automatically pass it when executing your step. Note : When using a StepContext inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the @step decorator or when initializing your custom step class. method __init__ __init__( step_name: str, output_materializers: Dict[str, Type[ForwardRef('BaseMaterializer')]], output_artifacts: Dict[str, ForwardRef('BaseArtifact')] ) Initializes a StepContext instance. Args: step_name : The name of the step that this context is used in. output_materializers : The output materializers of the step that this context is used in. output_artifacts : The output artifacts of the step that this context is used in. Raises: StepInterfaceError : If the keys of the output materializers and output artifacts do not match. method get_output_artifact_uri get_output_artifact_uri(output_name: Optional[str] = None) \u2192 str Returns the artifact URI for a given step output. Args: output_name : Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepInterfaceError : If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. method get_output_materializer get_output_materializer( output_name: Optional[str] = None, custom_materializer_class: Optional[Type[ForwardRef('BaseMaterializer')]] = None ) \u2192 BaseMaterializer Returns a materializer for a given step output. Args: output_name : Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class : If given, this BaseMaterializer subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepInterfaceError : If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. This file was automatically generated via lazydocs .","title":"Steps.step context"},{"location":"api_docs/steps.step_context/#module-stepsstep_context","text":"","title":"module steps.step_context"},{"location":"api_docs/steps.step_context/#global-variables","text":"TYPE_CHECKING","title":"Global Variables"},{"location":"api_docs/steps.step_context/#class-stepcontextoutput","text":"Tuple containing materializer class and artifact for a step output.","title":"class StepContextOutput"},{"location":"api_docs/steps.step_context/#class-stepcontext","text":"Provides additional context inside a step function. This class is used to access materializers and artifact URIs inside a step function. To use it, add a StepContext object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a StepContext object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the StepContext and automatically pass it when executing your step. Note : When using a StepContext inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the @step decorator or when initializing your custom step class.","title":"class StepContext"},{"location":"api_docs/steps.step_context/#method-__init__","text":"__init__( step_name: str, output_materializers: Dict[str, Type[ForwardRef('BaseMaterializer')]], output_artifacts: Dict[str, ForwardRef('BaseArtifact')] ) Initializes a StepContext instance. Args: step_name : The name of the step that this context is used in. output_materializers : The output materializers of the step that this context is used in. output_artifacts : The output artifacts of the step that this context is used in. Raises: StepInterfaceError : If the keys of the output materializers and output artifacts do not match.","title":"method __init__"},{"location":"api_docs/steps.step_context/#method-get_output_artifact_uri","text":"get_output_artifact_uri(output_name: Optional[str] = None) \u2192 str Returns the artifact URI for a given step output. Args: output_name : Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepInterfaceError : If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs.","title":"method get_output_artifact_uri"},{"location":"api_docs/steps.step_context/#method-get_output_materializer","text":"get_output_materializer( output_name: Optional[str] = None, custom_materializer_class: Optional[Type[ForwardRef('BaseMaterializer')]] = None ) \u2192 BaseMaterializer Returns a materializer for a given step output. Args: output_name : Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class : If given, this BaseMaterializer subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepInterfaceError : If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. This file was automatically generated via lazydocs .","title":"method get_output_materializer"},{"location":"api_docs/steps.step_decorator/","text":"module steps.step_decorator Global Variables TYPE_CHECKING INSTANCE_CONFIGURATION OUTPUT_SPEC PARAM_CREATED_BY_FUNCTIONAL_API PARAM_ENABLE_CACHE STEP_INNER_FUNC_NAME function step step( _func: Optional[~F] = None, name: Optional[str] = None, enable_cache: Optional[bool] = None, output_types: Optional[Dict[str, Type[ForwardRef('BaseArtifact')]]] = None ) \u2192 Union[Type[BaseStep], Callable[[~F], Type[BaseStep]]] Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as name , it features a nested decorator structure. Args: _func : The decorated function. name : The name of the step. If left empty, the name of the decorated function will be used as a fallback. enable_cache : Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a StepContext (see :class : zenml.steps.step_context.StepContext for more information). output_types : A dictionary which sets different outputs to non-default artifact types Returns: the inner decorator which creates the step class based on the ZenML BaseStep This file was automatically generated via lazydocs .","title":"Steps.step decorator"},{"location":"api_docs/steps.step_decorator/#module-stepsstep_decorator","text":"","title":"module steps.step_decorator"},{"location":"api_docs/steps.step_decorator/#global-variables","text":"TYPE_CHECKING INSTANCE_CONFIGURATION OUTPUT_SPEC PARAM_CREATED_BY_FUNCTIONAL_API PARAM_ENABLE_CACHE STEP_INNER_FUNC_NAME","title":"Global Variables"},{"location":"api_docs/steps.step_decorator/#function-step","text":"step( _func: Optional[~F] = None, name: Optional[str] = None, enable_cache: Optional[bool] = None, output_types: Optional[Dict[str, Type[ForwardRef('BaseArtifact')]]] = None ) \u2192 Union[Type[BaseStep], Callable[[~F], Type[BaseStep]]] Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as name , it features a nested decorator structure. Args: _func : The decorated function. name : The name of the step. If left empty, the name of the decorated function will be used as a fallback. enable_cache : Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a StepContext (see :class : zenml.steps.step_context.StepContext for more information). output_types : A dictionary which sets different outputs to non-default artifact types Returns: the inner decorator which creates the step class based on the ZenML BaseStep This file was automatically generated via lazydocs .","title":"function step"},{"location":"api_docs/steps.step_interfaces.base_analyzer_step/","text":"module steps.step_interfaces.base_analyzer_step class BaseAnalyzerConfig Base class for analyzer step configurations class BaseAnalyzerStep Base step implementation for any analyzer step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( dataset: DataArtifact, config: BaseAnalyzerConfig, context: StepContext ) \u2192 <Output object at 0x7f4c08f5d2b0> Base entrypoint for any analyzer implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base analyzer step"},{"location":"api_docs/steps.step_interfaces.base_analyzer_step/#module-stepsstep_interfacesbase_analyzer_step","text":"","title":"module steps.step_interfaces.base_analyzer_step"},{"location":"api_docs/steps.step_interfaces.base_analyzer_step/#class-baseanalyzerconfig","text":"Base class for analyzer step configurations","title":"class BaseAnalyzerConfig"},{"location":"api_docs/steps.step_interfaces.base_analyzer_step/#class-baseanalyzerstep","text":"Base step implementation for any analyzer step implementation on ZenML","title":"class BaseAnalyzerStep"},{"location":"api_docs/steps.step_interfaces.base_analyzer_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/steps.step_interfaces.base_analyzer_step/#method-entrypoint","text":"entrypoint( dataset: DataArtifact, config: BaseAnalyzerConfig, context: StepContext ) \u2192 <Output object at 0x7f4c08f5d2b0> Base entrypoint for any analyzer implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/steps.step_interfaces.base_datasource_step/","text":"module steps.step_interfaces.base_datasource_step class BaseDatasourceConfig Base class for datasource configs to inherit from class BaseDatasourceStep Base step implementation for any datasource step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint(config: BaseDatasourceConfig, context: StepContext) \u2192 DataArtifact Base entrypoint for any datasource implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base datasource step"},{"location":"api_docs/steps.step_interfaces.base_datasource_step/#module-stepsstep_interfacesbase_datasource_step","text":"","title":"module steps.step_interfaces.base_datasource_step"},{"location":"api_docs/steps.step_interfaces.base_datasource_step/#class-basedatasourceconfig","text":"Base class for datasource configs to inherit from","title":"class BaseDatasourceConfig"},{"location":"api_docs/steps.step_interfaces.base_datasource_step/#class-basedatasourcestep","text":"Base step implementation for any datasource step implementation on ZenML","title":"class BaseDatasourceStep"},{"location":"api_docs/steps.step_interfaces.base_datasource_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/steps.step_interfaces.base_datasource_step/#method-entrypoint","text":"entrypoint(config: BaseDatasourceConfig, context: StepContext) \u2192 DataArtifact Base entrypoint for any datasource implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/steps.step_interfaces.base_drift_detection_step/","text":"module steps.step_interfaces.base_drift_detection_step class BaseDriftDetectionConfig Base class for drift detection step configurations class BaseDriftDetectionStep Base step implementation for any drift detection step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( reference_dataset: DataArtifact, comparison_dataset: DataArtifact, config: BaseDriftDetectionConfig, context: StepContext ) \u2192 Any Base entrypoint for any drift detection implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base drift detection step"},{"location":"api_docs/steps.step_interfaces.base_drift_detection_step/#module-stepsstep_interfacesbase_drift_detection_step","text":"","title":"module steps.step_interfaces.base_drift_detection_step"},{"location":"api_docs/steps.step_interfaces.base_drift_detection_step/#class-basedriftdetectionconfig","text":"Base class for drift detection step configurations","title":"class BaseDriftDetectionConfig"},{"location":"api_docs/steps.step_interfaces.base_drift_detection_step/#class-basedriftdetectionstep","text":"Base step implementation for any drift detection step implementation on ZenML","title":"class BaseDriftDetectionStep"},{"location":"api_docs/steps.step_interfaces.base_drift_detection_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/steps.step_interfaces.base_drift_detection_step/#method-entrypoint","text":"entrypoint( reference_dataset: DataArtifact, comparison_dataset: DataArtifact, config: BaseDriftDetectionConfig, context: StepContext ) \u2192 Any Base entrypoint for any drift detection implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/steps.step_interfaces.base_evaluator_step/","text":"module steps.step_interfaces.base_evaluator_step class BaseEvaluatorConfig Base class for evaluator step configurations class BaseEvaluatorStep Base step implementation for any evaluator step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( dataset: DataArtifact, model: ModelArtifact, config: BaseEvaluatorConfig, context: StepContext ) \u2192 DataArtifact Base entrypoint for any evaluator implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base evaluator step"},{"location":"api_docs/steps.step_interfaces.base_evaluator_step/#module-stepsstep_interfacesbase_evaluator_step","text":"","title":"module steps.step_interfaces.base_evaluator_step"},{"location":"api_docs/steps.step_interfaces.base_evaluator_step/#class-baseevaluatorconfig","text":"Base class for evaluator step configurations","title":"class BaseEvaluatorConfig"},{"location":"api_docs/steps.step_interfaces.base_evaluator_step/#class-baseevaluatorstep","text":"Base step implementation for any evaluator step implementation on ZenML","title":"class BaseEvaluatorStep"},{"location":"api_docs/steps.step_interfaces.base_evaluator_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/steps.step_interfaces.base_evaluator_step/#method-entrypoint","text":"entrypoint( dataset: DataArtifact, model: ModelArtifact, config: BaseEvaluatorConfig, context: StepContext ) \u2192 DataArtifact Base entrypoint for any evaluator implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/steps.step_interfaces.base_preprocessor_step/","text":"module steps.step_interfaces.base_preprocessor_step class BasePreprocessorConfig Base class for Preprocessor step configurations class BasePreprocessorStep Base step implementation for any Preprocessor step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( train_dataset: DataArtifact, test_dataset: DataArtifact, validation_dataset: DataArtifact, statistics: StatisticsArtifact, schema: SchemaArtifact, config: BasePreprocessorConfig, context: StepContext ) \u2192 <Output object at 0x7f4c08f73070> Base entrypoint for any Preprocessor implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base preprocessor step"},{"location":"api_docs/steps.step_interfaces.base_preprocessor_step/#module-stepsstep_interfacesbase_preprocessor_step","text":"","title":"module steps.step_interfaces.base_preprocessor_step"},{"location":"api_docs/steps.step_interfaces.base_preprocessor_step/#class-basepreprocessorconfig","text":"Base class for Preprocessor step configurations","title":"class BasePreprocessorConfig"},{"location":"api_docs/steps.step_interfaces.base_preprocessor_step/#class-basepreprocessorstep","text":"Base step implementation for any Preprocessor step implementation on ZenML","title":"class BasePreprocessorStep"},{"location":"api_docs/steps.step_interfaces.base_preprocessor_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/steps.step_interfaces.base_preprocessor_step/#method-entrypoint","text":"entrypoint( train_dataset: DataArtifact, test_dataset: DataArtifact, validation_dataset: DataArtifact, statistics: StatisticsArtifact, schema: SchemaArtifact, config: BasePreprocessorConfig, context: StepContext ) \u2192 <Output object at 0x7f4c08f73070> Base entrypoint for any Preprocessor implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/steps.step_interfaces.base_split_step/","text":"module steps.step_interfaces.base_split_step class BaseSplitStepConfig Base class for split configs to inherit from class BaseSplitStep Base step implementation for any split step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( dataset: DataArtifact, config: BaseSplitStepConfig, context: StepContext ) \u2192 <Output object at 0x7f4c08f73310> Entrypoint for a function for the split steps to run This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base split step"},{"location":"api_docs/steps.step_interfaces.base_split_step/#module-stepsstep_interfacesbase_split_step","text":"","title":"module steps.step_interfaces.base_split_step"},{"location":"api_docs/steps.step_interfaces.base_split_step/#class-basesplitstepconfig","text":"Base class for split configs to inherit from","title":"class BaseSplitStepConfig"},{"location":"api_docs/steps.step_interfaces.base_split_step/#class-basesplitstep","text":"Base step implementation for any split step implementation on ZenML","title":"class BaseSplitStep"},{"location":"api_docs/steps.step_interfaces.base_split_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/steps.step_interfaces.base_split_step/#method-entrypoint","text":"entrypoint( dataset: DataArtifact, config: BaseSplitStepConfig, context: StepContext ) \u2192 <Output object at 0x7f4c08f73310> Entrypoint for a function for the split steps to run This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/steps.step_interfaces.base_trainer_step/","text":"module steps.step_interfaces.base_trainer_step class BaseTrainerConfig Base class for Trainer step configurations class BaseTrainerStep Base step implementation for any Trainer step implementation on ZenML property component Returns a TFX component. method entrypoint entrypoint( train_dataset: DataArtifact, validation_dataset: DataArtifact, config: BaseTrainerConfig, context: StepContext ) \u2192 ModelArtifact Base entrypoint for any Trainer implementation This file was automatically generated via lazydocs .","title":"Steps.step interfaces.base trainer step"},{"location":"api_docs/steps.step_interfaces.base_trainer_step/#module-stepsstep_interfacesbase_trainer_step","text":"","title":"module steps.step_interfaces.base_trainer_step"},{"location":"api_docs/steps.step_interfaces.base_trainer_step/#class-basetrainerconfig","text":"Base class for Trainer step configurations","title":"class BaseTrainerConfig"},{"location":"api_docs/steps.step_interfaces.base_trainer_step/#class-basetrainerstep","text":"Base step implementation for any Trainer step implementation on ZenML","title":"class BaseTrainerStep"},{"location":"api_docs/steps.step_interfaces.base_trainer_step/#property-component","text":"Returns a TFX component.","title":"property component"},{"location":"api_docs/steps.step_interfaces.base_trainer_step/#method-entrypoint","text":"entrypoint( train_dataset: DataArtifact, validation_dataset: DataArtifact, config: BaseTrainerConfig, context: StepContext ) \u2192 ModelArtifact Base entrypoint for any Trainer implementation This file was automatically generated via lazydocs .","title":"method entrypoint"},{"location":"api_docs/steps.step_interfaces/","text":"module steps.step_interfaces This file was automatically generated via lazydocs .","title":"Steps.step interfaces"},{"location":"api_docs/steps.step_interfaces/#module-stepsstep_interfaces","text":"This file was automatically generated via lazydocs .","title":"module steps.step_interfaces"},{"location":"api_docs/steps.step_output/","text":"module steps.step_output class Output A named tuple with a default name that cannot be overridden. method __init__ __init__(**kwargs: Type[Any]) method items items() \u2192 Iterator[Tuple[str, Type[Any]]] Yields a tuple of type (output_name, output_type). This file was automatically generated via lazydocs .","title":"Steps.step output"},{"location":"api_docs/steps.step_output/#module-stepsstep_output","text":"","title":"module steps.step_output"},{"location":"api_docs/steps.step_output/#class-output","text":"A named tuple with a default name that cannot be overridden.","title":"class Output"},{"location":"api_docs/steps.step_output/#method-__init__","text":"__init__(**kwargs: Type[Any])","title":"method __init__"},{"location":"api_docs/steps.step_output/#method-items","text":"items() \u2192 Iterator[Tuple[str, Type[Any]]] Yields a tuple of type (output_name, output_type). This file was automatically generated via lazydocs .","title":"method items"},{"location":"api_docs/steps.utils/","text":"module steps.utils The collection of utility functions/classes are inspired by their original implementation of the Tensorflow Extended team, which can be found here: https://github.com/tensorflow/tfx/blob/master/tfx/dsl/component/experimental /decorators.py This version is heavily adjusted to work with the Pipeline-Step paradigm which is proposed by ZenML. Global Variables STEP_INNER_FUNC_NAME SINGLE_RETURN_OUT_NAME PARAM_STEP_NAME PARAM_ENABLE_CACHE PARAM_PIPELINE_PARAMETER_NAME PARAM_CREATED_BY_FUNCTIONAL_API INTERNAL_EXECUTION_PARAMETER_PREFIX INSTANCE_CONFIGURATION OUTPUT_SPEC function do_types_match do_types_match(type_a: Type[Any], type_b: Type[Any]) \u2192 bool Check whether type_a and type_b match. Args: type_a : First Type to check. type_b : Second Type to check. Returns: True if types match, otherwise False. function generate_component_spec_class generate_component_spec_class( step_name: str, input_spec: Dict[str, Type[BaseArtifact]], output_spec: Dict[str, Type[BaseArtifact]], execution_parameter_names: Set[str] ) \u2192 Type[ComponentSpec] Generates a TFX component spec class for a ZenML step. Args: step_name : Name of the step for which the component will be created. input_spec : Input artifacts of the step. output_spec : Output artifacts of the step execution_parameter_names : Execution parameter names of the step. Returns: A TFX component spec class. function generate_component_class generate_component_class( step_name: str, step_module: str, input_spec: Dict[str, Type[BaseArtifact]], output_spec: Dict[str, Type[BaseArtifact]], execution_parameter_names: Set[str], step_function: Callable[, Any], materializers: Dict[str, Type[BaseMaterializer]] ) \u2192 Type[ForwardRef('_ZenMLSimpleComponent')] Generates a TFX component class for a ZenML step. Args: step_name : Name of the step for which the component will be created. step_module : Module in which the step class is defined. input_spec : Input artifacts of the step. output_spec : Output artifacts of the step execution_parameter_names : Execution parameter names of the step. step_function : The actual function to execute when running the step. materializers : Materializer classes for all outputs of the step. Returns: A TFX component class. This file was automatically generated via lazydocs .","title":"Steps.utils"},{"location":"api_docs/steps.utils/#module-stepsutils","text":"The collection of utility functions/classes are inspired by their original implementation of the Tensorflow Extended team, which can be found here: https://github.com/tensorflow/tfx/blob/master/tfx/dsl/component/experimental /decorators.py This version is heavily adjusted to work with the Pipeline-Step paradigm which is proposed by ZenML.","title":"module steps.utils"},{"location":"api_docs/steps.utils/#global-variables","text":"STEP_INNER_FUNC_NAME SINGLE_RETURN_OUT_NAME PARAM_STEP_NAME PARAM_ENABLE_CACHE PARAM_PIPELINE_PARAMETER_NAME PARAM_CREATED_BY_FUNCTIONAL_API INTERNAL_EXECUTION_PARAMETER_PREFIX INSTANCE_CONFIGURATION OUTPUT_SPEC","title":"Global Variables"},{"location":"api_docs/steps.utils/#function-do_types_match","text":"do_types_match(type_a: Type[Any], type_b: Type[Any]) \u2192 bool Check whether type_a and type_b match. Args: type_a : First Type to check. type_b : Second Type to check. Returns: True if types match, otherwise False.","title":"function do_types_match"},{"location":"api_docs/steps.utils/#function-generate_component_spec_class","text":"generate_component_spec_class( step_name: str, input_spec: Dict[str, Type[BaseArtifact]], output_spec: Dict[str, Type[BaseArtifact]], execution_parameter_names: Set[str] ) \u2192 Type[ComponentSpec] Generates a TFX component spec class for a ZenML step. Args: step_name : Name of the step for which the component will be created. input_spec : Input artifacts of the step. output_spec : Output artifacts of the step execution_parameter_names : Execution parameter names of the step. Returns: A TFX component spec class.","title":"function generate_component_spec_class"},{"location":"api_docs/steps.utils/#function-generate_component_class","text":"generate_component_class( step_name: str, step_module: str, input_spec: Dict[str, Type[BaseArtifact]], output_spec: Dict[str, Type[BaseArtifact]], execution_parameter_names: Set[str], step_function: Callable[, Any], materializers: Dict[str, Type[BaseMaterializer]] ) \u2192 Type[ForwardRef('_ZenMLSimpleComponent')] Generates a TFX component class for a ZenML step. Args: step_name : Name of the step for which the component will be created. step_module : Module in which the step class is defined. input_spec : Input artifacts of the step. output_spec : Output artifacts of the step execution_parameter_names : Execution parameter names of the step. step_function : The actual function to execute when running the step. materializers : Materializer classes for all outputs of the step. Returns: A TFX component class. This file was automatically generated via lazydocs .","title":"function generate_component_class"},{"location":"api_docs/utils.analytics_utils/","text":"module utils.analytics_utils Analytics code for ZenML Global Variables IS_DEBUG_ENV SEGMENT_KEY_DEV SEGMENT_KEY_PROD RUN_PIPELINE GET_PIPELINES GET_PIPELINE INITIALIZE_REPO REGISTERED_METADATA_STORE REGISTERED_ARTIFACT_STORE REGISTERED_ORCHESTRATOR REGISTERED_CONTAINER_REGISTRY REGISTERED_STACK SET_STACK OPT_IN_ANALYTICS OPT_OUT_ANALYTICS RUN_EXAMPLE EVENT_TEST function get_segment_key get_segment_key() \u2192 str Get key for authorizing to Segment backend. Returns: Segment key as a string. function in_docker in_docker() \u2192 bool Returns: True if running in a Docker container, else False function in_google_colab in_google_colab() \u2192 bool Returns: True if running in a Google Colab env, else False function in_paperspace_gradient in_paperspace_gradient() \u2192 bool Returns: True if running in a Paperspace Gradient env, else False function get_system_info get_system_info() \u2192 Dict[str, Any] Returns system info as a dict. Returns: A dict of system information. function get_environment get_environment() \u2192 str Returns a string representing the execution environment of the pipeline. Currently, one of docker , paperspace , 'colab', or native function track_event track_event(event: str, metadata: Optional[Dict[str, Any]] = None) \u2192 bool Track segment event if user opted-in. Args: event : Name of event to track in segment. metadata : Dict of metadata to track. Returns: True if event is sent successfully, False is not. function parametrized parametrized( dec: Callable[, Callable[, Any]] ) \u2192 Callable[, Callable[[Callable[, Any]], Callable[, Any]]] This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments: function layer layer(*args: Any, **kwargs: Any) \u2192 Callable[[Callable[, Any]], Callable[, Any]] Internal layer This file was automatically generated via lazydocs .","title":"Utils.analytics utils"},{"location":"api_docs/utils.analytics_utils/#module-utilsanalytics_utils","text":"Analytics code for ZenML","title":"module utils.analytics_utils"},{"location":"api_docs/utils.analytics_utils/#global-variables","text":"IS_DEBUG_ENV SEGMENT_KEY_DEV SEGMENT_KEY_PROD RUN_PIPELINE GET_PIPELINES GET_PIPELINE INITIALIZE_REPO REGISTERED_METADATA_STORE REGISTERED_ARTIFACT_STORE REGISTERED_ORCHESTRATOR REGISTERED_CONTAINER_REGISTRY REGISTERED_STACK SET_STACK OPT_IN_ANALYTICS OPT_OUT_ANALYTICS RUN_EXAMPLE EVENT_TEST","title":"Global Variables"},{"location":"api_docs/utils.analytics_utils/#function-get_segment_key","text":"get_segment_key() \u2192 str Get key for authorizing to Segment backend. Returns: Segment key as a string.","title":"function get_segment_key"},{"location":"api_docs/utils.analytics_utils/#function-in_docker","text":"in_docker() \u2192 bool Returns: True if running in a Docker container, else False","title":"function in_docker"},{"location":"api_docs/utils.analytics_utils/#function-in_google_colab","text":"in_google_colab() \u2192 bool Returns: True if running in a Google Colab env, else False","title":"function in_google_colab"},{"location":"api_docs/utils.analytics_utils/#function-in_paperspace_gradient","text":"in_paperspace_gradient() \u2192 bool Returns: True if running in a Paperspace Gradient env, else False","title":"function in_paperspace_gradient"},{"location":"api_docs/utils.analytics_utils/#function-get_system_info","text":"get_system_info() \u2192 Dict[str, Any] Returns system info as a dict. Returns: A dict of system information.","title":"function get_system_info"},{"location":"api_docs/utils.analytics_utils/#function-get_environment","text":"get_environment() \u2192 str Returns a string representing the execution environment of the pipeline. Currently, one of docker , paperspace , 'colab', or native","title":"function get_environment"},{"location":"api_docs/utils.analytics_utils/#function-track_event","text":"track_event(event: str, metadata: Optional[Dict[str, Any]] = None) \u2192 bool Track segment event if user opted-in. Args: event : Name of event to track in segment. metadata : Dict of metadata to track. Returns: True if event is sent successfully, False is not.","title":"function track_event"},{"location":"api_docs/utils.analytics_utils/#function-parametrized","text":"parametrized( dec: Callable[, Callable[, Any]] ) \u2192 Callable[, Callable[[Callable[, Any]], Callable[, Any]]] This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments:","title":"function parametrized"},{"location":"api_docs/utils.analytics_utils/#function-layer","text":"layer(*args: Any, **kwargs: Any) \u2192 Callable[[Callable[, Any]], Callable[, Any]] Internal layer This file was automatically generated via lazydocs .","title":"function layer"},{"location":"api_docs/utils.daemon/","text":"module utils.daemon Utility functions to start/stop daemon processes. This is only implemented for UNIX systems and therefore doesn't work on Windows. Based on https://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/ function run_as_daemon run_as_daemon( daemon_function: Callable[, Any], pid_file: str, log_file: Optional[str] = None, working_directory: str = '/' ) \u2192 None Runs a function as a daemon process. Args: daemon_function : The function to run as a daemon. pid_file : Path to file in which to store the PID of the daemon process. log_file : Optional file to which the daemons stdout/stderr will be redirected to. working_directory : Working directory for the daemon process, defaults to the root directory. Raises: FileExistsError : If the PID file already exists. function stop_daemon stop_daemon(pid_file: str, kill_children: bool = True) \u2192 None Stops a daemon process. Args: pid_file : Path to file containing the PID of the daemon process to kill. kill_children : If True , all child processes of the daemon process will be killed as well. function check_if_daemon_is_running check_if_daemon_is_running(pid_file: str) \u2192 bool Checks whether a daemon process indicated by the PID file is running. Args: pid_file : Path to file containing the PID of the daemon process to check. This file was automatically generated via lazydocs .","title":"Utils.daemon"},{"location":"api_docs/utils.daemon/#module-utilsdaemon","text":"Utility functions to start/stop daemon processes. This is only implemented for UNIX systems and therefore doesn't work on Windows. Based on https://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/","title":"module utils.daemon"},{"location":"api_docs/utils.daemon/#function-run_as_daemon","text":"run_as_daemon( daemon_function: Callable[, Any], pid_file: str, log_file: Optional[str] = None, working_directory: str = '/' ) \u2192 None Runs a function as a daemon process. Args: daemon_function : The function to run as a daemon. pid_file : Path to file in which to store the PID of the daemon process. log_file : Optional file to which the daemons stdout/stderr will be redirected to. working_directory : Working directory for the daemon process, defaults to the root directory. Raises: FileExistsError : If the PID file already exists.","title":"function run_as_daemon"},{"location":"api_docs/utils.daemon/#function-stop_daemon","text":"stop_daemon(pid_file: str, kill_children: bool = True) \u2192 None Stops a daemon process. Args: pid_file : Path to file containing the PID of the daemon process to kill. kill_children : If True , all child processes of the daemon process will be killed as well.","title":"function stop_daemon"},{"location":"api_docs/utils.daemon/#function-check_if_daemon_is_running","text":"check_if_daemon_is_running(pid_file: str) \u2192 bool Checks whether a daemon process indicated by the PID file is running. Args: pid_file : Path to file containing the PID of the daemon process to check. This file was automatically generated via lazydocs .","title":"function check_if_daemon_is_running"},{"location":"api_docs/utils/","text":"module utils The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions. This file was automatically generated via lazydocs .","title":"Utils"},{"location":"api_docs/utils/#module-utils","text":"The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions. This file was automatically generated via lazydocs .","title":"module utils"},{"location":"api_docs/utils.networking_utils/","text":"module utils.networking_utils function port_available port_available(port: int) \u2192 bool Checks if a local port is available. function find_available_port find_available_port() \u2192 int Finds a local unoccupied port. This file was automatically generated via lazydocs .","title":"Utils.networking utils"},{"location":"api_docs/utils.networking_utils/#module-utilsnetworking_utils","text":"","title":"module utils.networking_utils"},{"location":"api_docs/utils.networking_utils/#function-port_available","text":"port_available(port: int) \u2192 bool Checks if a local port is available.","title":"function port_available"},{"location":"api_docs/utils.networking_utils/#function-find_available_port","text":"find_available_port() \u2192 int Finds a local unoccupied port. This file was automatically generated via lazydocs .","title":"function find_available_port"},{"location":"api_docs/utils.source_utils/","text":"module utils.source_utils These utils are predicated on the following definitions: class_source: This is a python-import type path to a class, e.g. some.mod.class * module_source: This is a python-import type path to a module, e.g. some.mod * file_path, relative_path, absolute_path: These are file system paths. * source: This is a class_source or module_source. If it is a class_source, it can also be optionally pinned. * pin: Whatever comes after the @ symbol from a source, usually the git sha or the version of zenml as a string. Global Variables APP_NAME function is_standard_pin is_standard_pin(pin: str) \u2192 bool Returns True if pin is valid ZenML pin, else False. Args: pin : potential ZenML pin like 'zenml_0.1.1' function is_inside_repository is_inside_repository(file_path: str) \u2192 bool Returns whether a file is inside a zenml repository. function is_third_party_module is_third_party_module(file_path: str) \u2192 bool Returns whether a file belongs to a third party package. function create_zenml_pin create_zenml_pin() \u2192 str Creates a ZenML pin for source pinning from release version. function resolve_standard_source resolve_standard_source(source: str) \u2192 str Creates a ZenML pin for source pinning from release version. Args: source : class_source e.g. this.module.Class. function is_standard_source is_standard_source(source: str) \u2192 bool Returns True if source is a standard ZenML source. Args: source : class_source e.g. this.module.Class[@pin]. function get_class_source_from_source get_class_source_from_source(source: str) \u2192 str Gets class source from source, i.e. module.path@version, returns version. Args: source : source pointing to potentially pinned sha. function get_module_source_from_source get_module_source_from_source(source: str) \u2192 str Gets module source from source. E.g. some.module.file.class@version , returns some.module . Args: source : source pointing to potentially pinned sha. function get_module_source_from_file_path get_module_source_from_file_path(file_path: str) \u2192 str Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py returns myrepo.step.trainer if myrepo is the root of the repo. Args: file_path : Absolute file path to a file within the module. function get_relative_path_from_module_source get_relative_path_from_module_source(module_source: str) \u2192 str Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Args: module_source : A module e.g. zenml.core.step function get_absolute_path_from_module_source get_absolute_path_from_module_source(module: str) \u2192 str Get a directory path from module source. E.g. zenml.core.step will return full/path/to/zenml/core/step . Args: module : A module e.g. zenml.core.step . function get_module_source_from_class get_module_source_from_class( class_: Union[Type[Any], str] ) \u2192 Union[str, NoneType] Takes class input and returns module_source. If class is already string then returns the same. Args: class_ : object of type class. function resolve_class resolve_class(class_: Type[Any]) \u2192 str Resolves a class into a serializable source string. Args: class_ : A Python Class reference. Returns: source_path e.g. this.module.Class. function import_class_by_path import_class_by_path(class_path: str) \u2192 Type[Any] Imports a class based on a given path Args: class_path : str, class_source e.g. this.module.Class Returns: the given class function load_source_path_class load_source_path_class(source: str) \u2192 Type[Any] Loads a Python class from the source. Args: source : class_source e.g. this.module.Class[@sha] function import_python_file import_python_file(file_path: str) \u2192 module Imports a python file. Args: file_path : Path to python file that should be imported. Returns: The imported module. This file was automatically generated via lazydocs .","title":"Utils.source utils"},{"location":"api_docs/utils.source_utils/#module-utilssource_utils","text":"These utils are predicated on the following definitions: class_source: This is a python-import type path to a class, e.g. some.mod.class * module_source: This is a python-import type path to a module, e.g. some.mod * file_path, relative_path, absolute_path: These are file system paths. * source: This is a class_source or module_source. If it is a class_source, it can also be optionally pinned. * pin: Whatever comes after the @ symbol from a source, usually the git sha or the version of zenml as a string.","title":"module utils.source_utils"},{"location":"api_docs/utils.source_utils/#global-variables","text":"APP_NAME","title":"Global Variables"},{"location":"api_docs/utils.source_utils/#function-is_standard_pin","text":"is_standard_pin(pin: str) \u2192 bool Returns True if pin is valid ZenML pin, else False. Args: pin : potential ZenML pin like 'zenml_0.1.1'","title":"function is_standard_pin"},{"location":"api_docs/utils.source_utils/#function-is_inside_repository","text":"is_inside_repository(file_path: str) \u2192 bool Returns whether a file is inside a zenml repository.","title":"function is_inside_repository"},{"location":"api_docs/utils.source_utils/#function-is_third_party_module","text":"is_third_party_module(file_path: str) \u2192 bool Returns whether a file belongs to a third party package.","title":"function is_third_party_module"},{"location":"api_docs/utils.source_utils/#function-create_zenml_pin","text":"create_zenml_pin() \u2192 str Creates a ZenML pin for source pinning from release version.","title":"function create_zenml_pin"},{"location":"api_docs/utils.source_utils/#function-resolve_standard_source","text":"resolve_standard_source(source: str) \u2192 str Creates a ZenML pin for source pinning from release version. Args: source : class_source e.g. this.module.Class.","title":"function resolve_standard_source"},{"location":"api_docs/utils.source_utils/#function-is_standard_source","text":"is_standard_source(source: str) \u2192 bool Returns True if source is a standard ZenML source. Args: source : class_source e.g. this.module.Class[@pin].","title":"function is_standard_source"},{"location":"api_docs/utils.source_utils/#function-get_class_source_from_source","text":"get_class_source_from_source(source: str) \u2192 str Gets class source from source, i.e. module.path@version, returns version. Args: source : source pointing to potentially pinned sha.","title":"function get_class_source_from_source"},{"location":"api_docs/utils.source_utils/#function-get_module_source_from_source","text":"get_module_source_from_source(source: str) \u2192 str Gets module source from source. E.g. some.module.file.class@version , returns some.module . Args: source : source pointing to potentially pinned sha.","title":"function get_module_source_from_source"},{"location":"api_docs/utils.source_utils/#function-get_module_source_from_file_path","text":"get_module_source_from_file_path(file_path: str) \u2192 str Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py returns myrepo.step.trainer if myrepo is the root of the repo. Args: file_path : Absolute file path to a file within the module.","title":"function get_module_source_from_file_path"},{"location":"api_docs/utils.source_utils/#function-get_relative_path_from_module_source","text":"get_relative_path_from_module_source(module_source: str) \u2192 str Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Args: module_source : A module e.g. zenml.core.step","title":"function get_relative_path_from_module_source"},{"location":"api_docs/utils.source_utils/#function-get_absolute_path_from_module_source","text":"get_absolute_path_from_module_source(module: str) \u2192 str Get a directory path from module source. E.g. zenml.core.step will return full/path/to/zenml/core/step . Args: module : A module e.g. zenml.core.step .","title":"function get_absolute_path_from_module_source"},{"location":"api_docs/utils.source_utils/#function-get_module_source_from_class","text":"get_module_source_from_class( class_: Union[Type[Any], str] ) \u2192 Union[str, NoneType] Takes class input and returns module_source. If class is already string then returns the same. Args: class_ : object of type class.","title":"function get_module_source_from_class"},{"location":"api_docs/utils.source_utils/#function-resolve_class","text":"resolve_class(class_: Type[Any]) \u2192 str Resolves a class into a serializable source string. Args: class_ : A Python Class reference. Returns: source_path e.g. this.module.Class.","title":"function resolve_class"},{"location":"api_docs/utils.source_utils/#function-import_class_by_path","text":"import_class_by_path(class_path: str) \u2192 Type[Any] Imports a class based on a given path Args: class_path : str, class_source e.g. this.module.Class Returns: the given class","title":"function import_class_by_path"},{"location":"api_docs/utils.source_utils/#function-load_source_path_class","text":"load_source_path_class(source: str) \u2192 Type[Any] Loads a Python class from the source. Args: source : class_source e.g. this.module.Class[@sha]","title":"function load_source_path_class"},{"location":"api_docs/utils.source_utils/#function-import_python_file","text":"import_python_file(file_path: str) \u2192 module Imports a python file. Args: file_path : Path to python file that should be imported. Returns: The imported module. This file was automatically generated via lazydocs .","title":"function import_python_file"},{"location":"api_docs/utils.string_utils/","text":"module utils.string_utils function get_human_readable_time get_human_readable_time(seconds: float) \u2192 str Convert seconds into a human-readable string. function get_human_readable_filesize get_human_readable_filesize(bytes_: int) \u2192 str Convert a file size in bytes into a human-readable string. This file was automatically generated via lazydocs .","title":"Utils.string utils"},{"location":"api_docs/utils.string_utils/#module-utilsstring_utils","text":"","title":"module utils.string_utils"},{"location":"api_docs/utils.string_utils/#function-get_human_readable_time","text":"get_human_readable_time(seconds: float) \u2192 str Convert seconds into a human-readable string.","title":"function get_human_readable_time"},{"location":"api_docs/utils.string_utils/#function-get_human_readable_filesize","text":"get_human_readable_filesize(bytes_: int) \u2192 str Convert a file size in bytes into a human-readable string. This file was automatically generated via lazydocs .","title":"function get_human_readable_filesize"},{"location":"api_docs/utils.yaml_utils/","text":"module utils.yaml_utils function write_yaml write_yaml(file_path: str, contents: Dict[Any, Any]) \u2192 None Write contents as YAML format to file_path. Args: file_path : Path to YAML file. contents : Contents of YAML file as dict. Raises: FileNotFoundError if directory does not exist. function read_yaml read_yaml(file_path: str) \u2192 Any Read YAML on file path and returns contents as dict. Args: file_path : Path to YAML file. Returns: Contents of the file in a dict. Raises: FileNotFoundError if file does not exist. function is_yaml is_yaml(file_path: str) \u2192 bool Returns True if file_path is YAML, else False Args: file_path : Path to YAML file. Returns: True if is yaml, else False. function write_json write_json(file_path: str, contents: Dict[str, Any]) \u2192 None Write contents as JSON format to file_path. Args: file_path : Path to JSON file. contents : Contents of JSON file as dict. Returns: Contents of the file in a dict. Raises: FileNotFoundError if directory does not exist. function read_json read_json(file_path: str) \u2192 Any Read JSON on file path and returns contents as dict. Args: file_path : Path to JSON file. This file was automatically generated via lazydocs .","title":"Utils.yaml utils"},{"location":"api_docs/utils.yaml_utils/#module-utilsyaml_utils","text":"","title":"module utils.yaml_utils"},{"location":"api_docs/utils.yaml_utils/#function-write_yaml","text":"write_yaml(file_path: str, contents: Dict[Any, Any]) \u2192 None Write contents as YAML format to file_path. Args: file_path : Path to YAML file. contents : Contents of YAML file as dict. Raises: FileNotFoundError if directory does not exist.","title":"function write_yaml"},{"location":"api_docs/utils.yaml_utils/#function-read_yaml","text":"read_yaml(file_path: str) \u2192 Any Read YAML on file path and returns contents as dict. Args: file_path : Path to YAML file. Returns: Contents of the file in a dict. Raises: FileNotFoundError if file does not exist.","title":"function read_yaml"},{"location":"api_docs/utils.yaml_utils/#function-is_yaml","text":"is_yaml(file_path: str) \u2192 bool Returns True if file_path is YAML, else False Args: file_path : Path to YAML file. Returns: True if is yaml, else False.","title":"function is_yaml"},{"location":"api_docs/utils.yaml_utils/#function-write_json","text":"write_json(file_path: str, contents: Dict[str, Any]) \u2192 None Write contents as JSON format to file_path. Args: file_path : Path to JSON file. contents : Contents of JSON file as dict. Returns: Contents of the file in a dict. Raises: FileNotFoundError if directory does not exist.","title":"function write_json"},{"location":"api_docs/utils.yaml_utils/#function-read_json","text":"read_json(file_path: str) \u2192 Any Read JSON on file path and returns contents as dict. Args: file_path : Path to JSON file. This file was automatically generated via lazydocs .","title":"function read_json"},{"location":"api_docs/visualizers.base_pipeline_run_visualizer/","text":"module visualizers.base_pipeline_run_visualizer class BasePipelineRunVisualizer The base implementation of a ZenML Pipeline Run Visualizer. method visualize visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 None Method to visualize pipeline runs. This file was automatically generated via lazydocs .","title":"Visualizers.base pipeline run visualizer"},{"location":"api_docs/visualizers.base_pipeline_run_visualizer/#module-visualizersbase_pipeline_run_visualizer","text":"","title":"module visualizers.base_pipeline_run_visualizer"},{"location":"api_docs/visualizers.base_pipeline_run_visualizer/#class-basepipelinerunvisualizer","text":"The base implementation of a ZenML Pipeline Run Visualizer.","title":"class BasePipelineRunVisualizer"},{"location":"api_docs/visualizers.base_pipeline_run_visualizer/#method-visualize","text":"visualize(object: PipelineRunView, *args: Any, **kwargs: Any) \u2192 None Method to visualize pipeline runs. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"api_docs/visualizers.base_pipeline_visualizer/","text":"module visualizers.base_pipeline_visualizer class BasePipelineVisualizer The base implementation of a ZenML Pipeline Visualizer. method visualize visualize(object: PipelineView, *args: Any, **kwargs: Any) \u2192 Any Method to visualize pipelines. This file was automatically generated via lazydocs .","title":"Visualizers.base pipeline visualizer"},{"location":"api_docs/visualizers.base_pipeline_visualizer/#module-visualizersbase_pipeline_visualizer","text":"","title":"module visualizers.base_pipeline_visualizer"},{"location":"api_docs/visualizers.base_pipeline_visualizer/#class-basepipelinevisualizer","text":"The base implementation of a ZenML Pipeline Visualizer.","title":"class BasePipelineVisualizer"},{"location":"api_docs/visualizers.base_pipeline_visualizer/#method-visualize","text":"visualize(object: PipelineView, *args: Any, **kwargs: Any) \u2192 Any Method to visualize pipelines. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"api_docs/visualizers.base_step_visualizer/","text":"module visualizers.base_step_visualizer class BaseStepVisualizer The base implementation of a ZenML Step Visualizer. method running_in_notebook running_in_notebook() \u2192 bool Detect whether we're running in a Jupyter notebook or not method visualize visualize(object: StepView, *args: Any, **kwargs: Any) \u2192 Any Method to visualize steps. This file was automatically generated via lazydocs .","title":"Visualizers.base step visualizer"},{"location":"api_docs/visualizers.base_step_visualizer/#module-visualizersbase_step_visualizer","text":"","title":"module visualizers.base_step_visualizer"},{"location":"api_docs/visualizers.base_step_visualizer/#class-basestepvisualizer","text":"The base implementation of a ZenML Step Visualizer.","title":"class BaseStepVisualizer"},{"location":"api_docs/visualizers.base_step_visualizer/#method-running_in_notebook","text":"running_in_notebook() \u2192 bool Detect whether we're running in a Jupyter notebook or not","title":"method running_in_notebook"},{"location":"api_docs/visualizers.base_step_visualizer/#method-visualize","text":"visualize(object: StepView, *args: Any, **kwargs: Any) \u2192 Any Method to visualize steps. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"api_docs/visualizers.base_visualizer/","text":"module visualizers.base_visualizer class BaseVisualizer Base class for all ZenML Visualizers. method visualize visualize(object: Any, *args: Any, **kwargs: Any) \u2192 None Method to visualize objects. This file was automatically generated via lazydocs .","title":"Visualizers.base visualizer"},{"location":"api_docs/visualizers.base_visualizer/#module-visualizersbase_visualizer","text":"","title":"module visualizers.base_visualizer"},{"location":"api_docs/visualizers.base_visualizer/#class-basevisualizer","text":"Base class for all ZenML Visualizers.","title":"class BaseVisualizer"},{"location":"api_docs/visualizers.base_visualizer/#method-visualize","text":"visualize(object: Any, *args: Any, **kwargs: Any) \u2192 None Method to visualize objects. This file was automatically generated via lazydocs .","title":"method visualize"},{"location":"api_docs/visualizers/","text":"module visualizers The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves. This file was automatically generated via lazydocs .","title":"Visualizers"},{"location":"api_docs/visualizers/#module-visualizers","text":"The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves. This file was automatically generated via lazydocs .","title":"module visualizers"}]}