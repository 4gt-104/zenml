{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zenml.repo import Repository\n",
    "from zenml.datasources import CSVDatasource\n",
    "from zenml.pipelines import TrainingPipeline\n",
    "from zenml.steps.evaluator import TFMAEvaluator\n",
    "from zenml.steps.preprocesser import StandardPreprocesser\n",
    "from zenml.steps.split import RandomSplit\n",
    "from zenml.steps.trainer import TFFeedForwardTrainer\n",
    "from zenml.repo import Repository, ArtifactStore\n",
    "from zenml.utils.naming_utils import transformed_label_name\n",
    "from zenml.steps.deployer import GCAIPDeployer\n",
    "from zenml.steps.deployer import CortexDeployer\n",
    "from examples.cortex.predictor.tf import TensorFlowPredictor\n",
    "from zenml.backends.orchestrator import OrchestratorGCPBackend\n",
    "from zenml.metadata import MySQLMetadataStore\n",
    "from zenml.backends.processing import ProcessingDataFlowBackend\n",
    "from zenml.backends.training import SingleGPUTrainingGCAIPBackend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_BUCKET=os.getenv('GCP_BUCKET')\n",
    "GCP_PROJECT=os.getenv('GCP_PROJECT')\n",
    "GCP_REGION=os.getenv('GCP_REGION')\n",
    "GCP_CLOUD_SQL_INSTANCE_NAME=os.getenv('GCP_CLOUD_SQL_INSTANCE_NAME')\n",
    "MODEL_NAME=os.getenv('MODEL_NAME')\n",
    "CORTEX_ENV=os.getenv('CORTEX_ENV')\n",
    "MYSQL_DB=os.getenv('MYSQL_DB')\n",
    "MYSQL_USER=os.getenv('MYSQL_USER')\n",
    "MYSQL_PWD=os.getenv('MYSQL_PWD')\n",
    "MYSQL_PORT=os.getenv('MYSQL_PORT')\n",
    "MYSQL_HOST=os.getenv('MYSQL_HOST')\n",
    "CONNECTION_NAME = f'{GCP_PROJECT}:{GCP_REGION}:{GCP_CLOUD_SQL_INSTANCE_NAME}'\n",
    "TRAINING_JOB_DIR = os.path.join(GCP_BUCKET, 'gcp_gcaip_training/staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_BUCKET='gs://zenmlartifactstore'  # to be used as the artifact store\n",
    "GCP_PROJECT ='core-engine'\n",
    "GCP_REGION='europe-west1';\n",
    "GCP_CLOUD_SQL_INSTANCE_NAME='mlmetadata'\n",
    "MODEL_NAME='demomodel2'\n",
    "CORTEX_ENV = 'gcp'\n",
    "MYSQL_DB='vm_orchestrated'\n",
    "MYSQL_USER='mlmetadata'\n",
    "MYSQL_PWD='JjIwd3u7JbtveBgu'\n",
    "MYSQL_PORT=3306\n",
    "MYSQL_HOST='127.0.0.1'\n",
    "CONNECTION_NAME = f'{GCP_PROJECT}:{GCP_REGION}:{GCP_CLOUD_SQL_INSTANCE_NAME}'\n",
    "TRAINING_JOB_DIR = os.path.join(GCP_BUCKET, 'gcp_gcaip_training/staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo: Repository = Repository.get_instance()\n",
    "    \n",
    "artifact_store = ArtifactStore(os.path.join(GCP_BUCKET, 'all_feature_demo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create first pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:46:30,928 — zenml.pipelines.base_pipeline — INFO — Pipeline Experiment 1 created.\n"
     ]
    }
   ],
   "source": [
    "training_pipeline = TrainingPipeline(name='Experiment 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a datasource. This will automatically track and version it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:46:32,371 — zenml.datasources.base_datasource — INFO — Datasource Pima Indians Diabetes created.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ds = CSVDatasource(name='Pima Indians Diabetes', path='gs://zenml_quickstart/diabetes.csv')\n",
    "except:\n",
    "    repo: Repository = Repository.get_instance()\n",
    "    ds = repo.get_datasource_by_name('Pima Indians Diabetes')\n",
    "training_pipeline.add_datasource(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a split step to partition data into train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline.add_split(RandomSplit(split_map={'train': 0.7, 'eval': 0.2, 'test':0.1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a preprocessing step to transform data to be ML-capable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline.add_preprocesser(\n",
    "    StandardPreprocesser(\n",
    "        features=['times_pregnant', 'pgc', 'dbp', 'tst', 'insulin', 'bmi',\n",
    "                  'pedigree', 'age'],\n",
    "        labels=['has_diabetes'],\n",
    "        overwrite={'has_diabetes': {\n",
    "            'transform': [{'method': 'no_transform', 'parameters': {}}]}}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a trainer which defines model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline.add_trainer(TFFeedForwardTrainer(\n",
    "    loss='binary_crossentropy',\n",
    "    last_activation='sigmoid',\n",
    "    output_units=1,\n",
    "    metrics=['accuracy'],\n",
    "    epochs=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add an evaluator to calculate slicing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline.add_evaluator(\n",
    "    TFMAEvaluator(slices=[['has_diabetes']],\n",
    "                  metrics={transformed_label_name('has_diabetes'):\n",
    "                     ['binary_crossentropy', 'binary_accuracy']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:46:38,771 — zenml.pipelines.training_pipeline — INFO — Datasource Pima Indians Diabetes has no commits. Creating the first one..\n",
      "2021-04-27 10:46:38,774 — zenml.pipelines.base_pipeline — INFO — Pipeline 1619513198774 created.\n",
      "2021-04-27 10:46:38,821 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataGen is running.\n",
      "2021-04-27 10:46:39,401 — zenml.datasources.csv_datasource — INFO — Matched 1: ['gs://zenml_quickstart/diabetes.csv']\n",
      "2021-04-27 10:46:39,406 — zenml.datasources.csv_datasource — INFO — Using header from file: gs://zenml_quickstart/diabetes.csv.\n",
      "2021-04-27 10:46:39,797 — zenml.datasources.csv_datasource — INFO — Header: ['times_pregnant', 'pgc', 'dbp', 'tst', 'insulin', 'bmi', 'pedigree', 'age', 'has_diabetes'].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:46:42,833 — apache_beam.internal.gcp.auth — WARNING — Unable to find default credentials to use: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.\n",
      "Connecting anonymously.\n",
      "2021-04-27 10:46:43,277 — apache_beam.runners.interactive.interactive_environment — WARNING — Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:46:45,237 — apache_beam.io.tfrecordio — WARNING — Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:46:45,473 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataGen is finished.\n",
      "2021-04-27 10:46:45,481 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataStatistics is running.\n",
      "2021-04-27 10:46:46,799 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataStatistics is finished.\n",
      "2021-04-27 10:46:46,802 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataSchema is running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:46:46,843 — tensorflow — WARNING — From /home/hamza/.virtualenvs/zenml_main/lib/python3.6/site-packages/tensorflow_data_validation/utils/stats_util.py:247: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:46:46,857 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataSchema is finished.\n",
      "2021-04-27 10:46:46,992 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component ImporterNode.DataGen is running.\n",
      "2021-04-27 10:46:47,040 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component ImporterNode.DataGen is finished.\n",
      "2021-04-27 10:46:47,042 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component ImporterNode.DataSchema is running.\n",
      "2021-04-27 10:46:47,073 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component ImporterNode.DataSchema is finished.\n",
      "2021-04-27 10:46:47,075 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component ImporterNode.DataStatistics is running.\n",
      "2021-04-27 10:46:47,100 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component ImporterNode.DataStatistics is finished.\n",
      "2021-04-27 10:46:47,101 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitGen is running.\n",
      "2021-04-27 10:46:48,365 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitGen is finished.\n",
      "2021-04-27 10:46:48,367 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitStatistics is running.\n",
      "2021-04-27 10:46:51,516 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitStatistics is finished.\n",
      "2021-04-27 10:46:51,518 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitSchema is running.\n",
      "2021-04-27 10:46:51,566 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitSchema is finished.\n",
      "2021-04-27 10:46:51,568 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component Transform is running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:46:51,618 — tensorflow — WARNING — From /home/hamza/.virtualenvs/zenml_main/lib/python3.6/site-packages/tfx/components/transform/executor.py:541: Schema (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Schema is a deprecated, use schema_utils.schema_from_feature_spec to create a `Schema`\n",
      "2021-04-27 10:46:51,872 — tensorflow — WARNING — From /home/hamza/.virtualenvs/zenml_main/lib/python3.6/site-packages/tensorflow_transform/tf_utils.py:261: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "2021-04-27 10:46:52,444 — tensorflow — WARNING — TFT beam APIs accept both the TFXIO format and the instance dict format now. There is no need to set use_tfxio any more and it will be removed soon.\n",
      "2021-04-27 10:46:52,470 — root — WARNING — This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
      "2021-04-27 10:46:52,964 — root — WARNING — This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
      "2021-04-27 10:46:53,015 — tensorflow — WARNING — Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "2021-04-27 10:46:53,385 — tensorflow — WARNING — From /home/hamza/.virtualenvs/zenml_main/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "2021-04-27 10:46:53,411 — tensorflow — WARNING — Issue encountered when serializing tft_analyzer_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n",
      "2021-04-27 10:46:53,411 — tensorflow — WARNING — Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n",
      "2021-04-27 10:46:54,067 — tensorflow — WARNING — Issue encountered when serializing tft_analyzer_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n",
      "2021-04-27 10:46:54,068 — tensorflow — WARNING — Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n",
      "2021-04-27 10:46:54,671 — tensorflow — WARNING — Issue encountered when serializing tft_analyzer_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n",
      "2021-04-27 10:46:54,671 — tensorflow — WARNING — Issue encountered when serializing tft_mapper_use.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Counter' object has no attribute 'name'\n",
      "2021-04-27 10:46:55,636 — tensorflow — WARNING — Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "2021-04-27 10:46:55,674 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,674 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,675 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,676 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,678 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,678 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,720 — tensorflow — WARNING — Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "2021-04-27 10:46:55,753 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,754 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,754 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,755 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,757 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,757 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,825 — tensorflow — WARNING — Tensorflow version (2.3.2) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "2021-04-27 10:46:55,882 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,883 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,885 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,886 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,888 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-04-27 10:46:55,889 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:47:05,610 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component Transform is finished.\n",
      "2021-04-27 10:47:05,611 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component Trainer is running.\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "age_xf (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bmi_xf (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dbp_xf (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "insulin_xf (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pedigree_xf (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pgc_xf (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "times_pregnant_xf (InputLayer)  [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tst_xf (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8)            0           age_xf[0][0]                     \n",
      "                                                                 bmi_xf[0][0]                     \n",
      "                                                                 dbp_xf[0][0]                     \n",
      "                                                                 insulin_xf[0][0]                 \n",
      "                                                                 pedigree_xf[0][0]                \n",
      "                                                                 pgc_xf[0][0]                     \n",
      "                                                                 times_pregnant_xf[0][0]          \n",
      "                                                                 tst_xf[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           90          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 10)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "has_diabetes_xl (Dense)         (None, 1)            11          dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/.virtualenvs/zenml_main/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:543: UserWarning: Input dict contained keys ['age', 'bmi', 'dbp', 'has_diabetes', 'insulin', 'pedigree', 'pgc', 'times_pregnant', 'tst'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 0s 137us/step - loss: 0.5849 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:47:06,569 — tensorflow — WARNING — From /home/hamza/.virtualenvs/zenml_main/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "2021-04-27 10:47:06,583 — tensorflow — WARNING — Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0145s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================]0.6183 - accuracy: 0.6481  - 0s 2ms/step - loss: 0.6151 - accuracy: 0.676 - 0s 5ms/step - loss: 0.6124 - accuracy: 0.6797 - val_loss: 0.6636 - val_accuracy: 0.6726\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4887 - accuracy: 0.62 - ETA: 0s - loss: 0.5813 - accuracy: 0.71 - 0s 2ms/step - loss: 0.5849 - accuracy: 0.7090 - val_loss: 0.6420 - val_accuracy: 0.7083\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.4069 - accuracy: 1.00 - ETA: 0s - loss: 0.5569 - accuracy: 0.73 - 0s 3ms/step - loss: 0.5682 - accuracy: 0.7207 - val_loss: 0.6189 - val_accuracy: 0.7202\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.5433 - accuracy: 0.62 - ETA: 0s - loss: 0.5259 - accuracy: 0.76 - 0s 3ms/step - loss: 0.5430 - accuracy: 0.7461 - val_loss: 0.5854 - val_accuracy: 0.7202\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - ETA: 0s - loss: 0.6116 - accuracy: 0.75 - ETA: 0s - loss: 0.5374 - accuracy: 0.74 - 0s 3ms/step - loss: 0.5268 - accuracy: 0.7520 - val_loss: 0.5958 - val_accuracy: 0.7024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/.virtualenvs/zenml_main/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:543: UserWarning: Input dict contained keys ['insulin', 'pedigree', 'has_diabetes', 'times_pregnant', 'pgc', 'age', 'dbp', 'has_diabetes_xl', 'bmi', 'tst'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n",
      "/home/hamza/.virtualenvs/zenml_main/lib/python3.6/site-packages/tensorflow/python/keras/engine/functional.py:543: UserWarning: Input dict contained keys ['age', 'bmi', 'dbp', 'has_diabetes', 'has_diabetes_xl', 'insulin', 'pedigree', 'pgc', 'times_pregnant', 'tst'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n",
      "2021-04-27 10:47:11,246 — tensorflow — WARNING — From /home/hamza/.virtualenvs/zenml_main/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "2021-04-27 10:47:11,265 — tensorflow — WARNING — From /home/hamza/.virtualenvs/zenml_main/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-27 10:47:11,798 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component Trainer is finished.\n",
      "2021-04-27 10:47:11,802 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component Evaluator is running.\n",
      "2021-04-27 10:47:16,256 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component Evaluator is finished.\n"
     ]
    }
   ],
   "source": [
    "training_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline.view_statistics(magic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_pipeline.evaluate(magic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasources = repo.get_datasources()\n",
    "datasource = datasources[0]\n",
    "print(datasource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasource.sample_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip preprocessing with your next (warm-starting) pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone first experiment and only change one hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_2 = training_pipeline.copy('Experiment 2')\n",
    "training_pipeline_2.add_trainer(TFFeedForwardTrainer(\n",
    "    loss='binary_crossentropy',\n",
    "    last_activation='sigmoid',\n",
    "    output_units=1,\n",
    "    metrics=['accuracy'],\n",
    "    epochs=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline_2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_2.evaluate(magic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify theres still only one datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasources = repo.get_datasources()\n",
    "print(f\"We have {len(datasources)} datasources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.compare_training_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily train on the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_3 = training_pipeline.copy('Experiment 3')\n",
    "\n",
    "# Add a trainer with a GCAIP backend\n",
    "training_backend = SingleGPUTrainingGCAIPBackend(\n",
    "    project=GCP_PROJECT,\n",
    "    job_dir=TRAINING_JOB_DIR\n",
    ")\n",
    "\n",
    "training_pipeline.add_trainer(TFFeedForwardTrainer(\n",
    "    loss='binary_crossentropy',\n",
    "    last_activation='sigmoid',\n",
    "    output_units=1,\n",
    "    metrics=['accuracy'],\n",
    "    epochs=20).with_backend(training_backend))\n",
    "\n",
    "training_pipeline_3.run(artifact_store=artifact_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrate every step on the Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline_4 = training_pipeline.copy('Experiment 4')\n",
    "\n",
    "# Define the metadata store\n",
    "metadata_store = MySQLMetadataStore(\n",
    "    host=MYSQL_HOST,\n",
    "    port=int(MYSQL_PORT),\n",
    "    database=MYSQL_DB,\n",
    "    username=MYSQL_USER,\n",
    "    password=MYSQL_PWD,\n",
    ")\n",
    "\n",
    "\n",
    "# Define the orchestrator backend\n",
    "orchestrator_backend = OrchestratorGCPBackend(\n",
    "    cloudsql_connection_name=CONNECTION_NAME,\n",
    "    project=GCP_PROJECT,\n",
    "    preemptible=True,  # reduce costs by using preemptible instances\n",
    "    machine_type='n1-standard-4',\n",
    "    gpu='nvidia-tesla-k80',\n",
    "    gpu_count=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Run the pipeline\n",
    "training_pipeline_4.run(\n",
    "    backend=orchestrator_backend,\n",
    "    metadata_store=metadata_store,\n",
    "    artifact_store=artifact_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a deployer step with different integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Deploy to Google Cloud AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline_5 = training_pipeline.copy('Experiment 5')\n",
    "training_pipeline_5.add_deployment(\n",
    "    GCAIPDeployer(\n",
    "        project_id=GCP_PROJECT,\n",
    "        model_name=MODEL_NAME,\n",
    "    )\n",
    ")\n",
    "\n",
    "training_pipeline_5.run(artifact_store=artifact_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Deploy to Kubernetes via Cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline = repo.get_pipeline_by_name('Experiment 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline_6 = training_pipeline.copy('Experiment 7')\n",
    "\n",
    "# Add cortex deployer\n",
    "api_config = {\n",
    "    \"name\": MODEL_NAME,\n",
    "    \"kind\": \"RealtimeAPI\",\n",
    "    \"predictor\": {\n",
    "        \"type\": \"tensorflow\",\n",
    "        \"models\": {\"signature_key\": \"serving_default\"}}\n",
    "}\n",
    "training_pipeline.add_deployment(\n",
    "    CortexDeployer(\n",
    "        env=CORTEX_ENV,\n",
    "        api_config=api_config,\n",
    "        predictor=TensorFlowPredictor,\n",
    "    )\n",
    ")\n",
    "\n",
    "training_pipeline_6.run(artifact_store=artifact_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
