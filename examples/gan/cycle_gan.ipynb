{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to style images using CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from preprocessing.preprocessor import GANPreprocessor\n",
    "from trainer.trainer_step import CycleGANTrainer\n",
    "from zenml.datasources import ImageDatasource\n",
    "from zenml.pipelines import TrainingPipeline\n",
    "from zenml.repo import Repository\n",
    "from zenml.steps.split import CategoricalDomainSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we instantiate our current ZenML repository. That way, if you run this notebook\n",
    "multiple times in succession, you will not run into trouble with errors on datasource\n",
    "creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Before running this, initialize the repository by calling `zenml init`\n",
    "repo = Repository.get_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the pipeline\n",
    "\n",
    "Now we create the training pipeline that we will run our experiment with. We enable\n",
    "caching by default, as it saves computation time and resources by saving results that do not have to\n",
    "be recomputed over and over again, such as splits or preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:31,532 — zenml.pipelines.base_pipeline — INFO — Pipeline gan_test created.\n"
     ]
    }
   ],
   "source": [
    "gan_pipeline = TrainingPipeline(name=\"gan_test\", enable_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the datasource\n",
    "\n",
    "Next, we create the image datasource for the GAN. If you downloaded the data locally, and you want to run your pipeline\n",
    "with the local copy of the data on your machine, run the `gan_images_ce.py` script located in your images folder\n",
    "before executing the next cell. It will create a label file for use in your ZenML image source.\n",
    "\n",
    "**Tip**: If you want to try out the GAN pipeline training on the full data set, you can set\n",
    "`base_path=\"gs://zenml_quickstart/cycle_gan\"` in the try-block in the cell right below to work with the full dataset.\n",
    "For this demo, we focus on quick pipeline execution, so we do not use the data to its full extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:32,704 — zenml.datasources.base_datasource — INFO — Datasource gan_images created.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    base_path = \"gs://zenml_quickstart/cycle_gan_mini\"\n",
    "    ds = ImageDatasource(name=\"gan_images\", base_path=base_path)\n",
    "except:\n",
    "    ds = repo.get_datasource_by_name('gan_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the datasource to the pipeline\n",
    "\n",
    "The datasource is the first step in the pipeline, because it supplies the data on\n",
    "which the model will be trained. You can add it to the pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gan_pipeline.add_datasource(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a split\n",
    "\n",
    "To prepare our images, we need to separate the real images from the styled ones (Monet\n",
    "paintings in this case). Therefore, we add the image type (real or Monet) as a label\n",
    "and split on that. The real images are going to be saved as evaluation data, because we\n",
    "will use them later in image style generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gan_pipeline.add_split(CategoricalDomainSplit(categorical_column=\"label\",\n",
    "                                              split_map={\"train\": [\"monet\"],\n",
    "                                                         \"eval\": [\"real\"]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a preprocessing step\n",
    "\n",
    "Next, we add a small preprocessing step for our image. It is defined in the\n",
    "`preprocessing.py` file in this folder, and contains two separate steps: Loading the\n",
    "image (up to this point, it is persisted as a binary string) and normalizing its values\n",
    " between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gan_pipeline.add_preprocesser(GANPreprocessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a training step\n",
    "\n",
    "Now we come to what is the most involved part of this example - a custom trainer step.\n",
    "It is defined in the `gan_functions.py` file in the `trainer` subfolder, and contains a Keras\n",
    "implementation of the CycleGAN model architecture. Along with this, it also comes with\n",
    "utilities for loading data, preparing it as `tf.data.Dataset`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:35,594 — zenml.repo.git_wrapper — WARNING — Found uncommitted file. Pipelines run with this configuration may not be reproducible. Please commit all files in this module and then run the pipeline to ensure reproducibility.\n"
     ]
    }
   ],
   "source": [
    "gan_pipeline.add_trainer(CycleGANTrainer(epochs=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your pipeline\n",
    "Now that all the required steps are in place, it is time to run the newly created\n",
    "pipeline.\n",
    "\n",
    "### OPTIONAL: Running the pipeline on Google Cloud Platform\n",
    "\n",
    "If you want to run the pipeline on GCP instead, please skip the next cell and head\n",
    "straight to the section afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:36,587 — zenml.repo.git_wrapper — WARNING — Found uncommitted file. Pipelines run with this configuration may not be reproducible. Please commit all files in this module and then run the pipeline to ensure reproducibility.\n",
      "2021-03-17 13:57:36,592 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataGen is running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:36,894 — apache_beam.runners.interactive.interactive_environment — WARNING — Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:40,249 — apache_beam.internal.gcp.auth — WARNING — Unable to find default credentials to use: The Application Default Credentials are not available. They are available if running in Google Compute Engine. Otherwise, the environment variable GOOGLE_APPLICATION_CREDENTIALS must be defined pointing to a file defining the credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information.\n",
      "Connecting anonymously.\n",
      "2021-03-17 13:57:48,814 — apache_beam.io.tfrecordio — WARNING — Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:49,018 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataGen is finished.\n",
      "2021-03-17 13:57:49,020 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataStatistics is running.\n",
      "2021-03-17 13:57:50,215 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataStatistics is finished.\n",
      "2021-03-17 13:57:50,222 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataSchema is running.\n",
      "WARNING:tensorflow:From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow_data_validation/utils/stats_util.py:247: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:50,270 — tensorflow — WARNING — From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow_data_validation/utils/stats_util.py:247: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:50,291 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component DataSchema is finished.\n",
      "2021-03-17 13:57:50,293 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitGen is running.\n",
      "2021-03-17 13:57:51,108 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitGen is finished.\n",
      "2021-03-17 13:57:51,112 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitStatistics is running.\n",
      "2021-03-17 13:57:53,708 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitStatistics is finished.\n",
      "2021-03-17 13:57:53,711 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitSchema is running.\n",
      "2021-03-17 13:57:53,777 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component SplitSchema is finished.\n",
      "2021-03-17 13:57:53,778 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component Transform is running.\n",
      "WARNING:tensorflow:From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tfx/components/transform/executor.py:541: Schema (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Schema is a deprecated, use schema_utils.schema_from_feature_spec to create a `Schema`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:53,844 — tensorflow — WARNING — From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tfx/components/transform/executor.py:541: Schema (from tensorflow_transform.tf_metadata.dataset_schema) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Schema is a deprecated, use schema_utils.schema_from_feature_spec to create a `Schema`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:54,159 — tensorflow — WARNING — From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow_transform/tf_utils.py:261: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:54,490 — tensorflow — WARNING — From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow_transform/tf_utils.py:261: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TFT beam APIs accept both the TFXIO format and the instance dict format now. There is no need to set use_tfxio any more and it will be removed soon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:55,226 — tensorflow — WARNING — TFT beam APIs accept both the TFXIO format and the instance dict format now. There is no need to set use_tfxio any more and it will be removed soon.\n",
      "2021-03-17 13:57:55,280 — root — WARNING — This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
      "2021-03-17 13:57:56,038 — root — WARNING — This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:56,097 — tensorflow — WARNING — Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:56,484 — tensorflow — WARNING — From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:56,489 — tensorflow — INFO — Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:56,490 — tensorflow — INFO — No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: /home/hamza/workspace/maiot/github_temp/zenml/.zenml/local_store/c6f55900490949ef8d48b99eb7514ead/Transform/transform_graph/119/.temp_path/tftransform_tmp/f7d6cca9ef814824b643034cf15e976f/saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:56,511 — tensorflow — INFO — SavedModel written to: /home/hamza/workspace/maiot/github_temp/zenml/.zenml/local_store/c6f55900490949ef8d48b99eb7514ead/Transform/transform_graph/119/.temp_path/tftransform_tmp/f7d6cca9ef814824b643034cf15e976f/saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:56,719 — tensorflow — WARNING — Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "2021-03-17 13:57:56,767 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-03-17 13:57:56,768 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-03-17 13:57:56,769 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-03-17 13:57:56,770 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-03-17 13:57:56,772 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-03-17 13:57:56,773 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:56,832 — tensorflow — WARNING — Tensorflow version (2.3.1) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
      "2021-03-17 13:57:56,890 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-03-17 13:57:56,891 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-03-17 13:57:56,892 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-03-17 13:57:56,893 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n",
      "2021-03-17 13:57:56,898 — apache_beam.typehints.typehints — WARNING — Ignoring send_type hint: <class 'NoneType'>\n",
      "2021-03-17 13:57:56,899 — apache_beam.typehints.typehints — WARNING — Ignoring return_type hint: <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:57,664 — tensorflow — INFO — Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:57,799 — tensorflow — INFO — Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:57:59,735 — tensorflow — INFO — Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:58:00,712 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component Transform is finished.\n",
      "2021-03-17 13:58:00,713 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component Trainer is running.\n",
      "2021-03-17 13:58:00,956 — zenml.repo.git_wrapper — WARNING — Found uncommitted file. Pipelines run with this configuration may not be reproducible. Please commit all files in this module and then run the pipeline to ensure reproducibility.\n",
      "      1/Unknown - 0s 202us/step - monet_gen_loss: 12.7243 - photo_gen_loss: 12.3397 - monet_disc_loss: 0.6931 - photo_disc_loss: 0.6934WARNING:tensorflow:From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:58:53,908 — tensorflow — WARNING — From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================]_gen_loss: 12.1607 - photo_gen_loss: 12.3090 - monet_disc_loss: 0.6929 - photo_disc_loss: 0.6927  - 4s 2s/step - monet_gen_loss: 11.9728 - photo_gen_loss: 12.2987 - monet_disc_loss: 0.6928 - photo_disc_loss: 0.6925\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:58:54,541 — tensorflow — INFO — Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:59:05,219 — tensorflow — WARNING — From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:59:05,232 — tensorflow — WARNING — From /home/hamza/.virtualenvs/ce_opensource/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/hamza/workspace/maiot/github_temp/zenml/.zenml/local_store/c6f55900490949ef8d48b99eb7514ead/Trainer/model/120/serving_model_dir/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:59:22,485 — tensorflow — INFO — Assets written to: /home/hamza/workspace/maiot/github_temp/zenml/.zenml/local_store/c6f55900490949ef8d48b99eb7514ead/Trainer/model/120/serving_model_dir/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:59:23,375 — zenml.backends.orchestrator.base.zenml_local_orchestrator — INFO — Component Trainer is finished.\n"
     ]
    }
   ],
   "source": [
    "gan_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And that is it! If you studied the contents of the `gan_functions.py` file, you will\n",
    "notice that there's also a custom callback function implemented. It uses TensorBoard\n",
    "to display the same single, Monet-styled image generated by the CycleGAN after each\n",
    "epoch of learning. There is a neat little slider that lets you look through all logged\n",
    "epochs - that way, you can visualize the CycleGAN's style learning process. Let's try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Inspect your model's style evolution on TensorBoard\n",
    "\n",
    "Now we launch a TensorBoard extension to inspect our model's training progress. With our default configuration, we\n",
    "logged four different loss values, which measure the abilities of the sub–networks (Monet/real photo generators and\n",
    "discriminators) to create realistic data and discriminate real from artificial images, respectively.\n",
    "\n",
    "On top of that, we added a custom callback that logs a single image generated by the Monet Generator network from a\n",
    "real photo. From that, we are able to see how the style changes throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Executing this cell should spawn another cell below.\n",
    "# #Running that will add a TensorBoard dashboard.\n",
    "\n",
    "from zenml.utils.post_training.post_training_utils import get_tensorboard_block, create_new_cell\n",
    "from zenml.enums import GDPComponent\n",
    "\n",
    "tensorboard_root = gan_pipeline.get_artifacts_uri_by_component(GDPComponent.Trainer.name)[0]\n",
    "\n",
    "create_new_cell(get_tensorboard_block(tensorboard_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b3f2a2a20d906ea7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b3f2a2a20d906ea7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = '\"/home/hamza/workspace/maiot/github_temp/zenml/.zenml/local_store/c6f55900490949ef8d48b99eb7514ead/Trainer/model/120\"'\n",
    "logdir = os.path.join(model_path, 'logs')\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should see the TensorBoard dashboard initialized above. Click on the \"image\" tab in the header to see how\n",
    "the image evolves during training. As a next step if you are interested, you can increase the training time or tweak\n",
    "some of the hyperparameters (learning rates, regularization) to see how the style changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## OPTIONAL: Running a training pipeline on Google Cloud Platform\n",
    "\n",
    "ZenML also enables you to run your local configuration from above in the cloud\n",
    "with the help of cloud-based orchestrator backend. This section will give you\n",
    "an example of how to run a pipeline in the cloud.\n",
    "\n",
    "First, we import all the additional necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from zenml.backends.orchestrator import OrchestratorGCPBackend\n",
    "from zenml.metadata import MySQLMetadataStore\n",
    "from zenml.repo import ArtifactStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting your cloud variables\n",
    "\n",
    "In order for the cloud discovery to work, you will have to define some variables.\n",
    "Among those are the CloudSQL server and artifact store locations, your local\n",
    "metadata store, and your Google Cloud Project.\n",
    "\n",
    "Please customize these variables below to match your own cloud settings and desired\n",
    "configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# location of your cloud artifact store\n",
    "artifact_store = 'gs://your-bucket-name/optional-subfolder'\n",
    "\n",
    "# the GCP project to launch the VM in\n",
    "project = 'PROJECT'\n",
    "# the zone to launch the VM in\n",
    "zone = 'europe-west1-b'\n",
    "\n",
    "# your CloudSQL server configuration\n",
    "cloudsql_connection_name = 'PROJECT:REGION:INSTANCE'\n",
    "\n",
    "# your local metadata store setup\n",
    "mysql_db = 'DATABASE'\n",
    "mysql_user = 'USERNAME'\n",
    "mysql_pw = 'PASSWORD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating backends, metadata store and artifact store\n",
    "\n",
    "Now using the variables above, we can define our backends for orchestration and\n",
    "training, as well as artifact and metadata stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "orchestrator_backend = OrchestratorGCPBackend(\n",
    "    cloudsql_connection_name=cloudsql_connection_name,\n",
    "    project=project,\n",
    "    zone=zone)\n",
    "\n",
    "metadata_store=MySQLMetadataStore(\n",
    "        host='127.0.0.1',\n",
    "        port=3306,\n",
    "        database=mysql_db,\n",
    "        username=mysql_user,\n",
    "        password=mysql_pw)\n",
    "\n",
    "artifact_store=ArtifactStore(artifact_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the pipeline on Google Cloud\n",
    "\n",
    "Now you can run your pipeline on your own Google Cloud project. Simply specify the\n",
    "backends and custom artifact and metadata stores in your `pipeline.run()` call.\n",
    "\n",
    "**NB**: The CycleGAN network in this tutorial is really large (in excess of a gigabyte\n",
    "sometimes). If you run into problems with model deployment, it might be because of an\n",
    "incompatibility on GCAIP's side with such large models.\n",
    "\n",
    "An easy fix is to return the Monet Generator only in the CycleGANTrainer's `model_fn`\n",
    "if you want to generate Monet-styled images from reals only, or the Real Generator if\n",
    "you want to create real photos out of Monet paintings.\n",
    "\n",
    "Both of these agents are available as class attributes of the CycleGAN, so just change\n",
    "the return value of the `model_fn` to `return cycle_gan_model.{m_gen,p_gen}`\n",
    "depending on which one you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run the pipeline on a Google Cloud VM\n",
    "gan_pipeline.run(backend=orchestrator_backend,\n",
    "                 metadata_store=metadata_store,\n",
    "                 artifact_store=artifact_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have a successfully deployed model on your Google Cloud AI Platform\n",
    "project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requesting a deployed model in the Google Cloud\n",
    "\n",
    "This next section assumes that you successfully ran a ZenML Pipeline on the CycleGAN\n",
    "dataset, and that you deployed your model in the Google Cloud AI Platform.\n",
    "We are now going to generate images by sending prediction requests to a deployed\n",
    "Monet-styling CycleGAN model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and version checks\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import httplib2\n",
    "\n",
    "print(httplib2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially the httplib2 dependency is important here, since the prediction code\n",
    "using the Google API Client requires a version of httplib2 that is rather new\n",
    "(>= 0.16 should be fine). This example was run with version 0.18.1 of httplib2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a TFExample from an image and formatting it correctly\n",
    "\n",
    "First, we point to an image and make a TFExample out of it. The conversion code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_tfexample(image_path):\n",
    "    \"\"\"\n",
    "    Prepares an image into a TFExample understood by the served model.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        real_image_raw = img_file.read()\n",
    "        real_image = tf.io.decode_image(real_image_raw).numpy()\n",
    "\n",
    "    feature = {\n",
    "        \"image\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[real_image_raw])),\n",
    "    }\n",
    "\n",
    "    # return the TFExample\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now point to an example image from the CycleGAN dataset and convert it to a TFExample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# change this to a local image path (e.g. on your machine or on GCS)\n",
    "image_path = \"/path/to/sample_image.jpg\"\n",
    "\n",
    "example = prepare_tfexample(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting\n",
    "\n",
    "Looks good! Now we have to convert it to base64 so that AI Platform can parse the data properly.\n",
    "More information can be found here: https://cloud.google.com/ai-platform/prediction/docs/online-predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "instance = {\"examples\": {\"b64\": base64.b64encode(example.SerializeToString()).decode(\"utf-8\")}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requesting a prediction from our model\n",
    "\n",
    "First we point the environment variable GOOGLE_APPLICATION_CREDENTIALS (used frequently by GCP) to our service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# change this to point to your service account file\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/path/to/your/service_account.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual prediction request code\n",
    "\n",
    "The following code is used to query our model deployed in AI Platform. It can be found as well on the AI Platform Docs\n",
    "under https://cloud.google.com/ai-platform/prediction/docs/online-predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import googleapiclient\n",
    "def predict_json(project, model, instances, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "\n",
    "    Args:\n",
    "        project (str): project where the AI Platform Model is deployed.\n",
    "        model (str): model name.\n",
    "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
    "            your deployed model expects as inputs. Values should be datatypes\n",
    "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
    "            convertible to tensors.\n",
    "        version: str, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"\n",
    "    # Create the AI Platform service object.\n",
    "    # To authenticate set the environment variable\n",
    "    # GOOGLE_APPLICATION_CREDENTIALS=<path_to_service_account_file>\n",
    "    service = googleapiclient.discovery.build('ml', 'v1')\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "    return response['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# change this to your deployed model name\n",
    "model_name = \"YOUR_MODEL_NAME\"\n",
    "\n",
    "prediction = predict_json(project=project, model=model_name, instances=[instance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it! Let us visualize the response right away, along with our original image to look at the style\n",
    "transfer done by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predicted_image = np.array(prediction[0]['output_0'])\n",
    "\n",
    "predicted_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(image_path, \"rb\") as img_file:\n",
    "    real_image_raw = img_file.read()\n",
    "    real_image = tf.io.decode_image(real_image_raw).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head-to-head comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_, ax = plt.subplots(1, 2, figsize=(12, 12))\n",
    "\n",
    "predicted_image_revert = (predicted_image * 127.5 + 127.5).astype(np.uint8)\n",
    "\n",
    "ax[0].imshow(real_image)\n",
    "ax[1].imshow(predicted_image_revert)\n",
    "ax[0].set_title(\"Input Photo\")\n",
    "ax[1].set_title(\"Monet-esque\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It definitely looks different from our input image. If you are unhappy with your results, consider training the model\n",
    "further - GANs are rather infamous for exhibiting convergence issues and can take a long time to train sufficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "That's it for this tutorial! By tuning some hyperparameters and training for more\n",
    "epochs, you can also create high quality Monet renderings of real images. When\n",
    "you are successful, you immediately know the best configuration because it is all\n",
    "logged by the immutability of the pipeline! That way, you can immediately share your\n",
    "YAML configuration file to other people that can reproduce the results.\n",
    "\n",
    "We hope you had fun with this tutorial, and see you for the next one!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
