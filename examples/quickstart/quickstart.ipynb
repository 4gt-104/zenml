{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZenML Quickstart Guide\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "ZenML is an extensible, open-source MLOps framework for creating portable, \n",
    "production-ready machine learning pipelines. By decoupling infrastructure from \n",
    "code, ZenML enables developers across your organization to collaborate more \n",
    "effectively as they develop to production.\n",
    "\n",
    "![ZenMl Overview](_assets/zenml_overview.png)\n",
    "\n",
    "Let's see it in action and use ZenML to deploy an LLM into production. \n",
    "As an example, we will first train and deploy a simple LLM locally. \n",
    "Then we will switch the entire workflow to a production environment in the cloud \n",
    "that will automatically train the model on GPU-enabled hardware and deploy it to\n",
    "a scalable Kubernetes cluster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Requirements\n",
    "\n",
    "Let's install ZenML to get started. \n",
    "\n",
    "You might notice we also install some *integrations* and *hub plugins* here, \n",
    "these terms will be explained in more detail later when thez are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"zenml[server]\" gradio  # install ZenML and Gradio\n",
    "!zenml integration install pytorch mlflow -y  # install ZenML integrations\n",
    "!zenml hub install mingpt_example mlflow_steps -y  # install ZenML Hub plugins\n",
    "!zenml init  # Initialize a ZenML repository\n",
    "%pip install pyngrok pyparsing==2.4.2  # required for Colab\n",
    "\n",
    "import IPython\n",
    "\n",
    "# automatically restart kernel\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please wait for the installation to complete before running subsequent cells. At the end of the installation, the notebook kernel will automatically restart."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train and Deploy an LLM Locally\n",
    "\n",
    "TODO: motivate task, finetuning vs. prompt engineering?\n",
    "\n",
    "We will use Andrej Karpathy's [minGPT](https://github.com/karpathy/minGPT/tree/master/mingpt),\n",
    "which we previously installed from the [ZenML Hub TODO](TODO). The ZenML Hub is\n",
    "a place so dark... TODO\n",
    "\n",
    "Since training a multi-billion parameter model is not efficient to do locally,\n",
    "we will use the smallest GPT model `gpt-nano` for local training to validate our\n",
    "setup works. But don't worry, we will switch to a larger model later when we can\n",
    "train on a GPU in the cloud.\n",
    "\n",
    "To start, let's define a Python function to load the `gpt-nano` model and\n",
    "decorate it with a `@step` decorator. This is enough to turn the function into a\n",
    "ZenML step which can now be executed on any infrastructure, as we will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.hub.mingpt_example.mingpt.model import GPT\n",
    "from zenml import step\n",
    "\n",
    "@step\n",
    "def mingpt_model_loader_step(model_type=\"gpt-nano\") -> GPT:\n",
    "    model_config = GPT.get_default_config()\n",
    "    model_config.model_type = model_type\n",
    "    model_config.vocab_size = 50257  # openai's model vocabulary\n",
    "    model_config.block_size = 1024  # openai's model block_size\n",
    "    model = GPT(model_config)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not need to save the model within the step explicitly; \n",
    "ZenML is automatically taking care of this for us. \n",
    "Under the hood, ZenML persists all step inputs and outputs in an \n",
    "[Artifact Store](https://docs.zenml.io/component-gallery/artifact-stores). \n",
    "This also means that all of our data and models are automatically versioned and \n",
    "tracked.\n",
    "\n",
    "The `mingpt_example` plugin also contains a PyTorch dataset that can load the \n",
    "text of any website as lists of word embeddings to be used for language model \n",
    "training. Let us now define a ZenML step that uses this dataset to load a\n",
    "website of our choice for LLM training.\n",
    "\n",
    "For this example, we will use the BBC website but feel free to change it to a\n",
    "website of your choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.hub.mingpt_example.dataset import UrlTokenDataset\n",
    "\n",
    "\n",
    "@step\n",
    "def url_dataset_loader_step(urls=[\"https://www.bbc.com/\"]) -> UrlTokenDataset:\n",
    "    return UrlTokenDataset(urls=urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a step that uses the `mingpt` trainer class to train the \n",
    "`gpt-nano` model on our dataset and logs the model and metrics to MLflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from zenml.hub.mingpt_example.mingpt.trainer import Trainer\n",
    "\n",
    "\n",
    "@step(experiment_tracker=\"mlflow_tracker\")\n",
    "def mingpt_trainer_step(\n",
    "    dataset: Dataset, \n",
    "    model: Module,\n",
    "    max_iters=2000,\n",
    "    learning_rate=5e-4,\n",
    ") -> Module:\n",
    "    \n",
    "    train_config = Trainer.get_default_config()\n",
    "    train_config.learning_rate = learning_rate\n",
    "    train_config.max_iters = max_iters\n",
    "    train_config.device = \"mps\"  # TODO\n",
    "    trainer = Trainer(train_config, model, dataset)\n",
    "\n",
    "    def batch_end_callback(trainer):\n",
    "        if trainer.iter_num % 100 == 0:\n",
    "            mlflow.log_metric(\n",
    "                \"train_loss\", trainer.loss.item(), step=trainer.iter_num\n",
    "            )\n",
    "            print(\n",
    "                f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; \"\n",
    "                f\"iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\"\n",
    "            )\n",
    "    trainer.set_callback(\"on_batch_end\", batch_end_callback)\n",
    "\n",
    "    trainer.run()\n",
    "\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's add a step that deploys our model as prediction service using MLflow.\n",
    "The ZenML Hub already contains steps for this which we can simply import and use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.steps.mlflow_deployer import mlflow_model_deployer_step"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine all these steps into a ZenML Pipeline by defining a simple\n",
    "Python function decorated with ZenML's `@pipeline` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import pipeline\n",
    "\n",
    "@pipeline(enable_cache=False)\n",
    "def training_pipeline():\n",
    "    \"\"\"Train, evaluate, and deploy a model.\"\"\"\n",
    "    dataset = url_dataset_loader_step()\n",
    "    model = mingpt_model_loader_step()\n",
    "    model = mingpt_trainer_step(dataset, model)\n",
    "    mlflow_model_deployer_step(model=model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this pipeline, we will first need to setup MLflow.\n",
    "\n",
    "The `zenml integration install mlflow` command that we ran in the beginning of\n",
    "the notebook already installed MLflow for us together with ZenML's [MLflow integration TODO](TODO),\n",
    "which contains an MLflow experiment tracker, model deployer, as well as a model\n",
    "registry.\n",
    "\n",
    "Similarly to how we wrapped our code in a ZenML pipeline before, we will now\n",
    "setup an MLflow experiment tracker and model deployer as part of a [ZenML Stack TODO](),\n",
    "which will allow us to decouple our infrastructure and tools (MLflow in this case)\n",
    "from the code that we are running.\n",
    "\n",
    "Usually the easiest way to register new stacks is by using the ZenML dashboard,\n",
    "but we can also do it programmatically via the CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow experiment tracker\n",
    "!zenml experiment-tracker register mlflow_tracker --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model registry\n",
    "!zenml model-registry register mlflow_registry --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model deployer\n",
    "!zenml model-deployer register mlflow_deployer --flavor=mlflow\n",
    "\n",
    "# Register a new stack with the new stack components\n",
    "!zenml stack register quickstart_stack -a default\\\n",
    "                                       -o default\\\n",
    "                                       -d mlflow_deployer\\\n",
    "                                       -e mlflow_tracker\\\n",
    "                                       -r mlflow_registry\\"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up our stack, we can run arbitrary pipelines on it. Let's\n",
    "try it out and run our LLM training pipeline we have defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack set quickstart_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect Run Metadata and Lineage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZenML automatically tracks metadata of all runs, and saves all datasets and \n",
    "models to disk and versions them. Let's open the ZenML dashboard and check it\n",
    "out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zenml\n",
    "\n",
    "zenml.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will spin up a local ZenML server and connect you to it. \n",
    "You can login with username `default` and an empty password.\n",
    "\n",
    "![ZenML Server Up](_assets/zenml-up.gif)\n",
    "\n",
    "Go to the \"Runs\" tab and click on your run. You should now be able to see a\n",
    "detailed lineage graph of your run. Try clicking on some of the steps or\n",
    "artifacts to explore all the metadata ZenML tracks for you!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interact with the Deployed Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the last output of our run was a model prediction service that\n",
    "is now running in the background and waiting for requests.\n",
    "\n",
    "You can run `zenml model-deployer models list` to get an overview of all \n",
    "currently deployed models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml model-deployer models list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the exact URL where the deployed model is reachable, we can use ZenML's \n",
    "post-execution utility code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = training_pipeline.get_runs()[0]\n",
    "deployer_step = pipeline_run.get_step(\"mlflow_model_deployer_step\")\n",
    "deployed_model_url = deployer_step.metadata[\"deployed_model_url\"].value\n",
    "print(deployed_model_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, you can now simply send REST requests to this model from your \n",
    "website or app.\n",
    "\n",
    "To demonstrate this, we have built a simple frontend for our model using\n",
    "[gradio](https://gradio.app/). The code details are not too important at this\n",
    "point, but feel free to checkout `utils/frontend.py` if you're interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.frontend import QuestionAnsweringFrontend\n",
    "\n",
    "QuestionAnsweringFrontend(deployed_model_url=deployed_model_url).launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train GPT-XL on remote stack\n",
    "\n",
    "After playing with the frontend for a bit, you might notice that the overall\n",
    "question answering performance of our app is still quite poor. This is not\n",
    "surprising since we have used a fairly small model that wasn't even pretrained.\n",
    "\n",
    "Let's change this and use a pretrained version of the largest mingpt model, \n",
    "`gpt-xl` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def pretrained_gpt_xl_loader_step() -> GPT:\n",
    "    return GPT.from_pretrained(\"gpt2-xl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our model service available to users around the world we will \n",
    "also need to deploy it in a highly scalable cloud environment instead of \n",
    "our local machine.\n",
    "\n",
    "Fortunately, the ZenML Hub also contains model deployment steps for [Seldon TODO](), which can do just that. \n",
    "\n",
    "Let's import that step and define the corresponding pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.seldon.steps.seldon_deployer import seldon_model_deployer_step\n",
    "from zenml.hub.mingpt_example.steps import url_dataset_loader_step, pretrained_gpt_xl_loader_step, mingpt_trainer_step\n",
    "\n",
    "@pipeline(enable_cache=False)\n",
    "def gpt_xl_training_pipeline():\n",
    "    \"\"\"Train, evaluate, and deploy a model.\"\"\"\n",
    "    dataset = url_dataset_loader_step()\n",
    "    model = pretrained_gpt_xl_loader_step()\n",
    "    model = mingpt_trainer_step(dataset, model)\n",
    "    seldon_model_deployer_step(model=model, deployment_decision=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this pipeline, we will need a quite sophisticated infrastructure setup\n",
    "that contains a Kubernetes cluster to host Seldon as well as a GPU-enabled\n",
    "environment to train GPT-XL.\n",
    "\n",
    "Furthermore, we might also want to run our other pipeline steps in a more\n",
    "scalable cloud environment, for which we would need an orchestration tool like \n",
    "[Kubeflow TODO]() as well as a cloud storage bucket where we can save our \n",
    "datasets, models and other artifacts.\n",
    "\n",
    "To summarize, we need to provision an environment that:\n",
    "- runs fully in the cloud\n",
    "- has access to a GPU for our model trainer step\n",
    "- contains a Kubernetes cluster with Seldon and Kubeflow installed in it\n",
    "- contains a cloud storage bucket\n",
    "\n",
    "TODO: stack visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up an environment like this is usually not trivial. Fortuantely, ZenML\n",
    "provides a lot of utility tools to set up all the infrastructure you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show how to deploy this stack with ZenML (@Jayesh ?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you do not have a cloud project of your own where you can set \n",
    "this up, you can use the [ZenML Sandbox TODO](TODO) to\n",
    "temporarily provision this infrastructure stack, free of charge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: connect to sandbox (@Safoine ?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ZenML pipelines can run on any stack, it is quite easy to run our new\n",
    "training pipeline in this cloud environment now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml stack set ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_xl_training_pipeline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the pipeline is now running in the cloud and using a much larger model,\n",
    "it will take a few minutes until the run is complete.\n",
    "\n",
    "To keep track of your run, open the ZenML dashboard again and navigate to the\n",
    "detail page of your run where you can see a status indicator that shows you at \n",
    "a glance whether the run is finished or not.\n",
    "\n",
    "TODO: screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zenml.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the run is complete, we will again have our model deployed as a prediction\n",
    "service to which we can send REST API requests. However, this time the service\n",
    "is deployed in a highly scalable cloud environment, is reachable from anywhere\n",
    "around the world, and the quality of its answers should also be much better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model_url = None  # TODO\n",
    "QuestionAnsweringFrontend(deployed_model_url=deployed_model_url).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You just built your first ML Pipeline! You not only trained a model, you also deployed it, served it, and learned how to monitor and visualize everything that's going on. Did you notice how easy it was to bring all of the different components together using ZenML's abstractions? And that is just the tip of the iceberg of what ZenML can do; check out the [**Integrations**](https://zenml.io/integrations) page for a list of all the cool MLOps tools that ZenML supports!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go next\n",
    "\n",
    "* If you have questions or feedback... \n",
    "  * Join our [**Slack Community**](https://zenml.io/slack-invite) and become part of the ZenML family!\n",
    "* If this quickstart was a bit too quick for you... \n",
    "  * Check out [**ZenBytes**](https://github.com/zenml-io/zenbytes), our lesson series on practical MLOps, where we cover each MLOps concept in much more detail.\n",
    "* If you want to learn more about using or extending ZenML...\n",
    "  * Check out our [**Docs**](https://docs.zenml.io/) or read through our code on [**Github**](https://github.com/zenml-io/zenml).\n",
    "* If you want to quickly learn how to use a specific tool with ZenML...\n",
    "  * Check out our collection of [**Examples**](https://github.com/zenml-io/zenml/tree/doc/hamza-misc-updates/examples).\n",
    "* If you want to see some advanced ZenML use cases... \n",
    "  * Check out [**ZenML Projects**](https://github.com/zenml-io/zenml-projects), our collection of production-grade ML use-cases."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ZenML Quickstart.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d2a2855dd99d151bf9fdc91431d39c1e05805b2488b8cd3e8da8d54747db678"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
