- where do I keep the individual recipe configs?
- do I make the config_type of terraform provider a union of all recipe configs?
- then when I convert the config (overwrite convert_config) from the base service 
  config I choose the right class based on config.type
- overwrite provision of TerraformService in the TerraformZenServer class. 
  this provision function will take the recipe-specific server deployment config 
  and then update the locals.tf file at the directory_path with these values. 
  Then call super.provision()

1. DONE [STEFAN] where do we keep the ZenML server terraform recipes and how do we version,
package and distribute them ? 
  - keep them in the zenml repo, version/release them as pypi extras. We only
  need the server recipes to deploy the servers with `zenml deploy`.

1.a. [STEFAN] how do we version/release/reference the helm chart in the Terraform
recipes ?
  - use some public registry and release versions of the helm chart there
  - reference the chart by repo/name/version in the recipe using vars with defaults
  that change with every release

2. RDS database provisioning:
  - DONE version needs to be 5.7.* (8.0) doesn't work
  - SSL ? (later)

3. [STEFAN] the terraform recipe should be copied in the service runtime path, instead of
being updated and applied in place, to allow for multiple instances of the same
recipe to be deployed (in theory, at least). I.e. the `terraform_client.working_dir`
should be a subdirectory of the service runtime path where the recipe is copied to.
  - DONE: change the TerraformService init to do this
  - change the stack recipe logic to no longer copy the recipes

4. [STEFAN] deserializing the terraform server from disk loses the config type (subtype)

5. [JAYESH] how do we extract the URL (hostname, port) and credentials (username/password/CA certificate)
from the deployed server ?
  - username and password are in fact supplied as input (recipe config)
  - the URL is a recipe output
  - the CA certificate also needs to be a recipe output, when using TLS with self-signed certificates
  Q: can they also be "extracted" after the first deployment ? I.e. on updates ?
  A: yes, on updates, it will output new values
  - J: URL as deployment status
  - J: CA cert as deployment status

- what other input information is required in the deployment config ?
  - prerequisites:
    - install terraform (part of zenml's stacks extras)
    - install AWS client, kubectl, helm
    - set up AWS credentials
    - create an AWS EKS cluster yourself (docs) or use a ZenML recipe to create it
    (note: zenml server is not part of the stack)
    - configure kubectl to talk to the EKS cluster (kubectl config context)
    - `zenml deploy`
      - assume the default location for the .kube/config file
      - assume the current context
      - other assumptions for the noob experience:
        - always deploy a DB

7. [JAYESH] use config defaults that allow multi-instance deployments
  - if two ppl are using the same AWS cloud account, same region and same EKS
  cluster, but two different names (prefix values), they should not clash.
  - names of all resources (helm release, RDS DB, ingress) should use the prefix
  - exception: ingress controller. The recipe checks if another ingress controller
  is already deployed and if so, it uses that one. If not, it deploys its own.
  - the service name is used as a prefix

8. [JAYESH] terraform service implementation changes:
  - save the service config before making long-running changes (e.g. terraform init/apply)
  - without `check_status`, how do we know if the server is up and running ?
    - use terraform plan in the `check_status` function
      - if exit code is zero, then all resources are up and running -> ACTIVE
      - 1 -> ERROR
      - 2 -> INACTIVE

9. how do we update an existing server recipe deployment ?
  - run `zenml deploy` with an updated config ?
    -> [STEFAN] how do we prevent the user from creating a new server deployment
    instead of updating the existing one ?
    -> [STEFAN] an update does not need to stop the service (rolling update for
    Terraform)

10. [NOT NOW] what about service endpoints ? can we implement them with the
  extra TLS complications ?
  - what about using an orphan REST zenml store for this ?

11. [NOT NOW] server logs (recipe logs but also server logs)
  - we're streaming the recipe logs to the console
  - we need the pod name to get/stream the server logs
  - we can steal the code from the model deployer implementation

12. helm chart recipe glue problems
  - not all ingresses have paths. In fact, the default is to not use any path


13. other UX issues
  - aborting a deployment in progress leaves terraform running in the background
  and the recipe is locked. If killed, then the recipe thinks it's overwriting
  existing resources and fails.
  - no default timeout ??